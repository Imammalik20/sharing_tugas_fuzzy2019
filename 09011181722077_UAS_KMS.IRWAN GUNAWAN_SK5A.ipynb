{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Classification Bank Client data Job\n",
    "\n",
    "<div class=\"\">\n",
    "<br><br>\n",
    "<center><br><font size=\"12\"><font face=\"calibri\"><strong>Kecerdasan Buatan\n",
    "<center><br><font size=\"5\"><font face=\"calibri\"><strong>Bank Telemarketing Analysis Machine Learning\n",
    "<br>\n",
    "<br>\n",
    "<br><strong>Oleh :\n",
    "<br><strong>Kms.Irwan Gunawan\n",
    "<br><strong>09011181722077\n",
    "<br><br>\n",
    "<br><strong>Mata Kuliah Kecerdasan Buatan\n",
    "<br>\n",
    "<br><strong>Dosen Pengampuh : \n",
    "<br><strong>Prof. Dr. Ir. Siti Nurmaini, M.T.\n",
    "<br><br><br>\n",
    "<br><strong>Jurusan Sistem Komputer\n",
    "<br><strong>Fakultas Ilmu Komputer\n",
    "<br><strong>Universitas Sriwijaya\n",
    "<br><strong>2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modul yang diperlukan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modul Yang diperlukan adalah\n",
    "\n",
    "a. `numpy dan Pandas `\n",
    "    Ke 2 modul ini sudah standar dan wajib digunakan apabila diimport untuk menghandle data.\n",
    "\n",
    "b. `Matplotlib`\n",
    "    Matloplib ialah Library yang digunakan untuk men-plot hasil loss dan akurasi nanti.\n",
    "    \n",
    "c. `sklearn`\n",
    "    Adapun pada library sklearn ini, fungsi yang dipakai adalah\n",
    "    1. PCA(Principal Component Analysis) => mereduksi fitur\n",
    "    2. train_test_split  => Untuk men-split data menjadi training dan testing\n",
    "    3. classification_report => mendapatkan hasil pengukuran CM\n",
    "    4. confusion_matrix  => mengukur performa model learning \n",
    "\n",
    "d. `keras`\n",
    "    Pada Keras , Fungsi yang dipakai adalah :\n",
    "    1. Layers Dense\n",
    "    2. Optimizers Adam\n",
    "    3. Model Sequential\n",
    "    4. utils to_categorical\n",
    "    5. Aktivasi Relu\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder # ngubah categori ke angka\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "    Data set yang dipakai adalah BANK. Data ini bisa didapat di kaggle\n",
    "    Link Dataset BANK : \n",
    "    \n",
    "https://www.kaggle.com/yufengsui/ml-project-bank-telemarketing-analysis\n",
    "\n",
    "    Dataset ini bersumberkan dari Portuguese Bank Marketing Data Set . Dataset ini sudah didownsampling menjadi 125 node \n",
    "    perdetik oleh Aurelia Sui. \n",
    "    \n",
    "<h3>The PTB Diagnostic ECG Database</h3>\n",
    "<ul>\n",
    "    <li><strong> Jumlah data</strong> : 4552</li>\n",
    "    <li> <strong>Sumber data</strong> : Portuguese Bank Marketing Data</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "## Fitur data set\n",
    "\n",
    "Fitur dataset ini berupa beat yang terdiri atas 17\n",
    "\n",
    "## Label Dataset\n",
    "\n",
    "Jumlah job pada kolom label : `12`\n",
    "\n",
    "  Label yang akan diklasifikasikan adalah job\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data Set\n",
    "\n",
    "Dataset Bank \n",
    "\n",
    "    1.bank.csv   => Data set bank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv ('D:/FILE KULIAH/Semester 5/AI/UAS/bank.csv', sep=';') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### df.head menampilkan data set Bank 5 teratas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>unemployed</td>\n",
       "      <td>married</td>\n",
       "      <td>primary</td>\n",
       "      <td>no</td>\n",
       "      <td>1787</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>19</td>\n",
       "      <td>oct</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>4789</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>cellular</td>\n",
       "      <td>11</td>\n",
       "      <td>may</td>\n",
       "      <td>220</td>\n",
       "      <td>1</td>\n",
       "      <td>339</td>\n",
       "      <td>4</td>\n",
       "      <td>failure</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>35</td>\n",
       "      <td>management</td>\n",
       "      <td>single</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>1350</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>16</td>\n",
       "      <td>apr</td>\n",
       "      <td>185</td>\n",
       "      <td>1</td>\n",
       "      <td>330</td>\n",
       "      <td>1</td>\n",
       "      <td>failure</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>management</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>1476</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>unknown</td>\n",
       "      <td>3</td>\n",
       "      <td>jun</td>\n",
       "      <td>199</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>59</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>226</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          job  marital  education default  balance housing loan  \\\n",
       "0   30   unemployed  married    primary      no     1787      no   no   \n",
       "1   33     services  married  secondary      no     4789     yes  yes   \n",
       "2   35   management   single   tertiary      no     1350     yes   no   \n",
       "3   30   management  married   tertiary      no     1476     yes  yes   \n",
       "4   59  blue-collar  married  secondary      no        0     yes   no   \n",
       "\n",
       "    contact  day month  duration  campaign  pdays  previous poutcome  y  \n",
       "0  cellular   19   oct        79         1     -1         0  unknown  0  \n",
       "1  cellular   11   may       220         1    339         4  failure  0  \n",
       "2  cellular   16   apr       185         1    330         1  failure  0  \n",
       "3   unknown    3   jun       199         4     -1         0  unknown  0  \n",
       "4   unknown    5   may       226         1     -1         0  unknown  0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4521, 17)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "management       969\n",
       "blue-collar      946\n",
       "technician       768\n",
       "admin.           478\n",
       "services         417\n",
       "retired          230\n",
       "self-employed    183\n",
       "entrepreneur     168\n",
       "unemployed       128\n",
       "housemaid        112\n",
       "student           84\n",
       "unknown           38\n",
       "Name: job, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.job.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1475</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2030</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>216</td>\n",
       "      <td>0</td>\n",
       "      <td>228</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1303</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>181</td>\n",
       "      <td>0</td>\n",
       "      <td>219</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1352</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>195</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>274</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>222</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  job  marital  education  default  balance  housing  loan  contact  \\\n",
       "0   11   10        1          0        0     1475        0     0        0   \n",
       "1   14    7        1          1        0     2030        1     1        0   \n",
       "2   16    4        2          2        0     1303        1     0        0   \n",
       "3   11    4        1          2        0     1352        1     1        2   \n",
       "4   40    1        1          1        0      274        1     0        2   \n",
       "\n",
       "   day  month  duration  campaign  pdays  previous  poutcome  y  \n",
       "0   18     10        75         0      0         0         3  0  \n",
       "1   10      8       216         0    228         4         0  0  \n",
       "2   15      0       181         0    219         1         0  0  \n",
       "3    2      6       195         3      0         0         3  0  \n",
       "4    4      8       222         0      0         0         3  0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = LabelEncoder()\n",
    "dataset = df.apply(enc.fit_transform)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4521, 17)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pemisahan antara Fitur dan Label pada dataset\n",
    "\n",
    "    Fitur dan Label dipisah supaya mesin bisa mempelajari fitur dari label yang diberikan.\n",
    "    Fitur\n",
    "    x = iloc value\n",
    "    \n",
    "    Label\n",
    "    y = iloc value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= dataset.iloc[:,:16].values\n",
    "y = dataset.iloc[:,16].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### kemudian di rubah menjadi bilangan binnary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>4521.000000</td>\n",
       "      <td>4521.000000</td>\n",
       "      <td>4521.000000</td>\n",
       "      <td>4521.000000</td>\n",
       "      <td>4521.000000</td>\n",
       "      <td>4521.000000</td>\n",
       "      <td>4521.000000</td>\n",
       "      <td>4521.000000</td>\n",
       "      <td>4521.000000</td>\n",
       "      <td>4521.000000</td>\n",
       "      <td>4521.000000</td>\n",
       "      <td>4521.000000</td>\n",
       "      <td>4521.000000</td>\n",
       "      <td>4521.000000</td>\n",
       "      <td>4521.000000</td>\n",
       "      <td>4521.000000</td>\n",
       "      <td>4521.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>22.168104</td>\n",
       "      <td>4.411192</td>\n",
       "      <td>1.147755</td>\n",
       "      <td>1.231365</td>\n",
       "      <td>0.016810</td>\n",
       "      <td>894.699624</td>\n",
       "      <td>0.566025</td>\n",
       "      <td>0.152842</td>\n",
       "      <td>0.652289</td>\n",
       "      <td>14.915284</td>\n",
       "      <td>5.540146</td>\n",
       "      <td>241.323380</td>\n",
       "      <td>1.783234</td>\n",
       "      <td>25.368503</td>\n",
       "      <td>0.539925</td>\n",
       "      <td>2.559168</td>\n",
       "      <td>0.115240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>10.568155</td>\n",
       "      <td>3.255716</td>\n",
       "      <td>0.599650</td>\n",
       "      <td>0.748744</td>\n",
       "      <td>0.128575</td>\n",
       "      <td>632.935320</td>\n",
       "      <td>0.495676</td>\n",
       "      <td>0.359875</td>\n",
       "      <td>0.901498</td>\n",
       "      <td>8.247667</td>\n",
       "      <td>3.002763</td>\n",
       "      <td>196.360039</td>\n",
       "      <td>2.997180</td>\n",
       "      <td>63.823284</td>\n",
       "      <td>1.661181</td>\n",
       "      <td>0.992051</td>\n",
       "      <td>0.319347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>343.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1354.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>325.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2352.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>874.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>291.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               age          job      marital    education      default  \\\n",
       "count  4521.000000  4521.000000  4521.000000  4521.000000  4521.000000   \n",
       "mean     22.168104     4.411192     1.147755     1.231365     0.016810   \n",
       "std      10.568155     3.255716     0.599650     0.748744     0.128575   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%      14.000000     1.000000     1.000000     1.000000     0.000000   \n",
       "50%      20.000000     4.000000     1.000000     1.000000     0.000000   \n",
       "75%      30.000000     7.000000     2.000000     2.000000     0.000000   \n",
       "max      66.000000    11.000000     2.000000     3.000000     1.000000   \n",
       "\n",
       "           balance      housing         loan      contact          day  \\\n",
       "count  4521.000000  4521.000000  4521.000000  4521.000000  4521.000000   \n",
       "mean    894.699624     0.566025     0.152842     0.652289    14.915284   \n",
       "std     632.935320     0.495676     0.359875     0.901498     8.247667   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%     343.000000     0.000000     0.000000     0.000000     8.000000   \n",
       "50%     697.000000     1.000000     0.000000     0.000000    15.000000   \n",
       "75%    1354.000000     1.000000     0.000000     2.000000    20.000000   \n",
       "max    2352.000000     1.000000     1.000000     2.000000    30.000000   \n",
       "\n",
       "             month     duration     campaign        pdays     previous  \\\n",
       "count  4521.000000  4521.000000  4521.000000  4521.000000  4521.000000   \n",
       "mean      5.540146   241.323380     1.783234    25.368503     0.539925   \n",
       "std       3.002763   196.360039     2.997180    63.823284     1.661181   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       3.000000   100.000000     0.000000     0.000000     0.000000   \n",
       "50%       6.000000   181.000000     1.000000     0.000000     0.000000   \n",
       "75%       8.000000   325.000000     2.000000     0.000000     0.000000   \n",
       "max      11.000000   874.000000    31.000000   291.000000    23.000000   \n",
       "\n",
       "          poutcome            y  \n",
       "count  4521.000000  4521.000000  \n",
       "mean      2.559168     0.115240  \n",
       "std       0.992051     0.319347  \n",
       "min       0.000000     0.000000  \n",
       "25%       3.000000     0.000000  \n",
       "50%       3.000000     0.000000  \n",
       "75%       3.000000     0.000000  \n",
       "max       3.000000     1.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing dataset.\n",
    "\n",
    "Dataset yang diambil pada kaggle ini sudah dipreprosessing oleh `Aurelia Sui` dengan cara :\n",
    "    \n",
    "    1. Scaling => mengubah nilai data menjadi 0 sampai 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4521, 16)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler= MinMaxScaler()\n",
    "feature_scaled=scaler.fit_transform(x)\n",
    "feature_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data\n",
    "\n",
    "    Data fitur dan label dibagi menjadi data train dan test dengan ratio 9 : 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.1, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model ANN\n",
    "\n",
    "Jaringan saraf tiruan (JST) atau sistem koneksionis adalah sistem komputasi yang secara samar-samar terinspirasi oleh jaringan saraf biologis yang membentuk otak hewan.\n",
    "\n",
    "Disini digunakan model `Sequential()`. Pada Model ini terdapat `Input layer` , `Hidden layer` dan `Output layer`. \n",
    "\n",
    "<strong>Penentuan Jumlah Layer ditentukan sebagai berikut </strong>:\n",
    "\n",
    "    a. Jumlah dari input layer ialah sama dengan jumlah fitur yang diberikan kepada mesin yang ada.\n",
    "    b. Jumlah dari Hidden layer ditentukan dengan sesuai atau lebih besar dari Input layer yang di berikan.\n",
    "    c. Jumlah dari Output Layer sesuai dengan kolom yang diberikan kepada mesin.\n",
    "  \n",
    "  \n",
    "<strong>Jumlah layer yang ditentukan adalah </strong>:\n",
    "\n",
    "    1. Input layer          -> Layer pertama terbentuk dari 12 layer Neural sesuai fitur dengan menggunakan activation                                    relu.\n",
    "    2. Hidden Layer pertama -> 12 units Neural Network dengan menggunakan activation relu.\n",
    "    3. Hidden Layer Kedua   -> 12 units Neural Network dengan menggunakan activation relu.\n",
    "    4. Output layer         -> 1 unit Neural Network dengan menggunakan activation sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units= 100, input_dim =16 , activation='relu'))\n",
    "model.add(Dense(units= 100, activation='relu'))\n",
    "model.add(Dense(units= 1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile Model\n",
    "\n",
    "Model Dicompile dengan Optimizer adam dengan fungsi loss Binary Crossentropy.\n",
    "\n",
    "    Disini ada parameter optimizer adam('adam' adalah pengoptimal default yang baik untuk digunakan, dan umumnya akan bekerja dengan baik) yaitu learning_rate.\n",
    "                                \n",
    "    Learning_rate yaitu seberapa teliti mesin itu belajar.Semakin kecil nilai learning rate ,maka semakin teliti mesin belajar dan semakin tinggi kesempatan untuk mendapatkan hasil yang bagus.    \n",
    "\n",
    "    Learning rate yang tinggi memiliki kelemahan yaitu, waktu untuk proses training yang lama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(lr=0.0001), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MenTraining model dengan data train serta Validasi dengan data Test\n",
    "\n",
    "Model yang sudah dibentuk itu dilakukan proses pembelajaran sesuai dengan **data train** dan menvalidasi proses pembelajaran mesin dengan data validasi yaitu **data testing**. \n",
    "\n",
    "Adapun parameter yang bisa di tuning yaitu :\n",
    "\n",
    "1. `batch_size` => 16 size\n",
    "2. `epochs`  => 1000 epochs, Seberapa banyak mesin mengulang pembelajaran sampai nilai weight tidak berubah lagi. Nilai epochs yang direkomendasikan adalah 100-1000 karena biasanya dengan nilai 100-1000, performa mesin sudah konvergen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4068 samples, validate on 453 samples\n",
      "Epoch 1/1000\n",
      "4068/4068 [==============================] - 0s 83us/step - loss: 0.3929 - accuracy: 0.8751 - val_loss: 0.3373 - val_accuracy: 0.8918\n",
      "Epoch 2/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.3277 - accuracy: 0.8842 - val_loss: 0.3146 - val_accuracy: 0.8698\n",
      "Epoch 3/1000\n",
      "4068/4068 [==============================] - 0s 83us/step - loss: 0.3330 - accuracy: 0.8813 - val_loss: 0.5547 - val_accuracy: 0.9007\n",
      "Epoch 4/1000\n",
      "4068/4068 [==============================] - 0s 83us/step - loss: 0.4059 - accuracy: 0.8766 - val_loss: 0.4791 - val_accuracy: 0.8918\n",
      "Epoch 5/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.3332 - accuracy: 0.8795 - val_loss: 0.3758 - val_accuracy: 0.8808\n",
      "Epoch 6/1000\n",
      "4068/4068 [==============================] - 0s 85us/step - loss: 0.3217 - accuracy: 0.8850 - val_loss: 0.2681 - val_accuracy: 0.8985\n",
      "Epoch 7/1000\n",
      "4068/4068 [==============================] - 0s 85us/step - loss: 0.3714 - accuracy: 0.8724 - val_loss: 0.2814 - val_accuracy: 0.8852\n",
      "Epoch 8/1000\n",
      "4068/4068 [==============================] - 0s 94us/step - loss: 0.3178 - accuracy: 0.8820 - val_loss: 0.4222 - val_accuracy: 0.9007\n",
      "Epoch 9/1000\n",
      "4068/4068 [==============================] - 0s 94us/step - loss: 0.3456 - accuracy: 0.8773 - val_loss: 0.4044 - val_accuracy: 0.9051\n",
      "Epoch 10/1000\n",
      "4068/4068 [==============================] - 0s 91us/step - loss: 0.4421 - accuracy: 0.8695 - val_loss: 0.4649 - val_accuracy: 0.8124\n",
      "Epoch 11/1000\n",
      "4068/4068 [==============================] - 0s 83us/step - loss: 0.3424 - accuracy: 0.8773 - val_loss: 0.4474 - val_accuracy: 0.8985\n",
      "Epoch 12/1000\n",
      "4068/4068 [==============================] - 0s 75us/step - loss: 0.3335 - accuracy: 0.8823 - val_loss: 0.3579 - val_accuracy: 0.8455\n",
      "Epoch 13/1000\n",
      "4068/4068 [==============================] - 0s 76us/step - loss: 0.3607 - accuracy: 0.8783 - val_loss: 0.8501 - val_accuracy: 0.9029\n",
      "Epoch 14/1000\n",
      "4068/4068 [==============================] - 0s 75us/step - loss: 0.3870 - accuracy: 0.8778 - val_loss: 0.4251 - val_accuracy: 0.8985\n",
      "Epoch 15/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.4200 - accuracy: 0.8697 - val_loss: 0.3275 - val_accuracy: 0.8896\n",
      "Epoch 16/1000\n",
      "4068/4068 [==============================] - 0s 75us/step - loss: 0.3274 - accuracy: 0.8854 - val_loss: 0.3206 - val_accuracy: 0.8940\n",
      "Epoch 17/1000\n",
      "4068/4068 [==============================] - 0s 73us/step - loss: 0.3802 - accuracy: 0.8783 - val_loss: 0.3479 - val_accuracy: 0.8896\n",
      "Epoch 18/1000\n",
      "4068/4068 [==============================] - 0s 73us/step - loss: 0.3165 - accuracy: 0.8832 - val_loss: 0.4691 - val_accuracy: 0.8035\n",
      "Epoch 19/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.3438 - accuracy: 0.8805 - val_loss: 0.3569 - val_accuracy: 0.8455\n",
      "Epoch 20/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.4145 - accuracy: 0.8751 - val_loss: 0.2786 - val_accuracy: 0.9007\n",
      "Epoch 21/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.3790 - accuracy: 0.8783 - val_loss: 0.4270 - val_accuracy: 0.8675\n",
      "Epoch 22/1000\n",
      "4068/4068 [==============================] - 0s 75us/step - loss: 0.3435 - accuracy: 0.8788 - val_loss: 0.3175 - val_accuracy: 0.8852\n",
      "Epoch 23/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.3456 - accuracy: 0.8771 - val_loss: 0.3426 - val_accuracy: 0.8764\n",
      "Epoch 24/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.4314 - accuracy: 0.8771 - val_loss: 0.2974 - val_accuracy: 0.8830\n",
      "Epoch 25/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.3396 - accuracy: 0.8818 - val_loss: 0.3241 - val_accuracy: 0.8962\n",
      "Epoch 26/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.3002 - accuracy: 0.8891 - val_loss: 0.4521 - val_accuracy: 0.9029\n",
      "Epoch 27/1000\n",
      "4068/4068 [==============================] - 0s 75us/step - loss: 0.3434 - accuracy: 0.8783 - val_loss: 0.3168 - val_accuracy: 0.8896\n",
      "Epoch 28/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.3681 - accuracy: 0.8840 - val_loss: 0.3369 - val_accuracy: 0.8742\n",
      "Epoch 29/1000\n",
      "4068/4068 [==============================] - 0s 75us/step - loss: 0.3101 - accuracy: 0.8808 - val_loss: 0.3003 - val_accuracy: 0.8852\n",
      "Epoch 30/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.3228 - accuracy: 0.8791 - val_loss: 0.2818 - val_accuracy: 0.9029\n",
      "Epoch 31/1000\n",
      "4068/4068 [==============================] - 0s 76us/step - loss: 0.3450 - accuracy: 0.8818 - val_loss: 0.3191 - val_accuracy: 0.8962\n",
      "Epoch 32/1000\n",
      "4068/4068 [==============================] - 0s 94us/step - loss: 0.3679 - accuracy: 0.8759 - val_loss: 0.3253 - val_accuracy: 0.8675\n",
      "Epoch 33/1000\n",
      "4068/4068 [==============================] - 0s 83us/step - loss: 0.3536 - accuracy: 0.8749 - val_loss: 0.2752 - val_accuracy: 0.8962\n",
      "Epoch 34/1000\n",
      "4068/4068 [==============================] - 0s 82us/step - loss: 0.3092 - accuracy: 0.8869 - val_loss: 0.2923 - val_accuracy: 0.8918\n",
      "Epoch 35/1000\n",
      "4068/4068 [==============================] - 0s 90us/step - loss: 0.3578 - accuracy: 0.8741 - val_loss: 0.2792 - val_accuracy: 0.8830\n",
      "Epoch 36/1000\n",
      "4068/4068 [==============================] - 0s 116us/step - loss: 0.3541 - accuracy: 0.8771 - val_loss: 0.2910 - val_accuracy: 0.8940\n",
      "Epoch 37/1000\n",
      "4068/4068 [==============================] - 0s 110us/step - loss: 0.3195 - accuracy: 0.8808 - val_loss: 0.3866 - val_accuracy: 0.9029\n",
      "Epoch 38/1000\n",
      "4068/4068 [==============================] - 0s 91us/step - loss: 0.4257 - accuracy: 0.8695 - val_loss: 0.2803 - val_accuracy: 0.8985\n",
      "Epoch 39/1000\n",
      "4068/4068 [==============================] - 0s 82us/step - loss: 0.3475 - accuracy: 0.8818 - val_loss: 0.3635 - val_accuracy: 0.8565\n",
      "Epoch 40/1000\n",
      "4068/4068 [==============================] - 0s 83us/step - loss: 0.3569 - accuracy: 0.8783 - val_loss: 0.3177 - val_accuracy: 0.8521\n",
      "Epoch 41/1000\n",
      "4068/4068 [==============================] - 0s 93us/step - loss: 0.3291 - accuracy: 0.8845 - val_loss: 0.2924 - val_accuracy: 0.8918\n",
      "Epoch 42/1000\n",
      "4068/4068 [==============================] - 0s 82us/step - loss: 0.3276 - accuracy: 0.8850 - val_loss: 0.3501 - val_accuracy: 0.8675\n",
      "Epoch 43/1000\n",
      "4068/4068 [==============================] - ETA: 0s - loss: 0.3679 - accuracy: 0.87 - 0s 87us/step - loss: 0.3687 - accuracy: 0.8746 - val_loss: 0.2711 - val_accuracy: 0.8918\n",
      "Epoch 44/1000\n",
      "4068/4068 [==============================] - 0s 99us/step - loss: 0.3239 - accuracy: 0.8793 - val_loss: 0.3602 - val_accuracy: 0.8587\n",
      "Epoch 45/1000\n",
      "4068/4068 [==============================] - 0s 91us/step - loss: 0.3475 - accuracy: 0.8820 - val_loss: 0.2855 - val_accuracy: 0.8786\n",
      "Epoch 46/1000\n",
      "4068/4068 [==============================] - 0s 91us/step - loss: 0.3005 - accuracy: 0.8862 - val_loss: 0.3314 - val_accuracy: 0.9007\n",
      "Epoch 47/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.3311 - accuracy: 0.8847 - val_loss: 0.4289 - val_accuracy: 0.8411\n",
      "Epoch 48/1000\n",
      "4068/4068 [==============================] - 0s 92us/step - loss: 0.3717 - accuracy: 0.8776 - val_loss: 0.2878 - val_accuracy: 0.8852\n",
      "Epoch 49/1000\n",
      "4068/4068 [==============================] - 0s 74us/step - loss: 0.3246 - accuracy: 0.8877 - val_loss: 0.2938 - val_accuracy: 0.9029\n",
      "Epoch 50/1000\n",
      "4068/4068 [==============================] - 0s 87us/step - loss: 0.3610 - accuracy: 0.8835 - val_loss: 0.3468 - val_accuracy: 0.8962\n",
      "Epoch 51/1000\n",
      "4068/4068 [==============================] - 0s 75us/step - loss: 0.3246 - accuracy: 0.8830 - val_loss: 0.3565 - val_accuracy: 0.8609\n",
      "Epoch 52/1000\n",
      "4068/4068 [==============================] - 0s 76us/step - loss: 0.3525 - accuracy: 0.8837 - val_loss: 0.2793 - val_accuracy: 0.8985\n",
      "Epoch 53/1000\n",
      "4068/4068 [==============================] - 0s 76us/step - loss: 0.3476 - accuracy: 0.8786 - val_loss: 0.5120 - val_accuracy: 0.9029\n",
      "Epoch 54/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.3314 - accuracy: 0.8867 - val_loss: 0.2864 - val_accuracy: 0.9007\n",
      "Epoch 55/1000\n",
      "4068/4068 [==============================] - 0s 85us/step - loss: 0.3182 - accuracy: 0.8803 - val_loss: 0.2814 - val_accuracy: 0.8896\n",
      "Epoch 56/1000\n",
      "4068/4068 [==============================] - 0s 89us/step - loss: 0.3540 - accuracy: 0.8783 - val_loss: 0.2686 - val_accuracy: 0.9051\n",
      "Epoch 57/1000\n",
      "4068/4068 [==============================] - 0s 91us/step - loss: 0.3362 - accuracy: 0.8813 - val_loss: 0.2946 - val_accuracy: 0.8896\n",
      "Epoch 58/1000\n",
      "4068/4068 [==============================] - 0s 95us/step - loss: 0.3081 - accuracy: 0.8909 - val_loss: 0.3844 - val_accuracy: 0.8477\n",
      "Epoch 59/1000\n",
      "4068/4068 [==============================] - 0s 72us/step - loss: 0.2871 - accuracy: 0.8889 - val_loss: 0.2682 - val_accuracy: 0.9007\n",
      "Epoch 60/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.3247 - accuracy: 0.8852 - val_loss: 0.2986 - val_accuracy: 0.9007\n",
      "Epoch 61/1000\n",
      "4068/4068 [==============================] - 0s 72us/step - loss: 0.3795 - accuracy: 0.8729 - val_loss: 0.2921 - val_accuracy: 0.8786\n",
      "Epoch 62/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.3080 - accuracy: 0.8869 - val_loss: 0.4255 - val_accuracy: 0.8278\n",
      "Epoch 63/1000\n",
      "4068/4068 [==============================] - 0s 71us/step - loss: 0.2851 - accuracy: 0.8896 - val_loss: 0.2832 - val_accuracy: 0.8808\n",
      "Epoch 64/1000\n",
      "4068/4068 [==============================] - 0s 72us/step - loss: 0.3067 - accuracy: 0.8862 - val_loss: 0.2965 - val_accuracy: 0.8918\n",
      "Epoch 65/1000\n",
      "4068/4068 [==============================] - 0s 73us/step - loss: 0.3446 - accuracy: 0.8857 - val_loss: 0.2921 - val_accuracy: 0.8742\n",
      "Epoch 66/1000\n",
      "4068/4068 [==============================] - 0s 75us/step - loss: 0.3858 - accuracy: 0.8788 - val_loss: 0.2966 - val_accuracy: 0.9051\n",
      "Epoch 67/1000\n",
      "4068/4068 [==============================] - 0s 74us/step - loss: 0.3490 - accuracy: 0.8823 - val_loss: 0.3333 - val_accuracy: 0.8874\n",
      "Epoch 68/1000\n",
      "4068/4068 [==============================] - 0s 91us/step - loss: 0.3008 - accuracy: 0.8872 - val_loss: 0.4391 - val_accuracy: 0.9029\n",
      "Epoch 69/1000\n",
      "4068/4068 [==============================] - 0s 86us/step - loss: 0.3094 - accuracy: 0.8850 - val_loss: 0.2944 - val_accuracy: 0.8852\n",
      "Epoch 70/1000\n",
      "4068/4068 [==============================] - 0s 96us/step - loss: 0.3460 - accuracy: 0.8793 - val_loss: 0.4462 - val_accuracy: 0.8234\n",
      "Epoch 71/1000\n",
      "4068/4068 [==============================] - 0s 97us/step - loss: 0.3517 - accuracy: 0.8840 - val_loss: 0.3529 - val_accuracy: 0.8720\n",
      "Epoch 72/1000\n",
      "4068/4068 [==============================] - 0s 93us/step - loss: 0.2790 - accuracy: 0.8948 - val_loss: 0.3624 - val_accuracy: 0.9029\n",
      "Epoch 73/1000\n",
      "4068/4068 [==============================] - 0s 96us/step - loss: 0.3336 - accuracy: 0.8813 - val_loss: 0.4032 - val_accuracy: 0.8830\n",
      "Epoch 74/1000\n",
      "4068/4068 [==============================] - 0s 94us/step - loss: 0.3518 - accuracy: 0.8835 - val_loss: 0.3072 - val_accuracy: 0.8808\n",
      "Epoch 75/1000\n",
      "4068/4068 [==============================] - 0s 97us/step - loss: 0.3256 - accuracy: 0.8764 - val_loss: 0.4672 - val_accuracy: 0.9007\n",
      "Epoch 76/1000\n",
      "4068/4068 [==============================] - 0s 96us/step - loss: 0.3525 - accuracy: 0.8795 - val_loss: 0.3827 - val_accuracy: 0.9029\n",
      "Epoch 77/1000\n",
      "4068/4068 [==============================] - 0s 96us/step - loss: 0.2816 - accuracy: 0.8975 - val_loss: 0.2968 - val_accuracy: 0.8962\n",
      "Epoch 78/1000\n",
      "4068/4068 [==============================] - 0s 94us/step - loss: 0.3323 - accuracy: 0.8810 - val_loss: 0.2718 - val_accuracy: 0.8940\n",
      "Epoch 79/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.3173 - accuracy: 0.8842 - val_loss: 0.3547 - val_accuracy: 0.8940\n",
      "Epoch 80/1000\n",
      "4068/4068 [==============================] - 0s 84us/step - loss: 0.3705 - accuracy: 0.8791 - val_loss: 0.4054 - val_accuracy: 0.8940\n",
      "Epoch 81/1000\n",
      "4068/4068 [==============================] - 0s 96us/step - loss: 0.3927 - accuracy: 0.8766 - val_loss: 0.5833 - val_accuracy: 0.7947\n",
      "Epoch 82/1000\n",
      "4068/4068 [==============================] - 0s 98us/step - loss: 0.3116 - accuracy: 0.8882 - val_loss: 0.2659 - val_accuracy: 0.9007\n",
      "Epoch 83/1000\n",
      "4068/4068 [==============================] - 0s 92us/step - loss: 0.3148 - accuracy: 0.8840 - val_loss: 0.3355 - val_accuracy: 0.9029\n",
      "Epoch 84/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.3180 - accuracy: 0.8847 - val_loss: 0.3718 - val_accuracy: 0.8587\n",
      "Epoch 85/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.3339 - accuracy: 0.8823 - val_loss: 0.2796 - val_accuracy: 0.8962\n",
      "Epoch 86/1000\n",
      "4068/4068 [==============================] - 0s 97us/step - loss: 0.3069 - accuracy: 0.8872 - val_loss: 0.3264 - val_accuracy: 0.8985\n",
      "Epoch 87/1000\n",
      "4068/4068 [==============================] - 0s 107us/step - loss: 0.3756 - accuracy: 0.8744 - val_loss: 0.4223 - val_accuracy: 0.8389\n",
      "Epoch 88/1000\n",
      "4068/4068 [==============================] - 0s 91us/step - loss: 0.3964 - accuracy: 0.8732 - val_loss: 0.2936 - val_accuracy: 0.8830\n",
      "Epoch 89/1000\n",
      "4068/4068 [==============================] - 0s 98us/step - loss: 0.3567 - accuracy: 0.8832 - val_loss: 0.2702 - val_accuracy: 0.8985\n",
      "Epoch 90/1000\n",
      "4068/4068 [==============================] - 0s 90us/step - loss: 0.3341 - accuracy: 0.8773 - val_loss: 0.4915 - val_accuracy: 0.9029\n",
      "Epoch 91/1000\n",
      "4068/4068 [==============================] - 0s 88us/step - loss: 0.2919 - accuracy: 0.8845 - val_loss: 0.2806 - val_accuracy: 0.8896\n",
      "Epoch 92/1000\n",
      "4068/4068 [==============================] - 0s 83us/step - loss: 0.3123 - accuracy: 0.8889 - val_loss: 0.2844 - val_accuracy: 0.8786\n",
      "Epoch 93/1000\n",
      "4068/4068 [==============================] - 0s 97us/step - loss: 0.3033 - accuracy: 0.8877 - val_loss: 0.4332 - val_accuracy: 0.8962\n",
      "Epoch 94/1000\n",
      "4068/4068 [==============================] - 0s 86us/step - loss: 0.3482 - accuracy: 0.8832 - val_loss: 0.3096 - val_accuracy: 0.8985\n",
      "Epoch 95/1000\n",
      "4068/4068 [==============================] - 0s 96us/step - loss: 0.3710 - accuracy: 0.8729 - val_loss: 0.3416 - val_accuracy: 0.8631\n",
      "Epoch 96/1000\n",
      "4068/4068 [==============================] - 0s 87us/step - loss: 0.3537 - accuracy: 0.8764 - val_loss: 0.2890 - val_accuracy: 0.8962\n",
      "Epoch 97/1000\n",
      "4068/4068 [==============================] - 0s 98us/step - loss: 0.3039 - accuracy: 0.8889 - val_loss: 0.2909 - val_accuracy: 0.9007\n",
      "Epoch 98/1000\n",
      "4068/4068 [==============================] - 0s 111us/step - loss: 0.3144 - accuracy: 0.8884 - val_loss: 0.3745 - val_accuracy: 0.8962\n",
      "Epoch 99/1000\n",
      "4068/4068 [==============================] - 0s 102us/step - loss: 0.3157 - accuracy: 0.8886 - val_loss: 0.3303 - val_accuracy: 0.9007\n",
      "Epoch 100/1000\n",
      "4068/4068 [==============================] - 0s 103us/step - loss: 0.3114 - accuracy: 0.8874 - val_loss: 0.7593 - val_accuracy: 0.7528\n",
      "Epoch 101/1000\n",
      "4068/4068 [==============================] - 0s 105us/step - loss: 0.3422 - accuracy: 0.8803 - val_loss: 0.4336 - val_accuracy: 0.9029\n",
      "Epoch 102/1000\n",
      "4068/4068 [==============================] - 0s 94us/step - loss: 0.3273 - accuracy: 0.8823 - val_loss: 0.4034 - val_accuracy: 0.9029\n",
      "Epoch 103/1000\n",
      "4068/4068 [==============================] - 0s 89us/step - loss: 0.3334 - accuracy: 0.8818 - val_loss: 0.2751 - val_accuracy: 0.8852\n",
      "Epoch 104/1000\n",
      "4068/4068 [==============================] - 0s 92us/step - loss: 0.3260 - accuracy: 0.8854 - val_loss: 0.2891 - val_accuracy: 0.8874\n",
      "Epoch 105/1000\n",
      "4068/4068 [==============================] - 0s 88us/step - loss: 0.2910 - accuracy: 0.8960 - val_loss: 0.3076 - val_accuracy: 0.8874\n",
      "Epoch 106/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.3097 - accuracy: 0.8884 - val_loss: 0.2956 - val_accuracy: 0.8808\n",
      "Epoch 107/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.3287 - accuracy: 0.8840 - val_loss: 0.3207 - val_accuracy: 0.8940\n",
      "Epoch 108/1000\n",
      "4068/4068 [==============================] - 0s 84us/step - loss: 0.2954 - accuracy: 0.8913 - val_loss: 0.4415 - val_accuracy: 0.9051\n",
      "Epoch 109/1000\n",
      "4068/4068 [==============================] - 0s 83us/step - loss: 0.3235 - accuracy: 0.8823 - val_loss: 0.4111 - val_accuracy: 0.9007\n",
      "Epoch 110/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.2922 - accuracy: 0.8864 - val_loss: 0.3065 - val_accuracy: 0.9051\n",
      "Epoch 111/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2973 - accuracy: 0.8869 - val_loss: 0.3751 - val_accuracy: 0.8874\n",
      "Epoch 112/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.3005 - accuracy: 0.8896 - val_loss: 0.4387 - val_accuracy: 0.9007\n",
      "Epoch 113/1000\n",
      "4068/4068 [==============================] - 0s 83us/step - loss: 0.3451 - accuracy: 0.8800 - val_loss: 0.5078 - val_accuracy: 0.9029\n",
      "Epoch 114/1000\n",
      "4068/4068 [==============================] - 0s 83us/step - loss: 0.3293 - accuracy: 0.8832 - val_loss: 0.2874 - val_accuracy: 0.8896\n",
      "Epoch 115/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.3660 - accuracy: 0.8805 - val_loss: 0.3268 - val_accuracy: 0.8543\n",
      "Epoch 116/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.3551 - accuracy: 0.8857 - val_loss: 0.9119 - val_accuracy: 0.9029\n",
      "Epoch 117/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.3582 - accuracy: 0.8781 - val_loss: 0.2960 - val_accuracy: 0.8918\n",
      "Epoch 118/1000\n",
      "4068/4068 [==============================] - 0s 75us/step - loss: 0.3609 - accuracy: 0.8823 - val_loss: 0.3554 - val_accuracy: 0.9007\n",
      "Epoch 119/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2985 - accuracy: 0.8909 - val_loss: 0.3570 - val_accuracy: 0.8477\n",
      "Epoch 120/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2861 - accuracy: 0.8854 - val_loss: 0.3097 - val_accuracy: 0.8985\n",
      "Epoch 121/1000\n",
      "4068/4068 [==============================] - 0s 83us/step - loss: 0.3208 - accuracy: 0.8820 - val_loss: 0.4119 - val_accuracy: 0.8256\n",
      "Epoch 122/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.3497 - accuracy: 0.8869 - val_loss: 0.3136 - val_accuracy: 0.8698\n",
      "Epoch 123/1000\n",
      "4068/4068 [==============================] - 0s 76us/step - loss: 0.3127 - accuracy: 0.8830 - val_loss: 0.4246 - val_accuracy: 0.8256\n",
      "Epoch 124/1000\n",
      "4068/4068 [==============================] - 0s 76us/step - loss: 0.3126 - accuracy: 0.8857 - val_loss: 0.3623 - val_accuracy: 0.8896\n",
      "Epoch 125/1000\n",
      "4068/4068 [==============================] - 0s 82us/step - loss: 0.2930 - accuracy: 0.8916 - val_loss: 0.3017 - val_accuracy: 0.8962\n",
      "Epoch 126/1000\n",
      "4068/4068 [==============================] - 0s 93us/step - loss: 0.3126 - accuracy: 0.8854 - val_loss: 0.3854 - val_accuracy: 0.8411\n",
      "Epoch 127/1000\n",
      "4068/4068 [==============================] - 0s 96us/step - loss: 0.3552 - accuracy: 0.8771 - val_loss: 0.4629 - val_accuracy: 0.8234\n",
      "Epoch 128/1000\n",
      "4068/4068 [==============================] - 0s 87us/step - loss: 0.3344 - accuracy: 0.8827 - val_loss: 0.4443 - val_accuracy: 0.8985\n",
      "Epoch 129/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.3304 - accuracy: 0.8847 - val_loss: 0.3034 - val_accuracy: 0.8653\n",
      "Epoch 130/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2989 - accuracy: 0.8899 - val_loss: 0.3257 - val_accuracy: 0.8985\n",
      "Epoch 131/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.3412 - accuracy: 0.8805 - val_loss: 0.2839 - val_accuracy: 0.8852\n",
      "Epoch 132/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.2862 - accuracy: 0.8911 - val_loss: 0.3098 - val_accuracy: 0.8698\n",
      "Epoch 133/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.3109 - accuracy: 0.8874 - val_loss: 0.2716 - val_accuracy: 0.8874\n",
      "Epoch 134/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2874 - accuracy: 0.8913 - val_loss: 0.2660 - val_accuracy: 0.8918\n",
      "Epoch 135/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2866 - accuracy: 0.8882 - val_loss: 0.2825 - val_accuracy: 0.8985\n",
      "Epoch 136/1000\n",
      "4068/4068 [==============================] - 0s 76us/step - loss: 0.3234 - accuracy: 0.8830 - val_loss: 0.2771 - val_accuracy: 0.8852\n",
      "Epoch 137/1000\n",
      "4068/4068 [==============================] - 0s 76us/step - loss: 0.3048 - accuracy: 0.8872 - val_loss: 0.3461 - val_accuracy: 0.8631\n",
      "Epoch 138/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2785 - accuracy: 0.8894 - val_loss: 0.3067 - val_accuracy: 0.8962\n",
      "Epoch 139/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.3242 - accuracy: 0.8886 - val_loss: 0.5175 - val_accuracy: 0.7881\n",
      "Epoch 140/1000\n",
      "4068/4068 [==============================] - ETA: 0s - loss: 0.3208 - accuracy: 0.88 - 0s 77us/step - loss: 0.3249 - accuracy: 0.8854 - val_loss: 0.3264 - val_accuracy: 0.8720\n",
      "Epoch 141/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.3105 - accuracy: 0.8872 - val_loss: 0.4499 - val_accuracy: 0.8962\n",
      "Epoch 142/1000\n",
      "4068/4068 [==============================] - 0s 76us/step - loss: 0.2845 - accuracy: 0.8896 - val_loss: 0.2878 - val_accuracy: 0.9029\n",
      "Epoch 143/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.3268 - accuracy: 0.8864 - val_loss: 0.2803 - val_accuracy: 0.9007\n",
      "Epoch 144/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.3109 - accuracy: 0.8810 - val_loss: 0.3574 - val_accuracy: 0.8609\n",
      "Epoch 145/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.3030 - accuracy: 0.8879 - val_loss: 0.2956 - val_accuracy: 0.8786\n",
      "Epoch 146/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2881 - accuracy: 0.8867 - val_loss: 0.2888 - val_accuracy: 0.8874\n",
      "Epoch 147/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2810 - accuracy: 0.8938 - val_loss: 0.3159 - val_accuracy: 0.8631\n",
      "Epoch 148/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2972 - accuracy: 0.8874 - val_loss: 0.2951 - val_accuracy: 0.8742\n",
      "Epoch 149/1000\n",
      "4068/4068 [==============================] - ETA: 0s - loss: 0.3418 - accuracy: 0.88 - 0s 75us/step - loss: 0.3322 - accuracy: 0.8827 - val_loss: 0.5255 - val_accuracy: 0.7947\n",
      "Epoch 150/1000\n",
      "4068/4068 [==============================] - 0s 74us/step - loss: 0.3070 - accuracy: 0.8867 - val_loss: 0.3884 - val_accuracy: 0.8543\n",
      "Epoch 151/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.3176 - accuracy: 0.8837 - val_loss: 0.3558 - val_accuracy: 0.8455\n",
      "Epoch 152/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2952 - accuracy: 0.8886 - val_loss: 0.3296 - val_accuracy: 0.8742\n",
      "Epoch 153/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2886 - accuracy: 0.8906 - val_loss: 0.3954 - val_accuracy: 0.8389\n",
      "Epoch 154/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.3230 - accuracy: 0.8886 - val_loss: 0.3130 - val_accuracy: 0.8962\n",
      "Epoch 155/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2904 - accuracy: 0.8906 - val_loss: 0.2950 - val_accuracy: 0.8808\n",
      "Epoch 156/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.3503 - accuracy: 0.8835 - val_loss: 0.4198 - val_accuracy: 0.9007\n",
      "Epoch 157/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.3429 - accuracy: 0.8840 - val_loss: 0.4886 - val_accuracy: 0.8985\n",
      "Epoch 158/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.3462 - accuracy: 0.8854 - val_loss: 0.3029 - val_accuracy: 0.8962\n",
      "Epoch 159/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2931 - accuracy: 0.8906 - val_loss: 0.3785 - val_accuracy: 0.9051\n",
      "Epoch 160/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.3995 - accuracy: 0.8786 - val_loss: 0.3150 - val_accuracy: 0.8786\n",
      "Epoch 161/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.3192 - accuracy: 0.8810 - val_loss: 0.3009 - val_accuracy: 0.8874\n",
      "Epoch 162/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.3524 - accuracy: 0.8837 - val_loss: 0.4716 - val_accuracy: 0.8124\n",
      "Epoch 163/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.3183 - accuracy: 0.8845 - val_loss: 0.2860 - val_accuracy: 0.8940\n",
      "Epoch 164/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.3020 - accuracy: 0.8862 - val_loss: 0.4330 - val_accuracy: 0.8300\n",
      "Epoch 165/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.3065 - accuracy: 0.8953 - val_loss: 0.2815 - val_accuracy: 0.8808\n",
      "Epoch 166/1000\n",
      "4068/4068 [==============================] - 0s 76us/step - loss: 0.3104 - accuracy: 0.8899 - val_loss: 0.3569 - val_accuracy: 0.8543\n",
      "Epoch 167/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.3073 - accuracy: 0.8818 - val_loss: 0.5091 - val_accuracy: 0.9029\n",
      "Epoch 168/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.4046 - accuracy: 0.8754 - val_loss: 0.4034 - val_accuracy: 0.9051\n",
      "Epoch 169/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2637 - accuracy: 0.8921 - val_loss: 0.3008 - val_accuracy: 0.8742\n",
      "Epoch 170/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.3011 - accuracy: 0.8909 - val_loss: 0.4034 - val_accuracy: 0.8411\n",
      "Epoch 171/1000\n",
      "4068/4068 [==============================] - 0s 75us/step - loss: 0.3000 - accuracy: 0.8909 - val_loss: 0.2949 - val_accuracy: 0.8896\n",
      "Epoch 172/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2824 - accuracy: 0.8884 - val_loss: 0.3518 - val_accuracy: 0.8499\n",
      "Epoch 173/1000\n",
      "4068/4068 [==============================] - 0s 76us/step - loss: 0.2993 - accuracy: 0.8854 - val_loss: 0.3383 - val_accuracy: 0.8896\n",
      "Epoch 174/1000\n",
      "4068/4068 [==============================] - 0s 76us/step - loss: 0.2909 - accuracy: 0.8931 - val_loss: 0.4080 - val_accuracy: 0.8300\n",
      "Epoch 175/1000\n",
      "4068/4068 [==============================] - 0s 89us/step - loss: 0.3128 - accuracy: 0.8850 - val_loss: 0.5527 - val_accuracy: 0.9051\n",
      "Epoch 176/1000\n",
      "4068/4068 [==============================] - 0s 96us/step - loss: 0.3273 - accuracy: 0.8835 - val_loss: 0.3257 - val_accuracy: 0.8698\n",
      "Epoch 177/1000\n",
      "4068/4068 [==============================] - 0s 93us/step - loss: 0.3058 - accuracy: 0.8921 - val_loss: 0.5227 - val_accuracy: 0.7837\n",
      "Epoch 178/1000\n",
      "4068/4068 [==============================] - 0s 83us/step - loss: 0.3017 - accuracy: 0.8877 - val_loss: 0.3961 - val_accuracy: 0.8985\n",
      "Epoch 179/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2909 - accuracy: 0.8948 - val_loss: 0.2874 - val_accuracy: 0.8742\n",
      "Epoch 180/1000\n",
      "4068/4068 [==============================] - 0s 76us/step - loss: 0.3362 - accuracy: 0.8808 - val_loss: 0.3699 - val_accuracy: 0.8543\n",
      "Epoch 181/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2945 - accuracy: 0.8913 - val_loss: 0.3548 - val_accuracy: 0.8985\n",
      "Epoch 182/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2932 - accuracy: 0.8899 - val_loss: 0.3223 - val_accuracy: 0.8742\n",
      "Epoch 183/1000\n",
      "4068/4068 [==============================] - 0s 75us/step - loss: 0.3739 - accuracy: 0.8810 - val_loss: 0.3024 - val_accuracy: 0.8962\n",
      "Epoch 184/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2919 - accuracy: 0.8921 - val_loss: 0.3110 - val_accuracy: 0.8786\n",
      "Epoch 185/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.3094 - accuracy: 0.8818 - val_loss: 0.2913 - val_accuracy: 0.8940\n",
      "Epoch 186/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2868 - accuracy: 0.8879 - val_loss: 0.4714 - val_accuracy: 0.9029\n",
      "Epoch 187/1000\n",
      "4068/4068 [==============================] - 0s 75us/step - loss: 0.3491 - accuracy: 0.8825 - val_loss: 0.3242 - val_accuracy: 0.8830\n",
      "Epoch 188/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2775 - accuracy: 0.8904 - val_loss: 0.2883 - val_accuracy: 0.8874\n",
      "Epoch 189/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.3059 - accuracy: 0.8891 - val_loss: 0.2915 - val_accuracy: 0.8940\n",
      "Epoch 190/1000\n",
      "4068/4068 [==============================] - 0s 76us/step - loss: 0.3490 - accuracy: 0.8827 - val_loss: 0.5349 - val_accuracy: 0.8057\n",
      "Epoch 191/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.3374 - accuracy: 0.8783 - val_loss: 0.3015 - val_accuracy: 0.8940\n",
      "Epoch 192/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2910 - accuracy: 0.8884 - val_loss: 0.3037 - val_accuracy: 0.8852\n",
      "Epoch 193/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.3053 - accuracy: 0.8894 - val_loss: 0.3091 - val_accuracy: 0.8962\n",
      "Epoch 194/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.3209 - accuracy: 0.8901 - val_loss: 0.3610 - val_accuracy: 0.9007\n",
      "Epoch 195/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2702 - accuracy: 0.8938 - val_loss: 0.3051 - val_accuracy: 0.8874\n",
      "Epoch 196/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2983 - accuracy: 0.8869 - val_loss: 0.3812 - val_accuracy: 0.9029\n",
      "Epoch 197/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2738 - accuracy: 0.8891 - val_loss: 0.4194 - val_accuracy: 0.9007\n",
      "Epoch 198/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2939 - accuracy: 0.8901 - val_loss: 0.3736 - val_accuracy: 0.8985\n",
      "Epoch 199/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.3411 - accuracy: 0.8813 - val_loss: 0.3109 - val_accuracy: 0.8786\n",
      "Epoch 200/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.3117 - accuracy: 0.8894 - val_loss: 0.2833 - val_accuracy: 0.8962\n",
      "Epoch 201/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2749 - accuracy: 0.8913 - val_loss: 0.2900 - val_accuracy: 0.8698\n",
      "Epoch 202/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2859 - accuracy: 0.8913 - val_loss: 0.3437 - val_accuracy: 0.8543\n",
      "Epoch 203/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2777 - accuracy: 0.8943 - val_loss: 0.2903 - val_accuracy: 0.8786\n",
      "Epoch 204/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.2994 - accuracy: 0.8923 - val_loss: 0.2936 - val_accuracy: 0.8852\n",
      "Epoch 205/1000\n",
      "4068/4068 [==============================] - 0s 82us/step - loss: 0.2989 - accuracy: 0.8913 - val_loss: 0.3288 - val_accuracy: 0.8764\n",
      "Epoch 206/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.3342 - accuracy: 0.8845 - val_loss: 0.4407 - val_accuracy: 0.8256\n",
      "Epoch 207/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2918 - accuracy: 0.8825 - val_loss: 0.3932 - val_accuracy: 0.8587\n",
      "Epoch 208/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.3277 - accuracy: 0.8889 - val_loss: 0.4159 - val_accuracy: 0.8389\n",
      "Epoch 209/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2956 - accuracy: 0.8867 - val_loss: 0.3181 - val_accuracy: 0.8609\n",
      "Epoch 210/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.3342 - accuracy: 0.8818 - val_loss: 0.3215 - val_accuracy: 0.8742\n",
      "Epoch 211/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2813 - accuracy: 0.8899 - val_loss: 0.3115 - val_accuracy: 0.8786\n",
      "Epoch 212/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2992 - accuracy: 0.8884 - val_loss: 0.3414 - val_accuracy: 0.8698\n",
      "Epoch 213/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.3138 - accuracy: 0.8896 - val_loss: 0.5213 - val_accuracy: 0.7925\n",
      "Epoch 214/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.3030 - accuracy: 0.8884 - val_loss: 0.3753 - val_accuracy: 0.8852\n",
      "Epoch 215/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2987 - accuracy: 0.8889 - val_loss: 0.3644 - val_accuracy: 0.8830\n",
      "Epoch 216/1000\n",
      "4068/4068 [==============================] - 0s 82us/step - loss: 0.2992 - accuracy: 0.8926 - val_loss: 0.3428 - val_accuracy: 0.9051\n",
      "Epoch 217/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2731 - accuracy: 0.8980 - val_loss: 0.3373 - val_accuracy: 0.8962\n",
      "Epoch 218/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2839 - accuracy: 0.8921 - val_loss: 0.2940 - val_accuracy: 0.9007\n",
      "Epoch 219/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.3069 - accuracy: 0.8795 - val_loss: 0.4564 - val_accuracy: 0.8962\n",
      "Epoch 220/1000\n",
      "4068/4068 [==============================] - 0s 76us/step - loss: 0.3042 - accuracy: 0.8899 - val_loss: 0.3469 - val_accuracy: 0.8764\n",
      "Epoch 221/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2765 - accuracy: 0.8958 - val_loss: 0.3252 - val_accuracy: 0.8698\n",
      "Epoch 222/1000\n",
      "4068/4068 [==============================] - 0s 74us/step - loss: 0.2821 - accuracy: 0.8918 - val_loss: 0.3136 - val_accuracy: 0.8874\n",
      "Epoch 223/1000\n",
      "4068/4068 [==============================] - 0s 74us/step - loss: 0.2833 - accuracy: 0.8889 - val_loss: 0.3052 - val_accuracy: 0.8631\n",
      "Epoch 224/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2819 - accuracy: 0.8938 - val_loss: 0.3364 - val_accuracy: 0.8631\n",
      "Epoch 225/1000\n",
      "4068/4068 [==============================] - 0s 95us/step - loss: 0.2849 - accuracy: 0.8953 - val_loss: 0.2699 - val_accuracy: 0.8896\n",
      "Epoch 226/1000\n",
      "4068/4068 [==============================] - 0s 94us/step - loss: 0.2852 - accuracy: 0.8852 - val_loss: 0.2871 - val_accuracy: 0.8896\n",
      "Epoch 227/1000\n",
      "4068/4068 [==============================] - 0s 91us/step - loss: 0.3138 - accuracy: 0.8879 - val_loss: 0.3115 - val_accuracy: 0.8698\n",
      "Epoch 228/1000\n",
      "4068/4068 [==============================] - 0s 83us/step - loss: 0.2999 - accuracy: 0.8810 - val_loss: 0.3881 - val_accuracy: 0.8344\n",
      "Epoch 229/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2865 - accuracy: 0.8886 - val_loss: 0.3450 - val_accuracy: 0.8962\n",
      "Epoch 230/1000\n",
      "4068/4068 [==============================] - 0s 75us/step - loss: 0.2700 - accuracy: 0.8918 - val_loss: 0.3134 - val_accuracy: 0.9029\n",
      "Epoch 231/1000\n",
      "4068/4068 [==============================] - 0s 75us/step - loss: 0.3017 - accuracy: 0.8837 - val_loss: 0.3614 - val_accuracy: 0.9007\n",
      "Epoch 232/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.3208 - accuracy: 0.8864 - val_loss: 0.3036 - val_accuracy: 0.8962\n",
      "Epoch 233/1000\n",
      "4068/4068 [==============================] - 0s 76us/step - loss: 0.3150 - accuracy: 0.8847 - val_loss: 0.3675 - val_accuracy: 0.8433\n",
      "Epoch 234/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.3016 - accuracy: 0.8958 - val_loss: 0.3269 - val_accuracy: 0.8653\n",
      "Epoch 235/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2793 - accuracy: 0.8901 - val_loss: 0.3088 - val_accuracy: 0.8808\n",
      "Epoch 236/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.3308 - accuracy: 0.8830 - val_loss: 0.2846 - val_accuracy: 0.8808\n",
      "Epoch 237/1000\n",
      "4068/4068 [==============================] - 0s 75us/step - loss: 0.2773 - accuracy: 0.8980 - val_loss: 0.3246 - val_accuracy: 0.8896\n",
      "Epoch 238/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2763 - accuracy: 0.8904 - val_loss: 0.5046 - val_accuracy: 0.9051\n",
      "Epoch 239/1000\n",
      "4068/4068 [==============================] - 0s 76us/step - loss: 0.3056 - accuracy: 0.8837 - val_loss: 0.3164 - val_accuracy: 0.8764\n",
      "Epoch 240/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2801 - accuracy: 0.8928 - val_loss: 0.3444 - val_accuracy: 0.9007\n",
      "Epoch 241/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2867 - accuracy: 0.8963 - val_loss: 0.3556 - val_accuracy: 0.8565\n",
      "Epoch 242/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2942 - accuracy: 0.8879 - val_loss: 0.3206 - val_accuracy: 0.8808\n",
      "Epoch 243/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2666 - accuracy: 0.8975 - val_loss: 0.4860 - val_accuracy: 0.8102\n",
      "Epoch 244/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2770 - accuracy: 0.8943 - val_loss: 0.2784 - val_accuracy: 0.9051\n",
      "Epoch 245/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.3421 - accuracy: 0.8813 - val_loss: 0.3124 - val_accuracy: 0.8653\n",
      "Epoch 246/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2967 - accuracy: 0.8891 - val_loss: 0.3503 - val_accuracy: 0.8985\n",
      "Epoch 247/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.3291 - accuracy: 0.8850 - val_loss: 0.2978 - val_accuracy: 0.8852\n",
      "Epoch 248/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.3424 - accuracy: 0.8859 - val_loss: 0.3113 - val_accuracy: 0.8852\n",
      "Epoch 249/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2991 - accuracy: 0.8916 - val_loss: 0.3476 - val_accuracy: 0.8631\n",
      "Epoch 250/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2651 - accuracy: 0.8980 - val_loss: 0.3254 - val_accuracy: 0.8587\n",
      "Epoch 251/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2963 - accuracy: 0.8864 - val_loss: 0.3555 - val_accuracy: 0.9007\n",
      "Epoch 252/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2582 - accuracy: 0.8923 - val_loss: 0.4088 - val_accuracy: 0.9051\n",
      "Epoch 253/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.3023 - accuracy: 0.8889 - val_loss: 0.3419 - val_accuracy: 0.8808\n",
      "Epoch 254/1000\n",
      "4068/4068 [==============================] - 0s 76us/step - loss: 0.2962 - accuracy: 0.8945 - val_loss: 0.3484 - val_accuracy: 0.8521\n",
      "Epoch 255/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2775 - accuracy: 0.8916 - val_loss: 0.3342 - val_accuracy: 0.8543\n",
      "Epoch 256/1000\n",
      "4068/4068 [==============================] - 0s 76us/step - loss: 0.2705 - accuracy: 0.8965 - val_loss: 0.4144 - val_accuracy: 0.8499\n",
      "Epoch 257/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.2777 - accuracy: 0.8904 - val_loss: 0.4267 - val_accuracy: 0.8985\n",
      "Epoch 258/1000\n",
      "4068/4068 [==============================] - 0s 76us/step - loss: 0.3274 - accuracy: 0.8813 - val_loss: 0.3193 - val_accuracy: 0.8896\n",
      "Epoch 259/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2772 - accuracy: 0.8923 - val_loss: 0.3487 - val_accuracy: 0.8896\n",
      "Epoch 260/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2967 - accuracy: 0.8921 - val_loss: 0.3046 - val_accuracy: 0.8808\n",
      "Epoch 261/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2747 - accuracy: 0.8933 - val_loss: 0.4371 - val_accuracy: 0.8146\n",
      "Epoch 262/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2831 - accuracy: 0.8941 - val_loss: 0.3100 - val_accuracy: 0.8874\n",
      "Epoch 263/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2877 - accuracy: 0.8886 - val_loss: 0.5956 - val_accuracy: 0.9007\n",
      "Epoch 264/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2928 - accuracy: 0.8911 - val_loss: 0.2875 - val_accuracy: 0.8918\n",
      "Epoch 265/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2631 - accuracy: 0.8982 - val_loss: 0.3237 - val_accuracy: 0.8653\n",
      "Epoch 266/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2762 - accuracy: 0.8938 - val_loss: 0.4752 - val_accuracy: 0.8962\n",
      "Epoch 267/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.3090 - accuracy: 0.8859 - val_loss: 0.3003 - val_accuracy: 0.8653\n",
      "Epoch 268/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2862 - accuracy: 0.8955 - val_loss: 0.2882 - val_accuracy: 0.8786\n",
      "Epoch 269/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.3316 - accuracy: 0.8827 - val_loss: 0.3700 - val_accuracy: 0.9007\n",
      "Epoch 270/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2939 - accuracy: 0.8886 - val_loss: 0.3502 - val_accuracy: 0.8985\n",
      "Epoch 271/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2602 - accuracy: 0.8931 - val_loss: 0.2843 - val_accuracy: 0.8808\n",
      "Epoch 272/1000\n",
      "4068/4068 [==============================] - 0s 83us/step - loss: 0.2892 - accuracy: 0.8955 - val_loss: 0.2878 - val_accuracy: 0.8852\n",
      "Epoch 273/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2497 - accuracy: 0.8945 - val_loss: 0.2984 - val_accuracy: 0.8786\n",
      "Epoch 274/1000\n",
      "4068/4068 [==============================] - 0s 84us/step - loss: 0.2737 - accuracy: 0.8972 - val_loss: 0.3027 - val_accuracy: 0.8852\n",
      "Epoch 275/1000\n",
      "4068/4068 [==============================] - 0s 95us/step - loss: 0.2659 - accuracy: 0.8992 - val_loss: 0.2863 - val_accuracy: 0.8808\n",
      "Epoch 276/1000\n",
      "4068/4068 [==============================] - 0s 95us/step - loss: 0.2879 - accuracy: 0.8882 - val_loss: 0.3659 - val_accuracy: 0.8940\n",
      "Epoch 277/1000\n",
      "4068/4068 [==============================] - 0s 86us/step - loss: 0.2874 - accuracy: 0.8901 - val_loss: 0.3072 - val_accuracy: 0.8874\n",
      "Epoch 278/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.2942 - accuracy: 0.8906 - val_loss: 0.2836 - val_accuracy: 0.8808\n",
      "Epoch 279/1000\n",
      "4068/4068 [==============================] - 0s 76us/step - loss: 0.2620 - accuracy: 0.8982 - val_loss: 0.2904 - val_accuracy: 0.8653\n",
      "Epoch 280/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2735 - accuracy: 0.8968 - val_loss: 0.2975 - val_accuracy: 0.8830\n",
      "Epoch 281/1000\n",
      "4068/4068 [==============================] - 0s 75us/step - loss: 0.2910 - accuracy: 0.8884 - val_loss: 0.3620 - val_accuracy: 0.8720\n",
      "Epoch 282/1000\n",
      "4068/4068 [==============================] - 0s 75us/step - loss: 0.2736 - accuracy: 0.8938 - val_loss: 0.3615 - val_accuracy: 0.8477\n",
      "Epoch 283/1000\n",
      "4068/4068 [==============================] - 0s 75us/step - loss: 0.2801 - accuracy: 0.8918 - val_loss: 0.3093 - val_accuracy: 0.8940\n",
      "Epoch 284/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.3023 - accuracy: 0.8913 - val_loss: 0.3319 - val_accuracy: 0.8720\n",
      "Epoch 285/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.3022 - accuracy: 0.8916 - val_loss: 0.3595 - val_accuracy: 0.9029\n",
      "Epoch 286/1000\n",
      "4068/4068 [==============================] - 0s 75us/step - loss: 0.2887 - accuracy: 0.8916 - val_loss: 0.4079 - val_accuracy: 0.9029\n",
      "Epoch 287/1000\n",
      "4068/4068 [==============================] - 0s 76us/step - loss: 0.2997 - accuracy: 0.8891 - val_loss: 0.3019 - val_accuracy: 0.8852\n",
      "Epoch 288/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2676 - accuracy: 0.8958 - val_loss: 0.2790 - val_accuracy: 0.8940\n",
      "Epoch 289/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2575 - accuracy: 0.8975 - val_loss: 0.3669 - val_accuracy: 0.8587\n",
      "Epoch 290/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2811 - accuracy: 0.8891 - val_loss: 0.3042 - val_accuracy: 0.8896\n",
      "Epoch 291/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.3233 - accuracy: 0.8852 - val_loss: 0.4294 - val_accuracy: 0.8212\n",
      "Epoch 292/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2664 - accuracy: 0.8945 - val_loss: 0.2880 - val_accuracy: 0.8918\n",
      "Epoch 293/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.3179 - accuracy: 0.8886 - val_loss: 0.3594 - val_accuracy: 0.8742\n",
      "Epoch 294/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.3122 - accuracy: 0.8886 - val_loss: 0.4150 - val_accuracy: 0.8874\n",
      "Epoch 295/1000\n",
      "4068/4068 [==============================] - ETA: 0s - loss: 0.2640 - accuracy: 0.89 - 0s 76us/step - loss: 0.2628 - accuracy: 0.8970 - val_loss: 0.4203 - val_accuracy: 0.8985\n",
      "Epoch 296/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2788 - accuracy: 0.8933 - val_loss: 0.3000 - val_accuracy: 0.8962\n",
      "Epoch 297/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2820 - accuracy: 0.8916 - val_loss: 0.3103 - val_accuracy: 0.9051\n",
      "Epoch 298/1000\n",
      "4068/4068 [==============================] - ETA: 0s - loss: 0.2734 - accuracy: 0.89 - 0s 76us/step - loss: 0.2733 - accuracy: 0.8963 - val_loss: 0.3050 - val_accuracy: 0.9051\n",
      "Epoch 299/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2908 - accuracy: 0.8958 - val_loss: 0.4311 - val_accuracy: 0.8985\n",
      "Epoch 300/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2901 - accuracy: 0.8918 - val_loss: 0.2916 - val_accuracy: 0.8962\n",
      "Epoch 301/1000\n",
      "4068/4068 [==============================] - 0s 76us/step - loss: 0.3464 - accuracy: 0.8879 - val_loss: 0.2966 - val_accuracy: 0.8918\n",
      "Epoch 302/1000\n",
      "4068/4068 [==============================] - 0s 76us/step - loss: 0.2867 - accuracy: 0.8874 - val_loss: 0.3567 - val_accuracy: 0.8940\n",
      "Epoch 303/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.2796 - accuracy: 0.8943 - val_loss: 0.3605 - val_accuracy: 0.8653\n",
      "Epoch 304/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2916 - accuracy: 0.8911 - val_loss: 0.3317 - val_accuracy: 0.8653\n",
      "Epoch 305/1000\n",
      "4068/4068 [==============================] - 0s 95us/step - loss: 0.2892 - accuracy: 0.8911 - val_loss: 0.3691 - val_accuracy: 0.8830\n",
      "Epoch 306/1000\n",
      "4068/4068 [==============================] - 0s 103us/step - loss: 0.2783 - accuracy: 0.8938 - val_loss: 0.3500 - val_accuracy: 0.8455\n",
      "Epoch 307/1000\n",
      "4068/4068 [==============================] - 0s 111us/step - loss: 0.2705 - accuracy: 0.8975 - val_loss: 0.3505 - val_accuracy: 0.8455\n",
      "Epoch 308/1000\n",
      "4068/4068 [==============================] - 0s 111us/step - loss: 0.2698 - accuracy: 0.8899 - val_loss: 0.3540 - val_accuracy: 0.9029\n",
      "Epoch 309/1000\n",
      "4068/4068 [==============================] - 0s 100us/step - loss: 0.2783 - accuracy: 0.8882 - val_loss: 0.3408 - val_accuracy: 0.8477\n",
      "Epoch 310/1000\n",
      "4068/4068 [==============================] - 0s 106us/step - loss: 0.2719 - accuracy: 0.8904 - val_loss: 0.4037 - val_accuracy: 0.8234\n",
      "Epoch 311/1000\n",
      "4068/4068 [==============================] - 0s 107us/step - loss: 0.2812 - accuracy: 0.8891 - val_loss: 0.3009 - val_accuracy: 0.9029\n",
      "Epoch 312/1000\n",
      "4068/4068 [==============================] - 0s 110us/step - loss: 0.3077 - accuracy: 0.8958 - val_loss: 0.2995 - val_accuracy: 0.8985\n",
      "Epoch 313/1000\n",
      "4068/4068 [==============================] - 0s 103us/step - loss: 0.2823 - accuracy: 0.8972 - val_loss: 0.2838 - val_accuracy: 0.8852\n",
      "Epoch 314/1000\n",
      "4068/4068 [==============================] - 0s 89us/step - loss: 0.2860 - accuracy: 0.8877 - val_loss: 0.3026 - val_accuracy: 0.8918\n",
      "Epoch 315/1000\n",
      "4068/4068 [==============================] - 0s 83us/step - loss: 0.2692 - accuracy: 0.8950 - val_loss: 0.3080 - val_accuracy: 0.8852\n",
      "Epoch 316/1000\n",
      "4068/4068 [==============================] - 0s 86us/step - loss: 0.2651 - accuracy: 0.9002 - val_loss: 0.2978 - val_accuracy: 0.8764\n",
      "Epoch 317/1000\n",
      "4068/4068 [==============================] - 0s 82us/step - loss: 0.2760 - accuracy: 0.8980 - val_loss: 0.3976 - val_accuracy: 0.8698\n",
      "Epoch 318/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2788 - accuracy: 0.8936 - val_loss: 0.3621 - val_accuracy: 0.9007\n",
      "Epoch 319/1000\n",
      "4068/4068 [==============================] - 0s 82us/step - loss: 0.2799 - accuracy: 0.8909 - val_loss: 0.3039 - val_accuracy: 0.8896\n",
      "Epoch 320/1000\n",
      "4068/4068 [==============================] - 0s 87us/step - loss: 0.2866 - accuracy: 0.8941 - val_loss: 0.3145 - val_accuracy: 0.8962\n",
      "Epoch 321/1000\n",
      "4068/4068 [==============================] - 0s 102us/step - loss: 0.2957 - accuracy: 0.8891 - val_loss: 0.3715 - val_accuracy: 0.8962\n",
      "Epoch 322/1000\n",
      "4068/4068 [==============================] - 0s 100us/step - loss: 0.2675 - accuracy: 0.8955 - val_loss: 0.2888 - val_accuracy: 0.8918\n",
      "Epoch 323/1000\n",
      "4068/4068 [==============================] - 0s 91us/step - loss: 0.2682 - accuracy: 0.9000 - val_loss: 0.3007 - val_accuracy: 0.8698\n",
      "Epoch 324/1000\n",
      "4068/4068 [==============================] - 0s 85us/step - loss: 0.3048 - accuracy: 0.8884 - val_loss: 0.4482 - val_accuracy: 0.8962\n",
      "Epoch 325/1000\n",
      "4068/4068 [==============================] - 0s 83us/step - loss: 0.2640 - accuracy: 0.9002 - val_loss: 0.3235 - val_accuracy: 0.8985\n",
      "Epoch 326/1000\n",
      "4068/4068 [==============================] - 0s 82us/step - loss: 0.2623 - accuracy: 0.9012 - val_loss: 0.3612 - val_accuracy: 0.9007\n",
      "Epoch 327/1000\n",
      "4068/4068 [==============================] - 0s 101us/step - loss: 0.2497 - accuracy: 0.9017 - val_loss: 0.3440 - val_accuracy: 0.8499\n",
      "Epoch 328/1000\n",
      "4068/4068 [==============================] - 0s 107us/step - loss: 0.2895 - accuracy: 0.8909 - val_loss: 0.2742 - val_accuracy: 0.8874\n",
      "Epoch 329/1000\n",
      "4068/4068 [==============================] - 0s 93us/step - loss: 0.2831 - accuracy: 0.8960 - val_loss: 0.2870 - val_accuracy: 0.8830\n",
      "Epoch 330/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.2753 - accuracy: 0.8970 - val_loss: 0.3466 - val_accuracy: 0.9007\n",
      "Epoch 331/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2707 - accuracy: 0.8950 - val_loss: 0.2969 - val_accuracy: 0.8764\n",
      "Epoch 332/1000\n",
      "4068/4068 [==============================] - 0s 82us/step - loss: 0.2540 - accuracy: 0.8970 - val_loss: 0.3051 - val_accuracy: 0.8852\n",
      "Epoch 333/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.2525 - accuracy: 0.8990 - val_loss: 0.2951 - val_accuracy: 0.8962\n",
      "Epoch 334/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2462 - accuracy: 0.9022 - val_loss: 0.3463 - val_accuracy: 0.8675\n",
      "Epoch 335/1000\n",
      "4068/4068 [==============================] - 0s 82us/step - loss: 0.2848 - accuracy: 0.8931 - val_loss: 0.2925 - val_accuracy: 0.8830\n",
      "Epoch 336/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.2743 - accuracy: 0.8985 - val_loss: 0.2920 - val_accuracy: 0.8962\n",
      "Epoch 337/1000\n",
      "4068/4068 [==============================] - 0s 83us/step - loss: 0.2660 - accuracy: 0.9031 - val_loss: 0.3588 - val_accuracy: 0.8543\n",
      "Epoch 338/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2625 - accuracy: 0.8965 - val_loss: 0.2811 - val_accuracy: 0.8852\n",
      "Epoch 339/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.2952 - accuracy: 0.8921 - val_loss: 0.3543 - val_accuracy: 0.8521\n",
      "Epoch 340/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.2788 - accuracy: 0.8970 - val_loss: 0.2739 - val_accuracy: 0.8896\n",
      "Epoch 341/1000\n",
      "4068/4068 [==============================] - 0s 89us/step - loss: 0.2683 - accuracy: 0.8933 - val_loss: 0.4259 - val_accuracy: 0.8322\n",
      "Epoch 342/1000\n",
      "4068/4068 [==============================] - 0s 82us/step - loss: 0.2656 - accuracy: 0.8963 - val_loss: 0.2844 - val_accuracy: 0.8830\n",
      "Epoch 343/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2761 - accuracy: 0.8913 - val_loss: 0.3146 - val_accuracy: 0.8764\n",
      "Epoch 344/1000\n",
      "4068/4068 [==============================] - 0s 82us/step - loss: 0.2832 - accuracy: 0.8923 - val_loss: 0.4635 - val_accuracy: 0.8256\n",
      "Epoch 345/1000\n",
      "4068/4068 [==============================] - 0s 84us/step - loss: 0.3036 - accuracy: 0.8882 - val_loss: 0.3209 - val_accuracy: 0.8874\n",
      "Epoch 346/1000\n",
      "4068/4068 [==============================] - 0s 83us/step - loss: 0.2733 - accuracy: 0.8992 - val_loss: 0.3005 - val_accuracy: 0.8852\n",
      "Epoch 347/1000\n",
      "4068/4068 [==============================] - 0s 85us/step - loss: 0.2773 - accuracy: 0.8938 - val_loss: 0.3922 - val_accuracy: 0.8278\n",
      "Epoch 348/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2824 - accuracy: 0.8938 - val_loss: 0.2800 - val_accuracy: 0.8985\n",
      "Epoch 349/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2806 - accuracy: 0.8945 - val_loss: 0.5988 - val_accuracy: 0.8962\n",
      "Epoch 350/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2721 - accuracy: 0.8972 - val_loss: 0.3082 - val_accuracy: 0.8786\n",
      "Epoch 351/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.2716 - accuracy: 0.8975 - val_loss: 0.3366 - val_accuracy: 0.9117\n",
      "Epoch 352/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2535 - accuracy: 0.9012 - val_loss: 0.3099 - val_accuracy: 0.8808\n",
      "Epoch 353/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2761 - accuracy: 0.8970 - val_loss: 0.2989 - val_accuracy: 0.8962\n",
      "Epoch 354/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2597 - accuracy: 0.9022 - val_loss: 0.3176 - val_accuracy: 0.8985\n",
      "Epoch 355/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2648 - accuracy: 0.9014 - val_loss: 0.4651 - val_accuracy: 0.8190\n",
      "Epoch 356/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2576 - accuracy: 0.8965 - val_loss: 0.3189 - val_accuracy: 0.8852\n",
      "Epoch 357/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2586 - accuracy: 0.9002 - val_loss: 0.2982 - val_accuracy: 0.9029\n",
      "Epoch 358/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.2813 - accuracy: 0.8923 - val_loss: 0.2835 - val_accuracy: 0.8852\n",
      "Epoch 359/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2551 - accuracy: 0.9007 - val_loss: 0.2871 - val_accuracy: 0.8940\n",
      "Epoch 360/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2774 - accuracy: 0.8965 - val_loss: 0.3250 - val_accuracy: 0.8698\n",
      "Epoch 361/1000\n",
      "4068/4068 [==============================] - 0s 85us/step - loss: 0.2609 - accuracy: 0.8955 - val_loss: 0.3334 - val_accuracy: 0.8609\n",
      "Epoch 362/1000\n",
      "4068/4068 [==============================] - 0s 86us/step - loss: 0.2807 - accuracy: 0.8923 - val_loss: 0.3185 - val_accuracy: 0.8742\n",
      "Epoch 363/1000\n",
      "4068/4068 [==============================] - ETA: 0s - loss: 0.2781 - accuracy: 0.89 - 0s 84us/step - loss: 0.2798 - accuracy: 0.8923 - val_loss: 0.3643 - val_accuracy: 0.8521\n",
      "Epoch 364/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.2670 - accuracy: 0.8980 - val_loss: 0.2798 - val_accuracy: 0.8985\n",
      "Epoch 365/1000\n",
      "4068/4068 [==============================] - 0s 83us/step - loss: 0.2817 - accuracy: 0.8921 - val_loss: 0.2943 - val_accuracy: 0.9007\n",
      "Epoch 366/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2859 - accuracy: 0.8945 - val_loss: 0.2997 - val_accuracy: 0.8874\n",
      "Epoch 367/1000\n",
      "4068/4068 [==============================] - 0s 84us/step - loss: 0.2511 - accuracy: 0.9002 - val_loss: 0.4112 - val_accuracy: 0.8366\n",
      "Epoch 368/1000\n",
      "4068/4068 [==============================] - 0s 99us/step - loss: 0.2738 - accuracy: 0.8992 - val_loss: 0.3218 - val_accuracy: 0.8631\n",
      "Epoch 369/1000\n",
      "4068/4068 [==============================] - 0s 100us/step - loss: 0.2500 - accuracy: 0.8987 - val_loss: 0.3868 - val_accuracy: 0.9007\n",
      "Epoch 370/1000\n",
      "4068/4068 [==============================] - 0s 93us/step - loss: 0.2705 - accuracy: 0.8975 - val_loss: 0.2838 - val_accuracy: 0.8962\n",
      "Epoch 371/1000\n",
      "4068/4068 [==============================] - 0s 83us/step - loss: 0.2842 - accuracy: 0.8958 - val_loss: 0.2979 - val_accuracy: 0.8874\n",
      "Epoch 372/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.2640 - accuracy: 0.8997 - val_loss: 0.2877 - val_accuracy: 0.9051\n",
      "Epoch 373/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2716 - accuracy: 0.8953 - val_loss: 0.2838 - val_accuracy: 0.8874\n",
      "Epoch 374/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.2517 - accuracy: 0.9007 - val_loss: 0.2878 - val_accuracy: 0.8896\n",
      "Epoch 375/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.2473 - accuracy: 0.8972 - val_loss: 0.3155 - val_accuracy: 0.8764\n",
      "Epoch 376/1000\n",
      "4068/4068 [==============================] - 0s 82us/step - loss: 0.2834 - accuracy: 0.8958 - val_loss: 0.2915 - val_accuracy: 0.8874\n",
      "Epoch 377/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.3330 - accuracy: 0.8879 - val_loss: 0.3436 - val_accuracy: 0.8874\n",
      "Epoch 378/1000\n",
      "4068/4068 [==============================] - 0s 83us/step - loss: 0.2648 - accuracy: 0.8963 - val_loss: 0.2882 - val_accuracy: 0.8940\n",
      "Epoch 379/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2557 - accuracy: 0.9002 - val_loss: 0.3052 - val_accuracy: 0.8808\n",
      "Epoch 380/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2669 - accuracy: 0.8958 - val_loss: 0.3101 - val_accuracy: 0.8830\n",
      "Epoch 381/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2706 - accuracy: 0.8943 - val_loss: 0.4141 - val_accuracy: 0.8256\n",
      "Epoch 382/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.2383 - accuracy: 0.9039 - val_loss: 0.3100 - val_accuracy: 0.9051\n",
      "Epoch 383/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2508 - accuracy: 0.9041 - val_loss: 0.3689 - val_accuracy: 0.8918\n",
      "Epoch 384/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2579 - accuracy: 0.8960 - val_loss: 0.2920 - val_accuracy: 0.8852\n",
      "Epoch 385/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2578 - accuracy: 0.9029 - val_loss: 0.3635 - val_accuracy: 0.8521\n",
      "Epoch 386/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2511 - accuracy: 0.9022 - val_loss: 0.2895 - val_accuracy: 0.8896\n",
      "Epoch 387/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2549 - accuracy: 0.9007 - val_loss: 0.3674 - val_accuracy: 0.9051\n",
      "Epoch 388/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2578 - accuracy: 0.8977 - val_loss: 0.3057 - val_accuracy: 0.8852\n",
      "Epoch 389/1000\n",
      "4068/4068 [==============================] - ETA: 0s - loss: 0.2862 - accuracy: 0.89 - 0s 78us/step - loss: 0.2890 - accuracy: 0.8923 - val_loss: 0.3642 - val_accuracy: 0.8543\n",
      "Epoch 390/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2594 - accuracy: 0.8982 - val_loss: 0.3820 - val_accuracy: 0.8366\n",
      "Epoch 391/1000\n",
      "4068/4068 [==============================] - 0s 75us/step - loss: 0.2511 - accuracy: 0.9002 - val_loss: 0.2992 - val_accuracy: 0.8940\n",
      "Epoch 392/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2586 - accuracy: 0.8963 - val_loss: 0.2944 - val_accuracy: 0.8962\n",
      "Epoch 393/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2785 - accuracy: 0.8921 - val_loss: 0.2915 - val_accuracy: 0.8764\n",
      "Epoch 394/1000\n",
      "4068/4068 [==============================] - 0s 75us/step - loss: 0.2733 - accuracy: 0.8982 - val_loss: 0.3464 - val_accuracy: 0.8565\n",
      "Epoch 395/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2471 - accuracy: 0.9012 - val_loss: 0.2993 - val_accuracy: 0.8918\n",
      "Epoch 396/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2701 - accuracy: 0.9004 - val_loss: 0.4019 - val_accuracy: 0.8477\n",
      "Epoch 397/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2638 - accuracy: 0.8965 - val_loss: 0.3044 - val_accuracy: 0.8786\n",
      "Epoch 398/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2548 - accuracy: 0.8970 - val_loss: 0.5485 - val_accuracy: 0.7947\n",
      "Epoch 399/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2661 - accuracy: 0.8980 - val_loss: 0.3052 - val_accuracy: 0.8808\n",
      "Epoch 400/1000\n",
      "4068/4068 [==============================] - 0s 76us/step - loss: 0.2844 - accuracy: 0.8945 - val_loss: 0.3519 - val_accuracy: 0.8521\n",
      "Epoch 401/1000\n",
      "4068/4068 [==============================] - ETA: 0s - loss: 0.2418 - accuracy: 0.90 - 0s 77us/step - loss: 0.2377 - accuracy: 0.9027 - val_loss: 0.3790 - val_accuracy: 0.8962\n",
      "Epoch 402/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2673 - accuracy: 0.9007 - val_loss: 0.7397 - val_accuracy: 0.7307\n",
      "Epoch 403/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2513 - accuracy: 0.9024 - val_loss: 0.3223 - val_accuracy: 0.8962\n",
      "Epoch 404/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.2798 - accuracy: 0.8909 - val_loss: 0.3188 - val_accuracy: 0.8720\n",
      "Epoch 405/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2994 - accuracy: 0.8916 - val_loss: 0.3388 - val_accuracy: 0.8852\n",
      "Epoch 406/1000\n",
      "4068/4068 [==============================] - 0s 82us/step - loss: 0.2751 - accuracy: 0.8995 - val_loss: 0.3000 - val_accuracy: 0.9007\n",
      "Epoch 407/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2456 - accuracy: 0.9007 - val_loss: 0.3579 - val_accuracy: 0.8631\n",
      "Epoch 408/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.2581 - accuracy: 0.8995 - val_loss: 0.3435 - val_accuracy: 0.8896\n",
      "Epoch 409/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2366 - accuracy: 0.9063 - val_loss: 0.2875 - val_accuracy: 0.8830\n",
      "Epoch 410/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2698 - accuracy: 0.8987 - val_loss: 0.2804 - val_accuracy: 0.9007\n",
      "Epoch 411/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.2805 - accuracy: 0.8916 - val_loss: 0.3356 - val_accuracy: 0.9051\n",
      "Epoch 412/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2740 - accuracy: 0.8980 - val_loss: 0.5210 - val_accuracy: 0.8102\n",
      "Epoch 413/1000\n",
      "4068/4068 [==============================] - 0s 75us/step - loss: 0.2724 - accuracy: 0.8972 - val_loss: 0.2985 - val_accuracy: 0.8918\n",
      "Epoch 414/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2394 - accuracy: 0.9054 - val_loss: 0.2966 - val_accuracy: 0.8918\n",
      "Epoch 415/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2814 - accuracy: 0.8916 - val_loss: 0.3313 - val_accuracy: 0.8609\n",
      "Epoch 416/1000\n",
      "4068/4068 [==============================] - 0s 84us/step - loss: 0.2748 - accuracy: 0.9014 - val_loss: 0.3239 - val_accuracy: 0.8720\n",
      "Epoch 417/1000\n",
      "4068/4068 [==============================] - 0s 98us/step - loss: 0.2543 - accuracy: 0.9039 - val_loss: 0.3726 - val_accuracy: 0.8918\n",
      "Epoch 418/1000\n",
      "4068/4068 [==============================] - 0s 95us/step - loss: 0.2970 - accuracy: 0.8931 - val_loss: 0.3247 - val_accuracy: 0.9029\n",
      "Epoch 419/1000\n",
      "4068/4068 [==============================] - 0s 88us/step - loss: 0.2572 - accuracy: 0.9046 - val_loss: 0.3210 - val_accuracy: 0.8918\n",
      "Epoch 420/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2775 - accuracy: 0.8933 - val_loss: 0.3580 - val_accuracy: 0.8433\n",
      "Epoch 421/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2461 - accuracy: 0.9031 - val_loss: 0.2815 - val_accuracy: 0.8985\n",
      "Epoch 422/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.2503 - accuracy: 0.8995 - val_loss: 0.4103 - val_accuracy: 0.8962\n",
      "Epoch 423/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.3078 - accuracy: 0.8886 - val_loss: 0.4859 - val_accuracy: 0.8190\n",
      "Epoch 424/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2514 - accuracy: 0.9044 - val_loss: 0.3252 - val_accuracy: 0.9007\n",
      "Epoch 425/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2772 - accuracy: 0.8975 - val_loss: 0.3306 - val_accuracy: 0.8653\n",
      "Epoch 426/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2609 - accuracy: 0.8992 - val_loss: 0.3171 - val_accuracy: 0.8896\n",
      "Epoch 427/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2446 - accuracy: 0.8985 - val_loss: 0.4456 - val_accuracy: 0.9029\n",
      "Epoch 428/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2506 - accuracy: 0.9068 - val_loss: 0.3261 - val_accuracy: 0.8675\n",
      "Epoch 429/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2383 - accuracy: 0.9086 - val_loss: 0.3903 - val_accuracy: 0.8389\n",
      "Epoch 430/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2495 - accuracy: 0.9027 - val_loss: 0.3059 - val_accuracy: 0.8874\n",
      "Epoch 431/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2609 - accuracy: 0.8977 - val_loss: 0.3162 - val_accuracy: 0.8675\n",
      "Epoch 432/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2646 - accuracy: 0.8955 - val_loss: 0.3002 - val_accuracy: 0.8808\n",
      "Epoch 433/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2455 - accuracy: 0.9046 - val_loss: 0.4698 - val_accuracy: 0.9051\n",
      "Epoch 434/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.2973 - accuracy: 0.8918 - val_loss: 0.3360 - val_accuracy: 0.8874\n",
      "Epoch 435/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2612 - accuracy: 0.8960 - val_loss: 0.4174 - val_accuracy: 0.8322\n",
      "Epoch 436/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.2532 - accuracy: 0.8997 - val_loss: 0.3228 - val_accuracy: 0.9007\n",
      "Epoch 437/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2332 - accuracy: 0.9088 - val_loss: 0.2972 - val_accuracy: 0.8962\n",
      "Epoch 438/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2483 - accuracy: 0.9044 - val_loss: 0.3198 - val_accuracy: 0.8742\n",
      "Epoch 439/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2519 - accuracy: 0.9017 - val_loss: 0.5308 - val_accuracy: 0.9073\n",
      "Epoch 440/1000\n",
      "4068/4068 [==============================] - 0s 76us/step - loss: 0.2880 - accuracy: 0.8936 - val_loss: 0.3852 - val_accuracy: 0.8278\n",
      "Epoch 441/1000\n",
      "4068/4068 [==============================] - 0s 74us/step - loss: 0.2614 - accuracy: 0.8968 - val_loss: 0.3054 - val_accuracy: 0.8896\n",
      "Epoch 442/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2598 - accuracy: 0.8975 - val_loss: 0.5843 - val_accuracy: 0.7947\n",
      "Epoch 443/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2709 - accuracy: 0.8953 - val_loss: 0.3166 - val_accuracy: 0.8742\n",
      "Epoch 444/1000\n",
      "4068/4068 [==============================] - 0s 75us/step - loss: 0.2439 - accuracy: 0.9007 - val_loss: 0.2848 - val_accuracy: 0.8896\n",
      "Epoch 445/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2598 - accuracy: 0.8985 - val_loss: 0.3006 - val_accuracy: 0.8808\n",
      "Epoch 446/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2461 - accuracy: 0.9061 - val_loss: 0.2994 - val_accuracy: 0.8962\n",
      "Epoch 447/1000\n",
      "4068/4068 [==============================] - 0s 75us/step - loss: 0.2462 - accuracy: 0.9019 - val_loss: 0.3011 - val_accuracy: 0.8830\n",
      "Epoch 448/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2594 - accuracy: 0.9004 - val_loss: 0.3388 - val_accuracy: 0.8698\n",
      "Epoch 449/1000\n",
      "4068/4068 [==============================] - 0s 76us/step - loss: 0.3468 - accuracy: 0.8837 - val_loss: 0.3269 - val_accuracy: 0.8764\n",
      "Epoch 450/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2359 - accuracy: 0.9019 - val_loss: 0.3075 - val_accuracy: 0.8896\n",
      "Epoch 451/1000\n",
      "4068/4068 [==============================] - 0s 76us/step - loss: 0.2368 - accuracy: 0.9012 - val_loss: 0.3137 - val_accuracy: 0.8653\n",
      "Epoch 452/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2368 - accuracy: 0.9036 - val_loss: 0.3246 - val_accuracy: 0.8985\n",
      "Epoch 453/1000\n",
      "4068/4068 [==============================] - 0s 76us/step - loss: 0.2651 - accuracy: 0.8995 - val_loss: 0.3270 - val_accuracy: 0.8609\n",
      "Epoch 454/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2514 - accuracy: 0.9019 - val_loss: 0.3256 - val_accuracy: 0.8985\n",
      "Epoch 455/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2468 - accuracy: 0.9027 - val_loss: 0.3489 - val_accuracy: 0.8565\n",
      "Epoch 456/1000\n",
      "4068/4068 [==============================] - 0s 82us/step - loss: 0.2516 - accuracy: 0.8990 - val_loss: 0.3321 - val_accuracy: 0.8653\n",
      "Epoch 457/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2441 - accuracy: 0.9054 - val_loss: 0.3375 - val_accuracy: 0.8653\n",
      "Epoch 458/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2536 - accuracy: 0.8982 - val_loss: 0.3058 - val_accuracy: 0.8874\n",
      "Epoch 459/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2621 - accuracy: 0.8992 - val_loss: 0.4849 - val_accuracy: 0.8212\n",
      "Epoch 460/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2660 - accuracy: 0.8968 - val_loss: 0.3364 - val_accuracy: 0.8521\n",
      "Epoch 461/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.2357 - accuracy: 0.9071 - val_loss: 0.3601 - val_accuracy: 0.9007\n",
      "Epoch 462/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2854 - accuracy: 0.9004 - val_loss: 0.3967 - val_accuracy: 0.8455\n",
      "Epoch 463/1000\n",
      "4068/4068 [==============================] - 0s 82us/step - loss: 0.2551 - accuracy: 0.8977 - val_loss: 0.3176 - val_accuracy: 0.8742\n",
      "Epoch 464/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.2445 - accuracy: 0.9029 - val_loss: 0.5220 - val_accuracy: 0.9051\n",
      "Epoch 465/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.2592 - accuracy: 0.9000 - val_loss: 0.3001 - val_accuracy: 0.8896\n",
      "Epoch 466/1000\n",
      "4068/4068 [==============================] - 0s 94us/step - loss: 0.2345 - accuracy: 0.9039 - val_loss: 0.2933 - val_accuracy: 0.8808\n",
      "Epoch 467/1000\n",
      "4068/4068 [==============================] - 0s 99us/step - loss: 0.2478 - accuracy: 0.8987 - val_loss: 0.3109 - val_accuracy: 0.8896\n",
      "Epoch 468/1000\n",
      "4068/4068 [==============================] - 0s 93us/step - loss: 0.2539 - accuracy: 0.8972 - val_loss: 0.3474 - val_accuracy: 0.9095\n",
      "Epoch 469/1000\n",
      "4068/4068 [==============================] - 0s 83us/step - loss: 0.2449 - accuracy: 0.9036 - val_loss: 0.3056 - val_accuracy: 0.8852\n",
      "Epoch 470/1000\n",
      "4068/4068 [==============================] - 0s 82us/step - loss: 0.2584 - accuracy: 0.8990 - val_loss: 0.3064 - val_accuracy: 0.8918\n",
      "Epoch 471/1000\n",
      "4068/4068 [==============================] - 0s 82us/step - loss: 0.2640 - accuracy: 0.9034 - val_loss: 0.3901 - val_accuracy: 0.8940\n",
      "Epoch 472/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.2666 - accuracy: 0.9007 - val_loss: 0.3100 - val_accuracy: 0.8940\n",
      "Epoch 473/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.2714 - accuracy: 0.8960 - val_loss: 0.5408 - val_accuracy: 0.8940\n",
      "Epoch 474/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2590 - accuracy: 0.9022 - val_loss: 0.3588 - val_accuracy: 0.8587\n",
      "Epoch 475/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.2442 - accuracy: 0.9041 - val_loss: 0.3167 - val_accuracy: 0.9051\n",
      "Epoch 476/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2750 - accuracy: 0.9007 - val_loss: 0.3114 - val_accuracy: 0.8874\n",
      "Epoch 477/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2392 - accuracy: 0.9041 - val_loss: 0.7349 - val_accuracy: 0.9051\n",
      "Epoch 478/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2736 - accuracy: 0.8958 - val_loss: 0.4107 - val_accuracy: 0.8985\n",
      "Epoch 479/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2490 - accuracy: 0.8972 - val_loss: 0.3113 - val_accuracy: 0.8874\n",
      "Epoch 480/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2343 - accuracy: 0.9031 - val_loss: 0.3283 - val_accuracy: 0.8808\n",
      "Epoch 481/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2852 - accuracy: 0.8941 - val_loss: 0.3906 - val_accuracy: 0.9007\n",
      "Epoch 482/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2421 - accuracy: 0.9012 - val_loss: 0.3478 - val_accuracy: 0.9095\n",
      "Epoch 483/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2364 - accuracy: 0.9012 - val_loss: 0.3108 - val_accuracy: 0.9095\n",
      "Epoch 484/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2420 - accuracy: 0.9061 - val_loss: 0.3106 - val_accuracy: 0.8808\n",
      "Epoch 485/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.2479 - accuracy: 0.8972 - val_loss: 0.3056 - val_accuracy: 0.8896\n",
      "Epoch 486/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2633 - accuracy: 0.9009 - val_loss: 0.4186 - val_accuracy: 0.8962\n",
      "Epoch 487/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2484 - accuracy: 0.8970 - val_loss: 0.3054 - val_accuracy: 0.8852\n",
      "Epoch 488/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2311 - accuracy: 0.9081 - val_loss: 0.3205 - val_accuracy: 0.8918\n",
      "Epoch 489/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2612 - accuracy: 0.9024 - val_loss: 0.4213 - val_accuracy: 0.8234\n",
      "Epoch 490/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2648 - accuracy: 0.9022 - val_loss: 0.3229 - val_accuracy: 0.8653\n",
      "Epoch 491/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.2574 - accuracy: 0.8970 - val_loss: 0.3548 - val_accuracy: 0.9051\n",
      "Epoch 492/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2501 - accuracy: 0.8997 - val_loss: 0.3438 - val_accuracy: 0.8896\n",
      "Epoch 493/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2557 - accuracy: 0.8980 - val_loss: 0.3187 - val_accuracy: 0.8808\n",
      "Epoch 494/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2614 - accuracy: 0.8972 - val_loss: 0.3074 - val_accuracy: 0.9073\n",
      "Epoch 495/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2628 - accuracy: 0.8955 - val_loss: 0.3701 - val_accuracy: 0.8455\n",
      "Epoch 496/1000\n",
      "4068/4068 [==============================] - 0s 75us/step - loss: 0.2568 - accuracy: 0.9004 - val_loss: 0.4131 - val_accuracy: 0.9007\n",
      "Epoch 497/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2471 - accuracy: 0.9068 - val_loss: 0.3069 - val_accuracy: 0.8742\n",
      "Epoch 498/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2316 - accuracy: 0.9046 - val_loss: 0.3768 - val_accuracy: 0.8499\n",
      "Epoch 499/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2383 - accuracy: 0.9078 - val_loss: 0.3214 - val_accuracy: 0.8786\n",
      "Epoch 500/1000\n",
      "4068/4068 [==============================] - 0s 75us/step - loss: 0.2520 - accuracy: 0.9041 - val_loss: 0.3033 - val_accuracy: 0.8808\n",
      "Epoch 501/1000\n",
      "4068/4068 [==============================] - 0s 76us/step - loss: 0.2670 - accuracy: 0.8970 - val_loss: 0.3352 - val_accuracy: 0.8962\n",
      "Epoch 502/1000\n",
      "4068/4068 [==============================] - 0s 75us/step - loss: 0.2319 - accuracy: 0.9054 - val_loss: 0.3050 - val_accuracy: 0.8874\n",
      "Epoch 503/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2597 - accuracy: 0.8977 - val_loss: 0.2900 - val_accuracy: 0.8985\n",
      "Epoch 504/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2427 - accuracy: 0.9017 - val_loss: 0.3007 - val_accuracy: 0.9051\n",
      "Epoch 505/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2527 - accuracy: 0.8943 - val_loss: 0.3474 - val_accuracy: 0.8653\n",
      "Epoch 506/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2371 - accuracy: 0.9068 - val_loss: 0.3297 - val_accuracy: 0.8764\n",
      "Epoch 507/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2822 - accuracy: 0.8953 - val_loss: 0.3635 - val_accuracy: 0.8940\n",
      "Epoch 508/1000\n",
      "4068/4068 [==============================] - 0s 74us/step - loss: 0.2292 - accuracy: 0.9051 - val_loss: 0.3099 - val_accuracy: 0.8830\n",
      "Epoch 509/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2560 - accuracy: 0.8965 - val_loss: 0.3379 - val_accuracy: 0.9007\n",
      "Epoch 510/1000\n",
      "4068/4068 [==============================] - 0s 83us/step - loss: 0.2359 - accuracy: 0.9110 - val_loss: 0.3144 - val_accuracy: 0.9007\n",
      "Epoch 511/1000\n",
      "4068/4068 [==============================] - 0s 82us/step - loss: 0.2300 - accuracy: 0.9098 - val_loss: 0.3502 - val_accuracy: 0.8609\n",
      "Epoch 512/1000\n",
      "4068/4068 [==============================] - 0s 74us/step - loss: 0.2412 - accuracy: 0.9012 - val_loss: 0.3325 - val_accuracy: 0.8962\n",
      "Epoch 513/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2669 - accuracy: 0.8980 - val_loss: 0.2924 - val_accuracy: 0.8962\n",
      "Epoch 514/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2365 - accuracy: 0.9036 - val_loss: 0.3892 - val_accuracy: 0.8565\n",
      "Epoch 515/1000\n",
      "4068/4068 [==============================] - 0s 90us/step - loss: 0.2433 - accuracy: 0.9031 - val_loss: 0.3098 - val_accuracy: 0.9029\n",
      "Epoch 516/1000\n",
      "4068/4068 [==============================] - 0s 95us/step - loss: 0.2462 - accuracy: 0.9014 - val_loss: 0.3580 - val_accuracy: 0.8808\n",
      "Epoch 517/1000\n",
      "4068/4068 [==============================] - 0s 93us/step - loss: 0.2554 - accuracy: 0.9002 - val_loss: 0.3064 - val_accuracy: 0.8874\n",
      "Epoch 518/1000\n",
      "4068/4068 [==============================] - 0s 83us/step - loss: 0.2294 - accuracy: 0.9061 - val_loss: 0.3147 - val_accuracy: 0.8940\n",
      "Epoch 519/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2418 - accuracy: 0.9034 - val_loss: 0.3577 - val_accuracy: 0.8499\n",
      "Epoch 520/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2271 - accuracy: 0.9076 - val_loss: 0.3916 - val_accuracy: 0.8499\n",
      "Epoch 521/1000\n",
      "4068/4068 [==============================] - 0s 75us/step - loss: 0.2426 - accuracy: 0.9007 - val_loss: 0.3559 - val_accuracy: 0.8764\n",
      "Epoch 522/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2300 - accuracy: 0.9054 - val_loss: 0.3282 - val_accuracy: 0.8808\n",
      "Epoch 523/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2263 - accuracy: 0.9073 - val_loss: 0.2930 - val_accuracy: 0.8896\n",
      "Epoch 524/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2395 - accuracy: 0.9000 - val_loss: 0.3782 - val_accuracy: 0.9095\n",
      "Epoch 525/1000\n",
      "4068/4068 [==============================] - 0s 75us/step - loss: 0.2607 - accuracy: 0.9014 - val_loss: 0.3468 - val_accuracy: 0.8985\n",
      "Epoch 526/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2287 - accuracy: 0.9061 - val_loss: 0.3177 - val_accuracy: 0.8896\n",
      "Epoch 527/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2392 - accuracy: 0.9056 - val_loss: 0.3281 - val_accuracy: 0.8962\n",
      "Epoch 528/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2605 - accuracy: 0.9022 - val_loss: 0.3456 - val_accuracy: 0.8675\n",
      "Epoch 529/1000\n",
      "4068/4068 [==============================] - 0s 76us/step - loss: 0.2268 - accuracy: 0.9093 - val_loss: 0.4443 - val_accuracy: 0.8278\n",
      "Epoch 530/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2508 - accuracy: 0.8980 - val_loss: 0.3713 - val_accuracy: 0.8565\n",
      "Epoch 531/1000\n",
      "4068/4068 [==============================] - 0s 75us/step - loss: 0.2393 - accuracy: 0.9039 - val_loss: 0.3398 - val_accuracy: 0.8631\n",
      "Epoch 532/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.2538 - accuracy: 0.8985 - val_loss: 0.3210 - val_accuracy: 0.8852\n",
      "Epoch 533/1000\n",
      "4068/4068 [==============================] - 0s 76us/step - loss: 0.2333 - accuracy: 0.9046 - val_loss: 0.3254 - val_accuracy: 0.8764\n",
      "Epoch 534/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2337 - accuracy: 0.9051 - val_loss: 0.3385 - val_accuracy: 0.8720\n",
      "Epoch 535/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2339 - accuracy: 0.9063 - val_loss: 0.3910 - val_accuracy: 0.8433\n",
      "Epoch 536/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2386 - accuracy: 0.9024 - val_loss: 0.4149 - val_accuracy: 0.8344\n",
      "Epoch 537/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2530 - accuracy: 0.9027 - val_loss: 0.3555 - val_accuracy: 0.9029\n",
      "Epoch 538/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2496 - accuracy: 0.9044 - val_loss: 0.3384 - val_accuracy: 0.9029\n",
      "Epoch 539/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2465 - accuracy: 0.9027 - val_loss: 0.3105 - val_accuracy: 0.8962\n",
      "Epoch 540/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2615 - accuracy: 0.9012 - val_loss: 0.3340 - val_accuracy: 0.8918\n",
      "Epoch 541/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2550 - accuracy: 0.9004 - val_loss: 0.4490 - val_accuracy: 0.8962\n",
      "Epoch 542/1000\n",
      "4068/4068 [==============================] - 0s 82us/step - loss: 0.2573 - accuracy: 0.9044 - val_loss: 0.3287 - val_accuracy: 0.8808\n",
      "Epoch 543/1000\n",
      "4068/4068 [==============================] - 0s 82us/step - loss: 0.2273 - accuracy: 0.9078 - val_loss: 0.3168 - val_accuracy: 0.8874\n",
      "Epoch 544/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.2267 - accuracy: 0.9073 - val_loss: 0.3527 - val_accuracy: 0.8521\n",
      "Epoch 545/1000\n",
      "4068/4068 [==============================] - 0s 76us/step - loss: 0.2257 - accuracy: 0.9061 - val_loss: 0.3287 - val_accuracy: 0.8918\n",
      "Epoch 546/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2660 - accuracy: 0.8977 - val_loss: 0.3507 - val_accuracy: 0.8631\n",
      "Epoch 547/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.2441 - accuracy: 0.9083 - val_loss: 0.5276 - val_accuracy: 0.8146\n",
      "Epoch 548/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2332 - accuracy: 0.9066 - val_loss: 0.4174 - val_accuracy: 0.8455\n",
      "Epoch 549/1000\n",
      "4068/4068 [==============================] - 0s 75us/step - loss: 0.2458 - accuracy: 0.9039 - val_loss: 0.3974 - val_accuracy: 0.8521\n",
      "Epoch 550/1000\n",
      "4068/4068 [==============================] - 0s 75us/step - loss: 0.2335 - accuracy: 0.9009 - val_loss: 0.3043 - val_accuracy: 0.8918\n",
      "Epoch 551/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2442 - accuracy: 0.9027 - val_loss: 0.3845 - val_accuracy: 0.9007\n",
      "Epoch 552/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2343 - accuracy: 0.9068 - val_loss: 0.3306 - val_accuracy: 0.9007\n",
      "Epoch 553/1000\n",
      "4068/4068 [==============================] - 0s 74us/step - loss: 0.2274 - accuracy: 0.9088 - val_loss: 0.4130 - val_accuracy: 0.8455\n",
      "Epoch 554/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2515 - accuracy: 0.9041 - val_loss: 0.4082 - val_accuracy: 0.8587\n",
      "Epoch 555/1000\n",
      "4068/4068 [==============================] - 0s 82us/step - loss: 0.2462 - accuracy: 0.9014 - val_loss: 0.3328 - val_accuracy: 0.8874\n",
      "Epoch 556/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2349 - accuracy: 0.9017 - val_loss: 0.3490 - val_accuracy: 0.8543\n",
      "Epoch 557/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2384 - accuracy: 0.9098 - val_loss: 0.3639 - val_accuracy: 0.8587\n",
      "Epoch 558/1000\n",
      "4068/4068 [==============================] - ETA: 0s - loss: 0.2217 - accuracy: 0.91 - 0s 78us/step - loss: 0.2227 - accuracy: 0.9108 - val_loss: 0.3154 - val_accuracy: 0.9073\n",
      "Epoch 559/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2479 - accuracy: 0.9009 - val_loss: 0.4727 - val_accuracy: 0.8168\n",
      "Epoch 560/1000\n",
      "4068/4068 [==============================] - 0s 82us/step - loss: 0.2707 - accuracy: 0.8970 - val_loss: 0.3287 - val_accuracy: 0.8896\n",
      "Epoch 561/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2262 - accuracy: 0.9086 - val_loss: 0.4098 - val_accuracy: 0.8455\n",
      "Epoch 562/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2428 - accuracy: 0.9031 - val_loss: 0.3641 - val_accuracy: 0.8433\n",
      "Epoch 563/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2418 - accuracy: 0.9019 - val_loss: 0.3179 - val_accuracy: 0.9051\n",
      "Epoch 564/1000\n",
      "4068/4068 [==============================] - 0s 87us/step - loss: 0.2382 - accuracy: 0.9044 - val_loss: 0.3793 - val_accuracy: 0.8543\n",
      "Epoch 565/1000\n",
      "4068/4068 [==============================] - 0s 97us/step - loss: 0.2350 - accuracy: 0.9039 - val_loss: 0.3870 - val_accuracy: 0.8962\n",
      "Epoch 566/1000\n",
      "4068/4068 [==============================] - 0s 97us/step - loss: 0.2248 - accuracy: 0.9098 - val_loss: 0.3255 - val_accuracy: 0.8675\n",
      "Epoch 567/1000\n",
      "4068/4068 [==============================] - 0s 86us/step - loss: 0.2511 - accuracy: 0.9002 - val_loss: 0.3376 - val_accuracy: 0.9007\n",
      "Epoch 568/1000\n",
      "4068/4068 [==============================] - 0s 82us/step - loss: 0.2260 - accuracy: 0.9041 - val_loss: 0.3161 - val_accuracy: 0.8896\n",
      "Epoch 569/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2298 - accuracy: 0.9046 - val_loss: 0.3070 - val_accuracy: 0.8852\n",
      "Epoch 570/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2353 - accuracy: 0.9061 - val_loss: 0.4162 - val_accuracy: 0.8322\n",
      "Epoch 571/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2314 - accuracy: 0.9066 - val_loss: 0.4546 - val_accuracy: 0.8146\n",
      "Epoch 572/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2283 - accuracy: 0.9073 - val_loss: 0.3266 - val_accuracy: 0.9007\n",
      "Epoch 573/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2355 - accuracy: 0.9110 - val_loss: 0.3844 - val_accuracy: 0.8499\n",
      "Epoch 574/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2598 - accuracy: 0.8992 - val_loss: 0.3194 - val_accuracy: 0.8918\n",
      "Epoch 575/1000\n",
      "4068/4068 [==============================] - 0s 76us/step - loss: 0.2271 - accuracy: 0.9039 - val_loss: 0.3176 - val_accuracy: 0.8962\n",
      "Epoch 576/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2280 - accuracy: 0.9073 - val_loss: 0.3970 - val_accuracy: 0.8808\n",
      "Epoch 577/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2500 - accuracy: 0.9051 - val_loss: 0.3209 - val_accuracy: 0.8874\n",
      "Epoch 578/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2502 - accuracy: 0.9024 - val_loss: 0.3551 - val_accuracy: 0.8720\n",
      "Epoch 579/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2619 - accuracy: 0.9017 - val_loss: 0.3922 - val_accuracy: 0.8543\n",
      "Epoch 580/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2324 - accuracy: 0.9054 - val_loss: 0.3461 - val_accuracy: 0.8852\n",
      "Epoch 581/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2330 - accuracy: 0.9034 - val_loss: 0.3171 - val_accuracy: 0.9029\n",
      "Epoch 582/1000\n",
      "4068/4068 [==============================] - 0s 75us/step - loss: 0.2314 - accuracy: 0.9078 - val_loss: 0.3633 - val_accuracy: 0.8675\n",
      "Epoch 583/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2366 - accuracy: 0.9061 - val_loss: 0.3695 - val_accuracy: 0.8874\n",
      "Epoch 584/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2446 - accuracy: 0.9056 - val_loss: 0.3284 - val_accuracy: 0.8940\n",
      "Epoch 585/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2344 - accuracy: 0.9076 - val_loss: 0.3198 - val_accuracy: 0.8786\n",
      "Epoch 586/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2174 - accuracy: 0.9122 - val_loss: 0.3334 - val_accuracy: 0.8742\n",
      "Epoch 587/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2456 - accuracy: 0.9036 - val_loss: 0.3773 - val_accuracy: 0.8609\n",
      "Epoch 588/1000\n",
      "4068/4068 [==============================] - 0s 82us/step - loss: 0.2236 - accuracy: 0.9113 - val_loss: 0.3311 - val_accuracy: 0.8742\n",
      "Epoch 589/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2416 - accuracy: 0.9039 - val_loss: 0.3226 - val_accuracy: 0.8874\n",
      "Epoch 590/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2266 - accuracy: 0.9083 - val_loss: 0.3412 - val_accuracy: 0.9073\n",
      "Epoch 591/1000\n",
      "4068/4068 [==============================] - 0s 88us/step - loss: 0.2279 - accuracy: 0.9076 - val_loss: 0.4475 - val_accuracy: 0.8190\n",
      "Epoch 592/1000\n",
      "4068/4068 [==============================] - 0s 95us/step - loss: 0.2157 - accuracy: 0.9100 - val_loss: 0.4348 - val_accuracy: 0.8940\n",
      "Epoch 593/1000\n",
      "4068/4068 [==============================] - 0s 94us/step - loss: 0.2462 - accuracy: 0.9014 - val_loss: 0.3711 - val_accuracy: 0.9095\n",
      "Epoch 594/1000\n",
      "4068/4068 [==============================] - 0s 90us/step - loss: 0.2332 - accuracy: 0.9076 - val_loss: 0.4459 - val_accuracy: 0.8322\n",
      "Epoch 595/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.2483 - accuracy: 0.9019 - val_loss: 0.4319 - val_accuracy: 0.8455\n",
      "Epoch 596/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2515 - accuracy: 0.9024 - val_loss: 0.3399 - val_accuracy: 0.8742\n",
      "Epoch 597/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.2461 - accuracy: 0.9051 - val_loss: 0.3407 - val_accuracy: 0.9051\n",
      "Epoch 598/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.2214 - accuracy: 0.9046 - val_loss: 0.4662 - val_accuracy: 0.8212\n",
      "Epoch 599/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2558 - accuracy: 0.8990 - val_loss: 0.3555 - val_accuracy: 0.8609\n",
      "Epoch 600/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2161 - accuracy: 0.9095 - val_loss: 0.3725 - val_accuracy: 0.8587\n",
      "Epoch 601/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.2436 - accuracy: 0.9041 - val_loss: 0.3272 - val_accuracy: 0.8786\n",
      "Epoch 602/1000\n",
      "4068/4068 [==============================] - 0s 85us/step - loss: 0.2414 - accuracy: 0.9066 - val_loss: 0.3278 - val_accuracy: 0.8896\n",
      "Epoch 603/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2221 - accuracy: 0.9093 - val_loss: 0.4348 - val_accuracy: 0.9029\n",
      "Epoch 604/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2224 - accuracy: 0.9039 - val_loss: 0.3734 - val_accuracy: 0.8587\n",
      "Epoch 605/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2357 - accuracy: 0.9130 - val_loss: 0.3333 - val_accuracy: 0.8808\n",
      "Epoch 606/1000\n",
      "4068/4068 [==============================] - 0s 76us/step - loss: 0.2245 - accuracy: 0.9122 - val_loss: 0.3280 - val_accuracy: 0.9007\n",
      "Epoch 607/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2327 - accuracy: 0.9078 - val_loss: 0.3217 - val_accuracy: 0.8830\n",
      "Epoch 608/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2466 - accuracy: 0.9009 - val_loss: 0.3608 - val_accuracy: 0.8609\n",
      "Epoch 609/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2315 - accuracy: 0.9059 - val_loss: 0.3503 - val_accuracy: 0.8742\n",
      "Epoch 610/1000\n",
      "4068/4068 [==============================] - 0s 76us/step - loss: 0.2488 - accuracy: 0.9012 - val_loss: 0.3550 - val_accuracy: 0.8940\n",
      "Epoch 611/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2152 - accuracy: 0.9078 - val_loss: 0.3278 - val_accuracy: 0.8852\n",
      "Epoch 612/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2548 - accuracy: 0.9027 - val_loss: 0.3424 - val_accuracy: 0.8609\n",
      "Epoch 613/1000\n",
      "4068/4068 [==============================] - 0s 93us/step - loss: 0.2400 - accuracy: 0.9046 - val_loss: 0.3700 - val_accuracy: 0.8985\n",
      "Epoch 614/1000\n",
      "4068/4068 [==============================] - 0s 94us/step - loss: 0.2251 - accuracy: 0.9071 - val_loss: 0.3169 - val_accuracy: 0.8852\n",
      "Epoch 615/1000\n",
      "4068/4068 [==============================] - 0s 91us/step - loss: 0.2182 - accuracy: 0.9086 - val_loss: 0.3418 - val_accuracy: 0.9073\n",
      "Epoch 616/1000\n",
      "4068/4068 [==============================] - 0s 93us/step - loss: 0.2262 - accuracy: 0.9098 - val_loss: 0.3665 - val_accuracy: 0.8609\n",
      "Epoch 617/1000\n",
      "4068/4068 [==============================] - 0s 88us/step - loss: 0.2334 - accuracy: 0.9066 - val_loss: 0.3093 - val_accuracy: 0.9073\n",
      "Epoch 618/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.2312 - accuracy: 0.9056 - val_loss: 0.5376 - val_accuracy: 0.9007\n",
      "Epoch 619/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2372 - accuracy: 0.9076 - val_loss: 0.3457 - val_accuracy: 0.9029\n",
      "Epoch 620/1000\n",
      "4068/4068 [==============================] - 0s 91us/step - loss: 0.2195 - accuracy: 0.9083 - val_loss: 0.3827 - val_accuracy: 0.8587\n",
      "Epoch 621/1000\n",
      "4068/4068 [==============================] - 0s 86us/step - loss: 0.2434 - accuracy: 0.9029 - val_loss: 0.3897 - val_accuracy: 0.8985\n",
      "Epoch 622/1000\n",
      "4068/4068 [==============================] - 0s 90us/step - loss: 0.2250 - accuracy: 0.9090 - val_loss: 0.3578 - val_accuracy: 0.8764\n",
      "Epoch 623/1000\n",
      "4068/4068 [==============================] - 0s 88us/step - loss: 0.2264 - accuracy: 0.9051 - val_loss: 0.4438 - val_accuracy: 0.8344\n",
      "Epoch 624/1000\n",
      "4068/4068 [==============================] - 0s 89us/step - loss: 0.2432 - accuracy: 0.9073 - val_loss: 0.4217 - val_accuracy: 0.8278\n",
      "Epoch 625/1000\n",
      "4068/4068 [==============================] - 0s 88us/step - loss: 0.2312 - accuracy: 0.9031 - val_loss: 0.3299 - val_accuracy: 0.8940\n",
      "Epoch 626/1000\n",
      "4068/4068 [==============================] - 0s 91us/step - loss: 0.2184 - accuracy: 0.9122 - val_loss: 0.3366 - val_accuracy: 0.8720\n",
      "Epoch 627/1000\n",
      "4068/4068 [==============================] - 0s 87us/step - loss: 0.2564 - accuracy: 0.9095 - val_loss: 0.3159 - val_accuracy: 0.8962\n",
      "Epoch 628/1000\n",
      "4068/4068 [==============================] - 0s 89us/step - loss: 0.2229 - accuracy: 0.9090 - val_loss: 0.3319 - val_accuracy: 0.8874\n",
      "Epoch 629/1000\n",
      "4068/4068 [==============================] - 0s 88us/step - loss: 0.2185 - accuracy: 0.9105 - val_loss: 0.4013 - val_accuracy: 0.8366\n",
      "Epoch 630/1000\n",
      "4068/4068 [==============================] - 0s 87us/step - loss: 0.2534 - accuracy: 0.9012 - val_loss: 0.3368 - val_accuracy: 0.8720\n",
      "Epoch 631/1000\n",
      "4068/4068 [==============================] - 0s 87us/step - loss: 0.2682 - accuracy: 0.9004 - val_loss: 0.3706 - val_accuracy: 0.8565\n",
      "Epoch 632/1000\n",
      "4068/4068 [==============================] - 0s 87us/step - loss: 0.2222 - accuracy: 0.9066 - val_loss: 0.3497 - val_accuracy: 0.8742\n",
      "Epoch 633/1000\n",
      "4068/4068 [==============================] - 0s 93us/step - loss: 0.2288 - accuracy: 0.9095 - val_loss: 0.3457 - val_accuracy: 0.9029\n",
      "Epoch 634/1000\n",
      "4068/4068 [==============================] - 0s 92us/step - loss: 0.2309 - accuracy: 0.9100 - val_loss: 0.3408 - val_accuracy: 0.8830\n",
      "Epoch 635/1000\n",
      "4068/4068 [==============================] - 0s 90us/step - loss: 0.2189 - accuracy: 0.9071 - val_loss: 0.3249 - val_accuracy: 0.8808\n",
      "Epoch 636/1000\n",
      "4068/4068 [==============================] - 0s 94us/step - loss: 0.2270 - accuracy: 0.9073 - val_loss: 0.3383 - val_accuracy: 0.8675\n",
      "Epoch 637/1000\n",
      "4068/4068 [==============================] - 0s 88us/step - loss: 0.2371 - accuracy: 0.9066 - val_loss: 0.3155 - val_accuracy: 0.8985\n",
      "Epoch 638/1000\n",
      "4068/4068 [==============================] - 0s 91us/step - loss: 0.2293 - accuracy: 0.9152 - val_loss: 0.3426 - val_accuracy: 0.8962\n",
      "Epoch 639/1000\n",
      "4068/4068 [==============================] - 0s 91us/step - loss: 0.2280 - accuracy: 0.9059 - val_loss: 0.3309 - val_accuracy: 0.8852\n",
      "Epoch 640/1000\n",
      "4068/4068 [==============================] - 0s 93us/step - loss: 0.2358 - accuracy: 0.9036 - val_loss: 0.3212 - val_accuracy: 0.8918\n",
      "Epoch 641/1000\n",
      "4068/4068 [==============================] - 0s 92us/step - loss: 0.2199 - accuracy: 0.9127 - val_loss: 0.3256 - val_accuracy: 0.9007\n",
      "Epoch 642/1000\n",
      "4068/4068 [==============================] - 0s 90us/step - loss: 0.2319 - accuracy: 0.9056 - val_loss: 0.3599 - val_accuracy: 0.8521\n",
      "Epoch 643/1000\n",
      "4068/4068 [==============================] - 0s 93us/step - loss: 0.2456 - accuracy: 0.9017 - val_loss: 0.3437 - val_accuracy: 0.8985\n",
      "Epoch 644/1000\n",
      "4068/4068 [==============================] - 0s 86us/step - loss: 0.2231 - accuracy: 0.9088 - val_loss: 0.3673 - val_accuracy: 0.8675\n",
      "Epoch 645/1000\n",
      "4068/4068 [==============================] - 0s 90us/step - loss: 0.2264 - accuracy: 0.9051 - val_loss: 0.3394 - val_accuracy: 0.8808\n",
      "Epoch 646/1000\n",
      "4068/4068 [==============================] - 0s 90us/step - loss: 0.2476 - accuracy: 0.9090 - val_loss: 0.3260 - val_accuracy: 0.8918\n",
      "Epoch 647/1000\n",
      "4068/4068 [==============================] - 0s 89us/step - loss: 0.2385 - accuracy: 0.9054 - val_loss: 0.3390 - val_accuracy: 0.8742\n",
      "Epoch 648/1000\n",
      "4068/4068 [==============================] - 0s 85us/step - loss: 0.2242 - accuracy: 0.9115 - val_loss: 0.3509 - val_accuracy: 0.8918\n",
      "Epoch 649/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2424 - accuracy: 0.9024 - val_loss: 0.3334 - val_accuracy: 0.8985\n",
      "Epoch 650/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2207 - accuracy: 0.9108 - val_loss: 0.3233 - val_accuracy: 0.8918\n",
      "Epoch 651/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2195 - accuracy: 0.9088 - val_loss: 0.3991 - val_accuracy: 0.8985\n",
      "Epoch 652/1000\n",
      "4068/4068 [==============================] - 0s 76us/step - loss: 0.2348 - accuracy: 0.9051 - val_loss: 0.3157 - val_accuracy: 0.8896\n",
      "Epoch 653/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2246 - accuracy: 0.9081 - val_loss: 0.4363 - val_accuracy: 0.8300\n",
      "Epoch 654/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2190 - accuracy: 0.9120 - val_loss: 0.3671 - val_accuracy: 0.8675\n",
      "Epoch 655/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2248 - accuracy: 0.9039 - val_loss: 0.3166 - val_accuracy: 0.8896\n",
      "Epoch 656/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2159 - accuracy: 0.9120 - val_loss: 0.3523 - val_accuracy: 0.8830\n",
      "Epoch 657/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2167 - accuracy: 0.9127 - val_loss: 0.3290 - val_accuracy: 0.8940\n",
      "Epoch 658/1000\n",
      "4068/4068 [==============================] - 0s 92us/step - loss: 0.2214 - accuracy: 0.9078 - val_loss: 0.3372 - val_accuracy: 0.8962\n",
      "Epoch 659/1000\n",
      "4068/4068 [==============================] - 0s 97us/step - loss: 0.2147 - accuracy: 0.9140 - val_loss: 0.4148 - val_accuracy: 0.8940\n",
      "Epoch 660/1000\n",
      "4068/4068 [==============================] - 0s 89us/step - loss: 0.2202 - accuracy: 0.9113 - val_loss: 0.3195 - val_accuracy: 0.8896\n",
      "Epoch 661/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2441 - accuracy: 0.9054 - val_loss: 0.4562 - val_accuracy: 0.9073\n",
      "Epoch 662/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2271 - accuracy: 0.9076 - val_loss: 0.3506 - val_accuracy: 0.9007\n",
      "Epoch 663/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2406 - accuracy: 0.9068 - val_loss: 0.3403 - val_accuracy: 0.8985\n",
      "Epoch 664/1000\n",
      "4068/4068 [==============================] - 0s 76us/step - loss: 0.2220 - accuracy: 0.9081 - val_loss: 0.3564 - val_accuracy: 0.8720\n",
      "Epoch 665/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2211 - accuracy: 0.9088 - val_loss: 0.3287 - val_accuracy: 0.8918\n",
      "Epoch 666/1000\n",
      "4068/4068 [==============================] - 0s 75us/step - loss: 0.2463 - accuracy: 0.9029 - val_loss: 0.3384 - val_accuracy: 0.8874\n",
      "Epoch 667/1000\n",
      "4068/4068 [==============================] - 0s 76us/step - loss: 0.2158 - accuracy: 0.9073 - val_loss: 0.3399 - val_accuracy: 0.8742\n",
      "Epoch 668/1000\n",
      "4068/4068 [==============================] - 0s 75us/step - loss: 0.2280 - accuracy: 0.9019 - val_loss: 0.3183 - val_accuracy: 0.8940\n",
      "Epoch 669/1000\n",
      "4068/4068 [==============================] - ETA: 0s - loss: 0.2384 - accuracy: 0.90 - 0s 76us/step - loss: 0.2360 - accuracy: 0.9031 - val_loss: 0.3346 - val_accuracy: 0.8918\n",
      "Epoch 670/1000\n",
      "4068/4068 [==============================] - 0s 74us/step - loss: 0.2099 - accuracy: 0.9105 - val_loss: 0.3312 - val_accuracy: 0.9051\n",
      "Epoch 671/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2374 - accuracy: 0.9029 - val_loss: 0.3397 - val_accuracy: 0.8896\n",
      "Epoch 672/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2203 - accuracy: 0.9068 - val_loss: 0.3284 - val_accuracy: 0.8985\n",
      "Epoch 673/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2183 - accuracy: 0.9081 - val_loss: 0.4308 - val_accuracy: 0.8256\n",
      "Epoch 674/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2140 - accuracy: 0.9130 - val_loss: 0.3612 - val_accuracy: 0.8653\n",
      "Epoch 675/1000\n",
      "4068/4068 [==============================] - 0s 76us/step - loss: 0.2398 - accuracy: 0.9063 - val_loss: 0.3464 - val_accuracy: 0.8896\n",
      "Epoch 676/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2221 - accuracy: 0.9054 - val_loss: 0.3329 - val_accuracy: 0.8874\n",
      "Epoch 677/1000\n",
      "4068/4068 [==============================] - 0s 75us/step - loss: 0.2224 - accuracy: 0.9100 - val_loss: 0.3311 - val_accuracy: 0.8808\n",
      "Epoch 678/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2171 - accuracy: 0.9127 - val_loss: 0.3326 - val_accuracy: 0.9051\n",
      "Epoch 679/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2081 - accuracy: 0.9118 - val_loss: 0.3223 - val_accuracy: 0.8852\n",
      "Epoch 680/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2291 - accuracy: 0.9090 - val_loss: 0.3244 - val_accuracy: 0.8896\n",
      "Epoch 681/1000\n",
      "4068/4068 [==============================] - 0s 75us/step - loss: 0.2346 - accuracy: 0.9108 - val_loss: 0.3494 - val_accuracy: 0.8808\n",
      "Epoch 682/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2337 - accuracy: 0.9066 - val_loss: 0.4121 - val_accuracy: 0.8389\n",
      "Epoch 683/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2113 - accuracy: 0.9113 - val_loss: 0.3496 - val_accuracy: 0.8786\n",
      "Epoch 684/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2419 - accuracy: 0.9086 - val_loss: 0.3403 - val_accuracy: 0.8852\n",
      "Epoch 685/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2249 - accuracy: 0.9078 - val_loss: 0.3395 - val_accuracy: 0.8808\n",
      "Epoch 686/1000\n",
      "4068/4068 [==============================] - 0s 82us/step - loss: 0.2131 - accuracy: 0.9159 - val_loss: 0.4166 - val_accuracy: 0.9007\n",
      "Epoch 687/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.2321 - accuracy: 0.9081 - val_loss: 0.3529 - val_accuracy: 0.8852\n",
      "Epoch 688/1000\n",
      "4068/4068 [==============================] - 0s 86us/step - loss: 0.2297 - accuracy: 0.9063 - val_loss: 0.3631 - val_accuracy: 0.8631\n",
      "Epoch 689/1000\n",
      "4068/4068 [==============================] - 0s 85us/step - loss: 0.2271 - accuracy: 0.9056 - val_loss: 0.3495 - val_accuracy: 0.9007\n",
      "Epoch 690/1000\n",
      "4068/4068 [==============================] - 0s 84us/step - loss: 0.2236 - accuracy: 0.9132 - val_loss: 0.3572 - val_accuracy: 0.8653\n",
      "Epoch 691/1000\n",
      "4068/4068 [==============================] - 0s 85us/step - loss: 0.2333 - accuracy: 0.9063 - val_loss: 0.5566 - val_accuracy: 0.8057\n",
      "Epoch 692/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2323 - accuracy: 0.9098 - val_loss: 0.4548 - val_accuracy: 0.8322\n",
      "Epoch 693/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2284 - accuracy: 0.9115 - val_loss: 0.3575 - val_accuracy: 0.8653\n",
      "Epoch 694/1000\n",
      "4068/4068 [==============================] - 0s 82us/step - loss: 0.2212 - accuracy: 0.9086 - val_loss: 0.3393 - val_accuracy: 0.9029\n",
      "Epoch 695/1000\n",
      "4068/4068 [==============================] - 0s 82us/step - loss: 0.2110 - accuracy: 0.9113 - val_loss: 0.3619 - val_accuracy: 0.8653\n",
      "Epoch 696/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2167 - accuracy: 0.9118 - val_loss: 0.3638 - val_accuracy: 0.8720\n",
      "Epoch 697/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2186 - accuracy: 0.9093 - val_loss: 0.3594 - val_accuracy: 0.8940\n",
      "Epoch 698/1000\n",
      "4068/4068 [==============================] - ETA: 0s - loss: 0.2208 - accuracy: 0.91 - 0s 83us/step - loss: 0.2211 - accuracy: 0.9176 - val_loss: 0.3584 - val_accuracy: 0.8918\n",
      "Epoch 699/1000\n",
      "4068/4068 [==============================] - 0s 85us/step - loss: 0.2225 - accuracy: 0.9100 - val_loss: 0.3390 - val_accuracy: 0.8962\n",
      "Epoch 700/1000\n",
      "4068/4068 [==============================] - 0s 97us/step - loss: 0.2101 - accuracy: 0.9122 - val_loss: 0.5068 - val_accuracy: 0.9029\n",
      "Epoch 701/1000\n",
      "4068/4068 [==============================] - 0s 87us/step - loss: 0.2188 - accuracy: 0.9152 - val_loss: 0.5475 - val_accuracy: 0.8013\n",
      "Epoch 702/1000\n",
      "4068/4068 [==============================] - 0s 82us/step - loss: 0.2096 - accuracy: 0.9095 - val_loss: 0.4040 - val_accuracy: 0.9029\n",
      "Epoch 703/1000\n",
      "4068/4068 [==============================] - 0s 82us/step - loss: 0.2230 - accuracy: 0.9115 - val_loss: 0.3791 - val_accuracy: 0.9051\n",
      "Epoch 704/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2278 - accuracy: 0.9059 - val_loss: 0.3591 - val_accuracy: 0.8985\n",
      "Epoch 705/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2358 - accuracy: 0.9068 - val_loss: 0.4495 - val_accuracy: 0.8985\n",
      "Epoch 706/1000\n",
      "4068/4068 [==============================] - 0s 103us/step - loss: 0.2359 - accuracy: 0.9068 - val_loss: 0.3574 - val_accuracy: 0.8808\n",
      "Epoch 707/1000\n",
      "4068/4068 [==============================] - 0s 122us/step - loss: 0.2152 - accuracy: 0.9130 - val_loss: 0.3431 - val_accuracy: 0.8852\n",
      "Epoch 708/1000\n",
      "4068/4068 [==============================] - 1s 129us/step - loss: 0.2230 - accuracy: 0.9088 - val_loss: 0.3586 - val_accuracy: 0.9007\n",
      "Epoch 709/1000\n",
      "4068/4068 [==============================] - 0s 98us/step - loss: 0.2340 - accuracy: 0.9044 - val_loss: 0.3393 - val_accuracy: 0.8918\n",
      "Epoch 710/1000\n",
      "4068/4068 [==============================] - 0s 101us/step - loss: 0.2237 - accuracy: 0.9090 - val_loss: 0.3501 - val_accuracy: 0.8940\n",
      "Epoch 711/1000\n",
      "4068/4068 [==============================] - 0s 114us/step - loss: 0.2155 - accuracy: 0.9090 - val_loss: 0.3967 - val_accuracy: 0.8499\n",
      "Epoch 712/1000\n",
      "4068/4068 [==============================] - 0s 112us/step - loss: 0.2264 - accuracy: 0.9095 - val_loss: 0.3743 - val_accuracy: 0.9073\n",
      "Epoch 713/1000\n",
      "4068/4068 [==============================] - 0s 110us/step - loss: 0.2280 - accuracy: 0.9090 - val_loss: 0.3834 - val_accuracy: 0.8675\n",
      "Epoch 714/1000\n",
      "4068/4068 [==============================] - 0s 109us/step - loss: 0.2371 - accuracy: 0.9098 - val_loss: 0.6547 - val_accuracy: 0.8962\n",
      "Epoch 715/1000\n",
      "4068/4068 [==============================] - 0s 116us/step - loss: 0.2289 - accuracy: 0.9081 - val_loss: 0.3649 - val_accuracy: 0.8874\n",
      "Epoch 716/1000\n",
      "4068/4068 [==============================] - 1s 126us/step - loss: 0.2173 - accuracy: 0.9154 - val_loss: 0.3480 - val_accuracy: 0.8830\n",
      "Epoch 717/1000\n",
      "4068/4068 [==============================] - 0s 87us/step - loss: 0.2150 - accuracy: 0.9115 - val_loss: 0.3964 - val_accuracy: 0.8698\n",
      "Epoch 718/1000\n",
      "4068/4068 [==============================] - 0s 90us/step - loss: 0.2085 - accuracy: 0.9154 - val_loss: 0.3732 - val_accuracy: 0.8940\n",
      "Epoch 719/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2150 - accuracy: 0.9142 - val_loss: 0.3625 - val_accuracy: 0.9029\n",
      "Epoch 720/1000\n",
      "4068/4068 [==============================] - 0s 89us/step - loss: 0.2037 - accuracy: 0.9132 - val_loss: 0.4494 - val_accuracy: 0.8322\n",
      "Epoch 721/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2198 - accuracy: 0.9073 - val_loss: 0.3558 - val_accuracy: 0.8940\n",
      "Epoch 722/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2116 - accuracy: 0.9140 - val_loss: 0.3391 - val_accuracy: 0.8852\n",
      "Epoch 723/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.2142 - accuracy: 0.9122 - val_loss: 0.3774 - val_accuracy: 0.8720\n",
      "Epoch 724/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2376 - accuracy: 0.9088 - val_loss: 0.3483 - val_accuracy: 0.8962\n",
      "Epoch 725/1000\n",
      "4068/4068 [==============================] - 0s 85us/step - loss: 0.2200 - accuracy: 0.9083 - val_loss: 0.4376 - val_accuracy: 0.8962\n",
      "Epoch 726/1000\n",
      "4068/4068 [==============================] - 0s 90us/step - loss: 0.2243 - accuracy: 0.9095 - val_loss: 0.3820 - val_accuracy: 0.8631\n",
      "Epoch 727/1000\n",
      "4068/4068 [==============================] - 0s 82us/step - loss: 0.2109 - accuracy: 0.9115 - val_loss: 0.3515 - val_accuracy: 0.8962\n",
      "Epoch 728/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2281 - accuracy: 0.9076 - val_loss: 0.3500 - val_accuracy: 0.8786\n",
      "Epoch 729/1000\n",
      "4068/4068 [==============================] - 0s 84us/step - loss: 0.2064 - accuracy: 0.9189 - val_loss: 0.3689 - val_accuracy: 0.8896\n",
      "Epoch 730/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2040 - accuracy: 0.9181 - val_loss: 0.3712 - val_accuracy: 0.8852\n",
      "Epoch 731/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.2160 - accuracy: 0.9130 - val_loss: 0.3883 - val_accuracy: 0.9051\n",
      "Epoch 732/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2240 - accuracy: 0.9100 - val_loss: 0.4413 - val_accuracy: 0.9051\n",
      "Epoch 733/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.2261 - accuracy: 0.9093 - val_loss: 0.4576 - val_accuracy: 0.9051\n",
      "Epoch 734/1000\n",
      "4068/4068 [==============================] - 0s 83us/step - loss: 0.2294 - accuracy: 0.9088 - val_loss: 0.3688 - val_accuracy: 0.8631\n",
      "Epoch 735/1000\n",
      "4068/4068 [==============================] - 0s 85us/step - loss: 0.2177 - accuracy: 0.9162 - val_loss: 0.3647 - val_accuracy: 0.8962\n",
      "Epoch 736/1000\n",
      "4068/4068 [==============================] - 0s 95us/step - loss: 0.2094 - accuracy: 0.9157 - val_loss: 0.3362 - val_accuracy: 0.8896\n",
      "Epoch 737/1000\n",
      "4068/4068 [==============================] - 0s 89us/step - loss: 0.2138 - accuracy: 0.9167 - val_loss: 0.3789 - val_accuracy: 0.9007\n",
      "Epoch 738/1000\n",
      "4068/4068 [==============================] - 0s 85us/step - loss: 0.2265 - accuracy: 0.9127 - val_loss: 0.3767 - val_accuracy: 0.8918\n",
      "Epoch 739/1000\n",
      "4068/4068 [==============================] - 0s 87us/step - loss: 0.2215 - accuracy: 0.9125 - val_loss: 0.3553 - val_accuracy: 0.8808\n",
      "Epoch 740/1000\n",
      "4068/4068 [==============================] - 0s 84us/step - loss: 0.2159 - accuracy: 0.9130 - val_loss: 0.3541 - val_accuracy: 0.9073\n",
      "Epoch 741/1000\n",
      "4068/4068 [==============================] - 0s 86us/step - loss: 0.2201 - accuracy: 0.9122 - val_loss: 0.3843 - val_accuracy: 0.8874\n",
      "Epoch 742/1000\n",
      "4068/4068 [==============================] - 0s 85us/step - loss: 0.2217 - accuracy: 0.9068 - val_loss: 0.3515 - val_accuracy: 0.8786\n",
      "Epoch 743/1000\n",
      "4068/4068 [==============================] - 0s 83us/step - loss: 0.2267 - accuracy: 0.9122 - val_loss: 0.3457 - val_accuracy: 0.8874\n",
      "Epoch 744/1000\n",
      "4068/4068 [==============================] - 0s 99us/step - loss: 0.2192 - accuracy: 0.9100 - val_loss: 0.3694 - val_accuracy: 0.8896\n",
      "Epoch 745/1000\n",
      "4068/4068 [==============================] - 0s 95us/step - loss: 0.2039 - accuracy: 0.9174 - val_loss: 0.3439 - val_accuracy: 0.8830\n",
      "Epoch 746/1000\n",
      "4068/4068 [==============================] - 0s 86us/step - loss: 0.2313 - accuracy: 0.9066 - val_loss: 0.3994 - val_accuracy: 0.9029\n",
      "Epoch 747/1000\n",
      "4068/4068 [==============================] - 0s 97us/step - loss: 0.2288 - accuracy: 0.9036 - val_loss: 0.3661 - val_accuracy: 0.9007\n",
      "Epoch 748/1000\n",
      "4068/4068 [==============================] - 0s 91us/step - loss: 0.2138 - accuracy: 0.9157 - val_loss: 0.3714 - val_accuracy: 0.9007\n",
      "Epoch 749/1000\n",
      "4068/4068 [==============================] - 0s 113us/step - loss: 0.2332 - accuracy: 0.9081 - val_loss: 0.3732 - val_accuracy: 0.8874\n",
      "Epoch 750/1000\n",
      "4068/4068 [==============================] - 1s 129us/step - loss: 0.2089 - accuracy: 0.9118 - val_loss: 0.3882 - val_accuracy: 0.8653\n",
      "Epoch 751/1000\n",
      "4068/4068 [==============================] - 0s 113us/step - loss: 0.2219 - accuracy: 0.9063 - val_loss: 0.3608 - val_accuracy: 0.8830\n",
      "Epoch 752/1000\n",
      "4068/4068 [==============================] - 0s 106us/step - loss: 0.2052 - accuracy: 0.9169 - val_loss: 0.3469 - val_accuracy: 0.8896\n",
      "Epoch 753/1000\n",
      "4068/4068 [==============================] - 0s 106us/step - loss: 0.2112 - accuracy: 0.9108 - val_loss: 0.3577 - val_accuracy: 0.8918\n",
      "Epoch 754/1000\n",
      "4068/4068 [==============================] - 0s 105us/step - loss: 0.2536 - accuracy: 0.9051 - val_loss: 0.3961 - val_accuracy: 0.8675\n",
      "Epoch 755/1000\n",
      "4068/4068 [==============================] - 0s 95us/step - loss: 0.2134 - accuracy: 0.9152 - val_loss: 0.3564 - val_accuracy: 0.8896\n",
      "Epoch 756/1000\n",
      "4068/4068 [==============================] - 0s 102us/step - loss: 0.2312 - accuracy: 0.9061 - val_loss: 0.3895 - val_accuracy: 0.9051\n",
      "Epoch 757/1000\n",
      "4068/4068 [==============================] - 0s 105us/step - loss: 0.2110 - accuracy: 0.9164 - val_loss: 0.3580 - val_accuracy: 0.8962\n",
      "Epoch 758/1000\n",
      "4068/4068 [==============================] - 0s 101us/step - loss: 0.2221 - accuracy: 0.9081 - val_loss: 0.4226 - val_accuracy: 0.8433\n",
      "Epoch 759/1000\n",
      "4068/4068 [==============================] - 0s 88us/step - loss: 0.2138 - accuracy: 0.9137 - val_loss: 0.4049 - val_accuracy: 0.8609\n",
      "Epoch 760/1000\n",
      "4068/4068 [==============================] - 0s 104us/step - loss: 0.2192 - accuracy: 0.9118 - val_loss: 0.5034 - val_accuracy: 0.8874\n",
      "Epoch 761/1000\n",
      "4068/4068 [==============================] - 0s 92us/step - loss: 0.2103 - accuracy: 0.9157 - val_loss: 0.3599 - val_accuracy: 0.8896\n",
      "Epoch 762/1000\n",
      "4068/4068 [==============================] - 0s 88us/step - loss: 0.2071 - accuracy: 0.9164 - val_loss: 0.3730 - val_accuracy: 0.8675\n",
      "Epoch 763/1000\n",
      "4068/4068 [==============================] - 0s 88us/step - loss: 0.2083 - accuracy: 0.9113 - val_loss: 0.3668 - val_accuracy: 0.9029\n",
      "Epoch 764/1000\n",
      "4068/4068 [==============================] - 0s 86us/step - loss: 0.2192 - accuracy: 0.9127 - val_loss: 0.3643 - val_accuracy: 0.9007\n",
      "Epoch 765/1000\n",
      "4068/4068 [==============================] - 0s 82us/step - loss: 0.2127 - accuracy: 0.9100 - val_loss: 0.3888 - val_accuracy: 0.9007\n",
      "Epoch 766/1000\n",
      "4068/4068 [==============================] - 0s 83us/step - loss: 0.2069 - accuracy: 0.9186 - val_loss: 0.4425 - val_accuracy: 0.8918\n",
      "Epoch 767/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2116 - accuracy: 0.9118 - val_loss: 0.3330 - val_accuracy: 0.8985\n",
      "Epoch 768/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.2197 - accuracy: 0.9095 - val_loss: 0.3945 - val_accuracy: 0.8455\n",
      "Epoch 769/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2273 - accuracy: 0.9110 - val_loss: 0.3902 - val_accuracy: 0.9007\n",
      "Epoch 770/1000\n",
      "4068/4068 [==============================] - 0s 90us/step - loss: 0.1985 - accuracy: 0.9179 - val_loss: 0.3898 - val_accuracy: 0.8631\n",
      "Epoch 771/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2182 - accuracy: 0.9122 - val_loss: 0.3877 - val_accuracy: 0.8653\n",
      "Epoch 772/1000\n",
      "4068/4068 [==============================] - 0s 82us/step - loss: 0.2164 - accuracy: 0.9118 - val_loss: 0.3819 - val_accuracy: 0.8896\n",
      "Epoch 773/1000\n",
      "4068/4068 [==============================] - 0s 83us/step - loss: 0.2067 - accuracy: 0.9152 - val_loss: 0.3959 - val_accuracy: 0.8675\n",
      "Epoch 774/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2241 - accuracy: 0.9044 - val_loss: 0.3713 - val_accuracy: 0.8698\n",
      "Epoch 775/1000\n",
      "4068/4068 [==============================] - 0s 84us/step - loss: 0.2172 - accuracy: 0.9098 - val_loss: 0.3685 - val_accuracy: 0.8918\n",
      "Epoch 776/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.2064 - accuracy: 0.9179 - val_loss: 0.4060 - val_accuracy: 0.8742\n",
      "Epoch 777/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2141 - accuracy: 0.9142 - val_loss: 0.3924 - val_accuracy: 0.8675\n",
      "Epoch 778/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2091 - accuracy: 0.9125 - val_loss: 0.3544 - val_accuracy: 0.8962\n",
      "Epoch 779/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.2073 - accuracy: 0.9137 - val_loss: 0.3333 - val_accuracy: 0.8940\n",
      "Epoch 780/1000\n",
      "4068/4068 [==============================] - 0s 82us/step - loss: 0.2013 - accuracy: 0.9186 - val_loss: 0.3417 - val_accuracy: 0.9007\n",
      "Epoch 781/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2011 - accuracy: 0.9172 - val_loss: 0.3440 - val_accuracy: 0.8940\n",
      "Epoch 782/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2144 - accuracy: 0.9159 - val_loss: 0.3629 - val_accuracy: 0.8742\n",
      "Epoch 783/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2069 - accuracy: 0.9142 - val_loss: 0.4029 - val_accuracy: 0.8698\n",
      "Epoch 784/1000\n",
      "4068/4068 [==============================] - 0s 84us/step - loss: 0.2245 - accuracy: 0.9110 - val_loss: 0.3395 - val_accuracy: 0.9029\n",
      "Epoch 785/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2154 - accuracy: 0.9169 - val_loss: 0.3421 - val_accuracy: 0.8874\n",
      "Epoch 786/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.2042 - accuracy: 0.9176 - val_loss: 0.3604 - val_accuracy: 0.8830\n",
      "Epoch 787/1000\n",
      "4068/4068 [==============================] - 0s 92us/step - loss: 0.2004 - accuracy: 0.9122 - val_loss: 0.3591 - val_accuracy: 0.8698\n",
      "Epoch 788/1000\n",
      "4068/4068 [==============================] - 0s 94us/step - loss: 0.2203 - accuracy: 0.9118 - val_loss: 0.3807 - val_accuracy: 0.8675\n",
      "Epoch 789/1000\n",
      "4068/4068 [==============================] - 0s 98us/step - loss: 0.2036 - accuracy: 0.9184 - val_loss: 0.3695 - val_accuracy: 0.8830\n",
      "Epoch 790/1000\n",
      "4068/4068 [==============================] - 0s 99us/step - loss: 0.2196 - accuracy: 0.9110 - val_loss: 0.3696 - val_accuracy: 0.8786\n",
      "Epoch 791/1000\n",
      "4068/4068 [==============================] - 0s 84us/step - loss: 0.2100 - accuracy: 0.9181 - val_loss: 0.3889 - val_accuracy: 0.9029\n",
      "Epoch 792/1000\n",
      "4068/4068 [==============================] - 0s 95us/step - loss: 0.2114 - accuracy: 0.9110 - val_loss: 0.3782 - val_accuracy: 0.8631\n",
      "Epoch 793/1000\n",
      "4068/4068 [==============================] - 0s 112us/step - loss: 0.2056 - accuracy: 0.9176 - val_loss: 0.3913 - val_accuracy: 0.8896\n",
      "Epoch 794/1000\n",
      "4068/4068 [==============================] - 1s 133us/step - loss: 0.2095 - accuracy: 0.9169 - val_loss: 0.3838 - val_accuracy: 0.8653\n",
      "Epoch 795/1000\n",
      "4068/4068 [==============================] - 0s 109us/step - loss: 0.2146 - accuracy: 0.9115 - val_loss: 0.4820 - val_accuracy: 0.8433\n",
      "Epoch 796/1000\n",
      "4068/4068 [==============================] - 0s 107us/step - loss: 0.2171 - accuracy: 0.9086 - val_loss: 0.4183 - val_accuracy: 0.8985\n",
      "Epoch 797/1000\n",
      "4068/4068 [==============================] - 0s 112us/step - loss: 0.2104 - accuracy: 0.9152 - val_loss: 0.3811 - val_accuracy: 0.8565\n",
      "Epoch 798/1000\n",
      "4068/4068 [==============================] - 0s 99us/step - loss: 0.2015 - accuracy: 0.9189 - val_loss: 0.4396 - val_accuracy: 0.8918\n",
      "Epoch 799/1000\n",
      "4068/4068 [==============================] - 0s 96us/step - loss: 0.2097 - accuracy: 0.9164 - val_loss: 0.4115 - val_accuracy: 0.8433\n",
      "Epoch 800/1000\n",
      "4068/4068 [==============================] - 0s 87us/step - loss: 0.2109 - accuracy: 0.9130 - val_loss: 0.4372 - val_accuracy: 0.8918\n",
      "Epoch 801/1000\n",
      "4068/4068 [==============================] - 0s 98us/step - loss: 0.2037 - accuracy: 0.9179 - val_loss: 0.3679 - val_accuracy: 0.8874\n",
      "Epoch 802/1000\n",
      "4068/4068 [==============================] - 0s 88us/step - loss: 0.2255 - accuracy: 0.9113 - val_loss: 0.4374 - val_accuracy: 0.9095\n",
      "Epoch 803/1000\n",
      "4068/4068 [==============================] - 0s 85us/step - loss: 0.2057 - accuracy: 0.9142 - val_loss: 0.4629 - val_accuracy: 0.8389\n",
      "Epoch 804/1000\n",
      "4068/4068 [==============================] - 0s 95us/step - loss: 0.2222 - accuracy: 0.9140 - val_loss: 0.5011 - val_accuracy: 0.8212\n",
      "Epoch 805/1000\n",
      "4068/4068 [==============================] - 0s 89us/step - loss: 0.2113 - accuracy: 0.9164 - val_loss: 0.3672 - val_accuracy: 0.8830\n",
      "Epoch 806/1000\n",
      "4068/4068 [==============================] - 0s 96us/step - loss: 0.2114 - accuracy: 0.9110 - val_loss: 0.3508 - val_accuracy: 0.9029\n",
      "Epoch 807/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.2009 - accuracy: 0.9189 - val_loss: 0.3823 - val_accuracy: 0.8940\n",
      "Epoch 808/1000\n",
      "4068/4068 [==============================] - 0s 85us/step - loss: 0.2105 - accuracy: 0.9132 - val_loss: 0.4666 - val_accuracy: 0.8985\n",
      "Epoch 809/1000\n",
      "4068/4068 [==============================] - 0s 77us/step - loss: 0.2145 - accuracy: 0.9167 - val_loss: 0.5336 - val_accuracy: 0.7991\n",
      "Epoch 810/1000\n",
      "4068/4068 [==============================] - 0s 84us/step - loss: 0.2132 - accuracy: 0.9142 - val_loss: 0.3706 - val_accuracy: 0.9007\n",
      "Epoch 811/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2036 - accuracy: 0.9162 - val_loss: 0.5012 - val_accuracy: 0.8278\n",
      "Epoch 812/1000\n",
      "4068/4068 [==============================] - 0s 83us/step - loss: 0.2098 - accuracy: 0.9179 - val_loss: 0.3685 - val_accuracy: 0.8896\n",
      "Epoch 813/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2121 - accuracy: 0.9110 - val_loss: 0.3844 - val_accuracy: 0.8896\n",
      "Epoch 814/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2061 - accuracy: 0.9154 - val_loss: 0.4477 - val_accuracy: 0.8962\n",
      "Epoch 815/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2107 - accuracy: 0.9113 - val_loss: 0.3973 - val_accuracy: 0.8830\n",
      "Epoch 816/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.2109 - accuracy: 0.9130 - val_loss: 0.3930 - val_accuracy: 0.8852\n",
      "Epoch 817/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2229 - accuracy: 0.9083 - val_loss: 0.3677 - val_accuracy: 0.8874\n",
      "Epoch 818/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.2075 - accuracy: 0.9154 - val_loss: 0.4479 - val_accuracy: 0.8477\n",
      "Epoch 819/1000\n",
      "4068/4068 [==============================] - 0s 85us/step - loss: 0.2096 - accuracy: 0.9132 - val_loss: 0.3769 - val_accuracy: 0.8808\n",
      "Epoch 820/1000\n",
      "4068/4068 [==============================] - 0s 84us/step - loss: 0.1988 - accuracy: 0.9179 - val_loss: 0.3712 - val_accuracy: 0.8675\n",
      "Epoch 821/1000\n",
      "4068/4068 [==============================] - 0s 93us/step - loss: 0.2019 - accuracy: 0.9147 - val_loss: 0.3540 - val_accuracy: 0.8985\n",
      "Epoch 822/1000\n",
      "4068/4068 [==============================] - 0s 85us/step - loss: 0.2038 - accuracy: 0.9199 - val_loss: 0.3864 - val_accuracy: 0.8808\n",
      "Epoch 823/1000\n",
      "4068/4068 [==============================] - 0s 97us/step - loss: 0.2052 - accuracy: 0.9145 - val_loss: 0.3745 - val_accuracy: 0.8896\n",
      "Epoch 824/1000\n",
      "4068/4068 [==============================] - 0s 85us/step - loss: 0.2312 - accuracy: 0.9132 - val_loss: 0.3594 - val_accuracy: 0.8940\n",
      "Epoch 825/1000\n",
      "4068/4068 [==============================] - 0s 87us/step - loss: 0.2002 - accuracy: 0.9164 - val_loss: 0.3744 - val_accuracy: 0.8631\n",
      "Epoch 826/1000\n",
      "4068/4068 [==============================] - 0s 86us/step - loss: 0.2053 - accuracy: 0.9169 - val_loss: 0.3617 - val_accuracy: 0.9051\n",
      "Epoch 827/1000\n",
      "4068/4068 [==============================] - 0s 102us/step - loss: 0.2188 - accuracy: 0.9127 - val_loss: 0.3753 - val_accuracy: 0.8764\n",
      "Epoch 828/1000\n",
      "4068/4068 [==============================] - 0s 108us/step - loss: 0.2019 - accuracy: 0.9186 - val_loss: 0.5372 - val_accuracy: 0.8124\n",
      "Epoch 829/1000\n",
      "4068/4068 [==============================] - 0s 105us/step - loss: 0.2264 - accuracy: 0.9056 - val_loss: 0.3889 - val_accuracy: 0.9029\n",
      "Epoch 830/1000\n",
      "4068/4068 [==============================] - 0s 101us/step - loss: 0.2009 - accuracy: 0.9164 - val_loss: 0.6202 - val_accuracy: 0.9007\n",
      "Epoch 831/1000\n",
      "4068/4068 [==============================] - 0s 91us/step - loss: 0.2109 - accuracy: 0.9159 - val_loss: 0.3894 - val_accuracy: 0.8808\n",
      "Epoch 832/1000\n",
      "4068/4068 [==============================] - 0s 105us/step - loss: 0.2182 - accuracy: 0.9147 - val_loss: 0.3692 - val_accuracy: 0.8962\n",
      "Epoch 833/1000\n",
      "4068/4068 [==============================] - 0s 91us/step - loss: 0.2015 - accuracy: 0.9147 - val_loss: 0.3621 - val_accuracy: 0.8896\n",
      "Epoch 834/1000\n",
      "4068/4068 [==============================] - 0s 91us/step - loss: 0.1941 - accuracy: 0.9221 - val_loss: 0.3672 - val_accuracy: 0.8830\n",
      "Epoch 835/1000\n",
      "4068/4068 [==============================] - 0s 106us/step - loss: 0.2120 - accuracy: 0.9176 - val_loss: 0.3647 - val_accuracy: 0.8808\n",
      "Epoch 836/1000\n",
      "4068/4068 [==============================] - 1s 124us/step - loss: 0.2109 - accuracy: 0.9130 - val_loss: 0.3763 - val_accuracy: 0.8830\n",
      "Epoch 837/1000\n",
      "4068/4068 [==============================] - 0s 93us/step - loss: 0.2111 - accuracy: 0.9125 - val_loss: 0.3567 - val_accuracy: 0.9007\n",
      "Epoch 838/1000\n",
      "4068/4068 [==============================] - 0s 87us/step - loss: 0.2032 - accuracy: 0.9142 - val_loss: 0.3881 - val_accuracy: 0.9007\n",
      "Epoch 839/1000\n",
      "4068/4068 [==============================] - 0s 98us/step - loss: 0.2075 - accuracy: 0.9130 - val_loss: 0.3904 - val_accuracy: 0.8653\n",
      "Epoch 840/1000\n",
      "4068/4068 [==============================] - 0s 83us/step - loss: 0.2053 - accuracy: 0.9167 - val_loss: 0.3548 - val_accuracy: 0.8940\n",
      "Epoch 841/1000\n",
      "4068/4068 [==============================] - 0s 102us/step - loss: 0.2087 - accuracy: 0.9169 - val_loss: 0.3709 - val_accuracy: 0.8940\n",
      "Epoch 842/1000\n",
      "4068/4068 [==============================] - 0s 84us/step - loss: 0.2194 - accuracy: 0.9110 - val_loss: 0.4518 - val_accuracy: 0.8322\n",
      "Epoch 843/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2145 - accuracy: 0.9149 - val_loss: 0.3686 - val_accuracy: 0.9029\n",
      "Epoch 844/1000\n",
      "4068/4068 [==============================] - 0s 75us/step - loss: 0.2027 - accuracy: 0.9142 - val_loss: 0.4198 - val_accuracy: 0.8477\n",
      "Epoch 845/1000\n",
      "4068/4068 [==============================] - 0s 84us/step - loss: 0.2196 - accuracy: 0.9132 - val_loss: 0.3853 - val_accuracy: 0.8786\n",
      "Epoch 846/1000\n",
      "4068/4068 [==============================] - 0s 87us/step - loss: 0.2053 - accuracy: 0.9159 - val_loss: 0.3956 - val_accuracy: 0.8675\n",
      "Epoch 847/1000\n",
      "4068/4068 [==============================] - 0s 93us/step - loss: 0.2028 - accuracy: 0.9176 - val_loss: 0.3913 - val_accuracy: 0.9029\n",
      "Epoch 848/1000\n",
      "4068/4068 [==============================] - 0s 87us/step - loss: 0.2095 - accuracy: 0.9132 - val_loss: 0.4633 - val_accuracy: 0.8344\n",
      "Epoch 849/1000\n",
      "4068/4068 [==============================] - 0s 85us/step - loss: 0.1998 - accuracy: 0.9157 - val_loss: 0.3665 - val_accuracy: 0.9029\n",
      "Epoch 850/1000\n",
      "4068/4068 [==============================] - 0s 82us/step - loss: 0.2157 - accuracy: 0.9110 - val_loss: 0.3748 - val_accuracy: 0.8985\n",
      "Epoch 851/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.1998 - accuracy: 0.9213 - val_loss: 0.3805 - val_accuracy: 0.8985\n",
      "Epoch 852/1000\n",
      "4068/4068 [==============================] - 0s 86us/step - loss: 0.2053 - accuracy: 0.9159 - val_loss: 0.3713 - val_accuracy: 0.8764\n",
      "Epoch 853/1000\n",
      "4068/4068 [==============================] - 0s 93us/step - loss: 0.2096 - accuracy: 0.9179 - val_loss: 0.3447 - val_accuracy: 0.8985\n",
      "Epoch 854/1000\n",
      "4068/4068 [==============================] - 0s 82us/step - loss: 0.2224 - accuracy: 0.9137 - val_loss: 0.4029 - val_accuracy: 0.8808\n",
      "Epoch 855/1000\n",
      "4068/4068 [==============================] - 0s 83us/step - loss: 0.2008 - accuracy: 0.9179 - val_loss: 0.3624 - val_accuracy: 0.8830\n",
      "Epoch 856/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.2065 - accuracy: 0.9157 - val_loss: 0.4819 - val_accuracy: 0.9029\n",
      "Epoch 857/1000\n",
      "4068/4068 [==============================] - 0s 83us/step - loss: 0.1991 - accuracy: 0.9204 - val_loss: 0.3841 - val_accuracy: 0.8962\n",
      "Epoch 858/1000\n",
      "4068/4068 [==============================] - 0s 82us/step - loss: 0.2052 - accuracy: 0.9147 - val_loss: 0.4365 - val_accuracy: 0.8543\n",
      "Epoch 859/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2083 - accuracy: 0.9164 - val_loss: 0.4331 - val_accuracy: 0.8985\n",
      "Epoch 860/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.2125 - accuracy: 0.9191 - val_loss: 0.3995 - val_accuracy: 0.9007\n",
      "Epoch 861/1000\n",
      "4068/4068 [==============================] - 0s 84us/step - loss: 0.2000 - accuracy: 0.9149 - val_loss: 0.4291 - val_accuracy: 0.8940\n",
      "Epoch 862/1000\n",
      "4068/4068 [==============================] - 0s 85us/step - loss: 0.1917 - accuracy: 0.9216 - val_loss: 0.4108 - val_accuracy: 0.8808\n",
      "Epoch 863/1000\n",
      "4068/4068 [==============================] - 0s 85us/step - loss: 0.2020 - accuracy: 0.9191 - val_loss: 0.3749 - val_accuracy: 0.8896\n",
      "Epoch 864/1000\n",
      "4068/4068 [==============================] - 0s 82us/step - loss: 0.2022 - accuracy: 0.9189 - val_loss: 0.3942 - val_accuracy: 0.8698\n",
      "Epoch 865/1000\n",
      "4068/4068 [==============================] - 0s 98us/step - loss: 0.2387 - accuracy: 0.9083 - val_loss: 0.3782 - val_accuracy: 0.8874\n",
      "Epoch 866/1000\n",
      "4068/4068 [==============================] - 0s 95us/step - loss: 0.2054 - accuracy: 0.9167 - val_loss: 0.3702 - val_accuracy: 0.8786\n",
      "Epoch 867/1000\n",
      "4068/4068 [==============================] - 0s 91us/step - loss: 0.2019 - accuracy: 0.9174 - val_loss: 0.3721 - val_accuracy: 0.8985\n",
      "Epoch 868/1000\n",
      "4068/4068 [==============================] - 0s 103us/step - loss: 0.1928 - accuracy: 0.9147 - val_loss: 0.3878 - val_accuracy: 0.8742\n",
      "Epoch 869/1000\n",
      "4068/4068 [==============================] - 0s 91us/step - loss: 0.1920 - accuracy: 0.9184 - val_loss: 0.3832 - val_accuracy: 0.8653\n",
      "Epoch 870/1000\n",
      "4068/4068 [==============================] - 0s 109us/step - loss: 0.2136 - accuracy: 0.9147 - val_loss: 0.3659 - val_accuracy: 0.9007\n",
      "Epoch 871/1000\n",
      "4068/4068 [==============================] - 0s 106us/step - loss: 0.1945 - accuracy: 0.9149 - val_loss: 0.3973 - val_accuracy: 0.8830\n",
      "Epoch 872/1000\n",
      "4068/4068 [==============================] - 0s 93us/step - loss: 0.2062 - accuracy: 0.9127 - val_loss: 0.4805 - val_accuracy: 0.8985\n",
      "Epoch 873/1000\n",
      "4068/4068 [==============================] - 0s 86us/step - loss: 0.2174 - accuracy: 0.9157 - val_loss: 0.3897 - val_accuracy: 0.8742\n",
      "Epoch 874/1000\n",
      "4068/4068 [==============================] - 0s 96us/step - loss: 0.1975 - accuracy: 0.9172 - val_loss: 0.3784 - val_accuracy: 0.8962\n",
      "Epoch 875/1000\n",
      "4068/4068 [==============================] - 0s 102us/step - loss: 0.2100 - accuracy: 0.9169 - val_loss: 0.3637 - val_accuracy: 0.8962\n",
      "Epoch 876/1000\n",
      "4068/4068 [==============================] - 0s 93us/step - loss: 0.2046 - accuracy: 0.9169 - val_loss: 0.4443 - val_accuracy: 0.8411\n",
      "Epoch 877/1000\n",
      "4068/4068 [==============================] - 0s 92us/step - loss: 0.2013 - accuracy: 0.9164 - val_loss: 0.3872 - val_accuracy: 0.8852\n",
      "Epoch 878/1000\n",
      "4068/4068 [==============================] - 0s 94us/step - loss: 0.2312 - accuracy: 0.9103 - val_loss: 0.4049 - val_accuracy: 0.8830\n",
      "Epoch 879/1000\n",
      "4068/4068 [==============================] - 0s 116us/step - loss: 0.1987 - accuracy: 0.9167 - val_loss: 0.3745 - val_accuracy: 0.8918\n",
      "Epoch 880/1000\n",
      "4068/4068 [==============================] - 0s 121us/step - loss: 0.1981 - accuracy: 0.9174 - val_loss: 0.3909 - val_accuracy: 0.8609\n",
      "Epoch 881/1000\n",
      "4068/4068 [==============================] - 0s 106us/step - loss: 0.2126 - accuracy: 0.9142 - val_loss: 0.4076 - val_accuracy: 0.8675\n",
      "Epoch 882/1000\n",
      "4068/4068 [==============================] - 0s 100us/step - loss: 0.1947 - accuracy: 0.9142 - val_loss: 0.3750 - val_accuracy: 0.8985\n",
      "Epoch 883/1000\n",
      "4068/4068 [==============================] - 0s 99us/step - loss: 0.1930 - accuracy: 0.9218 - val_loss: 0.4226 - val_accuracy: 0.8521\n",
      "Epoch 884/1000\n",
      "4068/4068 [==============================] - 0s 96us/step - loss: 0.2276 - accuracy: 0.9078 - val_loss: 0.3904 - val_accuracy: 0.8675\n",
      "Epoch 885/1000\n",
      "4068/4068 [==============================] - 0s 90us/step - loss: 0.1983 - accuracy: 0.9235 - val_loss: 0.4116 - val_accuracy: 0.8565\n",
      "Epoch 886/1000\n",
      "4068/4068 [==============================] - 0s 88us/step - loss: 0.2036 - accuracy: 0.9162 - val_loss: 0.3898 - val_accuracy: 0.8985\n",
      "Epoch 887/1000\n",
      "4068/4068 [==============================] - 0s 91us/step - loss: 0.2010 - accuracy: 0.9204 - val_loss: 0.4266 - val_accuracy: 0.8543\n",
      "Epoch 888/1000\n",
      "4068/4068 [==============================] - 0s 85us/step - loss: 0.1934 - accuracy: 0.9174 - val_loss: 0.3919 - val_accuracy: 0.8742\n",
      "Epoch 889/1000\n",
      "4068/4068 [==============================] - 0s 93us/step - loss: 0.2105 - accuracy: 0.9140 - val_loss: 0.4511 - val_accuracy: 0.9029\n",
      "Epoch 890/1000\n",
      "4068/4068 [==============================] - 0s 85us/step - loss: 0.2121 - accuracy: 0.9130 - val_loss: 0.3938 - val_accuracy: 0.8896\n",
      "Epoch 891/1000\n",
      "4068/4068 [==============================] - 0s 96us/step - loss: 0.1982 - accuracy: 0.9174 - val_loss: 0.4215 - val_accuracy: 0.8918\n",
      "Epoch 892/1000\n",
      "4068/4068 [==============================] - 0s 103us/step - loss: 0.1946 - accuracy: 0.9189 - val_loss: 0.4403 - val_accuracy: 0.8433\n",
      "Epoch 893/1000\n",
      "4068/4068 [==============================] - 0s 107us/step - loss: 0.2105 - accuracy: 0.9140 - val_loss: 0.3897 - val_accuracy: 0.8808\n",
      "Epoch 894/1000\n",
      "4068/4068 [==============================] - 0s 102us/step - loss: 0.2108 - accuracy: 0.9179 - val_loss: 0.4735 - val_accuracy: 0.8366\n",
      "Epoch 895/1000\n",
      "4068/4068 [==============================] - 0s 105us/step - loss: 0.2034 - accuracy: 0.9157 - val_loss: 0.3986 - val_accuracy: 0.8631\n",
      "Epoch 896/1000\n",
      "4068/4068 [==============================] - 0s 98us/step - loss: 0.1972 - accuracy: 0.9228 - val_loss: 0.3905 - val_accuracy: 0.9029\n",
      "Epoch 897/1000\n",
      "4068/4068 [==============================] - 0s 87us/step - loss: 0.2072 - accuracy: 0.9152 - val_loss: 0.4117 - val_accuracy: 0.8521\n",
      "Epoch 898/1000\n",
      "4068/4068 [==============================] - 0s 93us/step - loss: 0.2070 - accuracy: 0.9142 - val_loss: 0.3813 - val_accuracy: 0.8852\n",
      "Epoch 899/1000\n",
      "4068/4068 [==============================] - 0s 84us/step - loss: 0.2037 - accuracy: 0.9196 - val_loss: 0.4503 - val_accuracy: 0.8521\n",
      "Epoch 900/1000\n",
      "4068/4068 [==============================] - 0s 84us/step - loss: 0.2058 - accuracy: 0.9184 - val_loss: 0.3908 - val_accuracy: 0.8962\n",
      "Epoch 901/1000\n",
      "4068/4068 [==============================] - 0s 82us/step - loss: 0.1932 - accuracy: 0.9194 - val_loss: 0.4215 - val_accuracy: 0.8521\n",
      "Epoch 902/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2065 - accuracy: 0.9174 - val_loss: 0.3824 - val_accuracy: 0.8918\n",
      "Epoch 903/1000\n",
      "4068/4068 [==============================] - 0s 83us/step - loss: 0.1960 - accuracy: 0.9159 - val_loss: 0.3850 - val_accuracy: 0.8985\n",
      "Epoch 904/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.1933 - accuracy: 0.9228 - val_loss: 0.3780 - val_accuracy: 0.8962\n",
      "Epoch 905/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2019 - accuracy: 0.9196 - val_loss: 0.4023 - val_accuracy: 0.9073\n",
      "Epoch 906/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2004 - accuracy: 0.9147 - val_loss: 0.3940 - val_accuracy: 0.9007\n",
      "Epoch 907/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.1949 - accuracy: 0.9179 - val_loss: 0.3703 - val_accuracy: 0.9051\n",
      "Epoch 908/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2012 - accuracy: 0.9179 - val_loss: 0.3891 - val_accuracy: 0.8742\n",
      "Epoch 909/1000\n",
      "4068/4068 [==============================] - 0s 82us/step - loss: 0.2061 - accuracy: 0.9154 - val_loss: 0.4312 - val_accuracy: 0.8587\n",
      "Epoch 910/1000\n",
      "4068/4068 [==============================] - 0s 84us/step - loss: 0.1940 - accuracy: 0.9238 - val_loss: 0.4233 - val_accuracy: 0.9007\n",
      "Epoch 911/1000\n",
      "4068/4068 [==============================] - 0s 94us/step - loss: 0.1982 - accuracy: 0.9189 - val_loss: 0.3902 - val_accuracy: 0.8808\n",
      "Epoch 912/1000\n",
      "4068/4068 [==============================] - 0s 102us/step - loss: 0.2005 - accuracy: 0.9159 - val_loss: 0.4193 - val_accuracy: 0.8631\n",
      "Epoch 913/1000\n",
      "4068/4068 [==============================] - 0s 111us/step - loss: 0.1901 - accuracy: 0.9189 - val_loss: 0.3842 - val_accuracy: 0.8896\n",
      "Epoch 914/1000\n",
      "4068/4068 [==============================] - 0s 110us/step - loss: 0.2091 - accuracy: 0.9169 - val_loss: 0.4032 - val_accuracy: 0.8698\n",
      "Epoch 915/1000\n",
      "4068/4068 [==============================] - 0s 105us/step - loss: 0.1956 - accuracy: 0.9226 - val_loss: 0.4346 - val_accuracy: 0.8985\n",
      "Epoch 916/1000\n",
      "4068/4068 [==============================] - 0s 111us/step - loss: 0.1926 - accuracy: 0.9184 - val_loss: 0.3704 - val_accuracy: 0.8830\n",
      "Epoch 917/1000\n",
      "4068/4068 [==============================] - 0s 86us/step - loss: 0.1995 - accuracy: 0.9154 - val_loss: 0.3789 - val_accuracy: 0.8896\n",
      "Epoch 918/1000\n",
      "4068/4068 [==============================] - 0s 91us/step - loss: 0.1953 - accuracy: 0.9191 - val_loss: 0.3877 - val_accuracy: 0.9095\n",
      "Epoch 919/1000\n",
      "4068/4068 [==============================] - 0s 84us/step - loss: 0.2132 - accuracy: 0.9162 - val_loss: 0.4356 - val_accuracy: 0.8852\n",
      "Epoch 920/1000\n",
      "4068/4068 [==============================] - 0s 111us/step - loss: 0.1993 - accuracy: 0.9181 - val_loss: 0.3742 - val_accuracy: 0.8852\n",
      "Epoch 921/1000\n",
      "4068/4068 [==============================] - 0s 98us/step - loss: 0.1942 - accuracy: 0.9152 - val_loss: 0.4016 - val_accuracy: 0.8609\n",
      "Epoch 922/1000\n",
      "4068/4068 [==============================] - 0s 112us/step - loss: 0.2039 - accuracy: 0.9164 - val_loss: 0.3998 - val_accuracy: 0.8896\n",
      "Epoch 923/1000\n",
      "4068/4068 [==============================] - 0s 97us/step - loss: 0.1939 - accuracy: 0.9179 - val_loss: 0.4483 - val_accuracy: 0.8366\n",
      "Epoch 924/1000\n",
      "4068/4068 [==============================] - 0s 102us/step - loss: 0.1954 - accuracy: 0.9164 - val_loss: 0.4269 - val_accuracy: 0.8609\n",
      "Epoch 925/1000\n",
      "4068/4068 [==============================] - 0s 109us/step - loss: 0.2081 - accuracy: 0.9142 - val_loss: 0.3775 - val_accuracy: 0.8985\n",
      "Epoch 926/1000\n",
      "4068/4068 [==============================] - 0s 92us/step - loss: 0.1965 - accuracy: 0.9172 - val_loss: 0.3996 - val_accuracy: 0.8852\n",
      "Epoch 927/1000\n",
      "4068/4068 [==============================] - 0s 87us/step - loss: 0.1959 - accuracy: 0.9194 - val_loss: 0.3841 - val_accuracy: 0.8940\n",
      "Epoch 928/1000\n",
      "4068/4068 [==============================] - 0s 85us/step - loss: 0.1932 - accuracy: 0.9159 - val_loss: 0.3953 - val_accuracy: 0.8940\n",
      "Epoch 929/1000\n",
      "4068/4068 [==============================] - 0s 87us/step - loss: 0.1953 - accuracy: 0.9196 - val_loss: 0.4086 - val_accuracy: 0.8786\n",
      "Epoch 930/1000\n",
      "4068/4068 [==============================] - 0s 84us/step - loss: 0.1916 - accuracy: 0.9211 - val_loss: 0.4033 - val_accuracy: 0.8675\n",
      "Epoch 931/1000\n",
      "4068/4068 [==============================] - 0s 86us/step - loss: 0.1961 - accuracy: 0.9199 - val_loss: 0.4028 - val_accuracy: 0.8742\n",
      "Epoch 932/1000\n",
      "4068/4068 [==============================] - 0s 82us/step - loss: 0.1954 - accuracy: 0.9211 - val_loss: 0.4268 - val_accuracy: 0.8985\n",
      "Epoch 933/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2000 - accuracy: 0.9162 - val_loss: 0.4376 - val_accuracy: 0.8985\n",
      "Epoch 934/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.1924 - accuracy: 0.9184 - val_loss: 0.4407 - val_accuracy: 0.8962\n",
      "Epoch 935/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2046 - accuracy: 0.9145 - val_loss: 0.3845 - val_accuracy: 0.8786\n",
      "Epoch 936/1000\n",
      "4068/4068 [==============================] - 0s 75us/step - loss: 0.1948 - accuracy: 0.9184 - val_loss: 0.3869 - val_accuracy: 0.8874\n",
      "Epoch 937/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.1955 - accuracy: 0.9223 - val_loss: 0.3863 - val_accuracy: 0.8962\n",
      "Epoch 938/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.1921 - accuracy: 0.9255 - val_loss: 0.3847 - val_accuracy: 0.8940\n",
      "Epoch 939/1000\n",
      "4068/4068 [==============================] - 0s 78us/step - loss: 0.1906 - accuracy: 0.9174 - val_loss: 0.4062 - val_accuracy: 0.8896\n",
      "Epoch 940/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.2260 - accuracy: 0.9088 - val_loss: 0.4851 - val_accuracy: 0.8256\n",
      "Epoch 941/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.1927 - accuracy: 0.9186 - val_loss: 0.4138 - val_accuracy: 0.8940\n",
      "Epoch 942/1000\n",
      "4068/4068 [==============================] - 0s 80us/step - loss: 0.1997 - accuracy: 0.9221 - val_loss: 0.4065 - val_accuracy: 0.9051\n",
      "Epoch 943/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.2067 - accuracy: 0.9154 - val_loss: 0.3977 - val_accuracy: 0.8830\n",
      "Epoch 944/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.1937 - accuracy: 0.9208 - val_loss: 0.4070 - val_accuracy: 0.8609\n",
      "Epoch 945/1000\n",
      "4068/4068 [==============================] - 0s 76us/step - loss: 0.1968 - accuracy: 0.9199 - val_loss: 0.3985 - val_accuracy: 0.8808\n",
      "Epoch 946/1000\n",
      "4068/4068 [==============================] - 0s 82us/step - loss: 0.1908 - accuracy: 0.9176 - val_loss: 0.3815 - val_accuracy: 0.8940\n",
      "Epoch 947/1000\n",
      "4068/4068 [==============================] - 0s 76us/step - loss: 0.1964 - accuracy: 0.9176 - val_loss: 0.4787 - val_accuracy: 0.8940\n",
      "Epoch 948/1000\n",
      "4068/4068 [==============================] - 0s 82us/step - loss: 0.1917 - accuracy: 0.9201 - val_loss: 0.3978 - val_accuracy: 0.8653\n",
      "Epoch 949/1000\n",
      "4068/4068 [==============================] - 0s 84us/step - loss: 0.1917 - accuracy: 0.9186 - val_loss: 0.4090 - val_accuracy: 0.8830\n",
      "Epoch 950/1000\n",
      "4068/4068 [==============================] - 0s 85us/step - loss: 0.2063 - accuracy: 0.9142 - val_loss: 0.3995 - val_accuracy: 0.9073\n",
      "Epoch 951/1000\n",
      "4068/4068 [==============================] - 0s 84us/step - loss: 0.2162 - accuracy: 0.9137 - val_loss: 0.4310 - val_accuracy: 0.8962\n",
      "Epoch 952/1000\n",
      "4068/4068 [==============================] - 0s 90us/step - loss: 0.1993 - accuracy: 0.9191 - val_loss: 0.3749 - val_accuracy: 0.8896\n",
      "Epoch 953/1000\n",
      "4068/4068 [==============================] - 0s 105us/step - loss: 0.2000 - accuracy: 0.9149 - val_loss: 0.5446 - val_accuracy: 0.8896\n",
      "Epoch 954/1000\n",
      "4068/4068 [==============================] - 0s 105us/step - loss: 0.2100 - accuracy: 0.9164 - val_loss: 0.4445 - val_accuracy: 0.9029\n",
      "Epoch 955/1000\n",
      "4068/4068 [==============================] - 0s 106us/step - loss: 0.1972 - accuracy: 0.9186 - val_loss: 0.3820 - val_accuracy: 0.8874\n",
      "Epoch 956/1000\n",
      "4068/4068 [==============================] - 0s 108us/step - loss: 0.1866 - accuracy: 0.9267 - val_loss: 0.4089 - val_accuracy: 0.8587\n",
      "Epoch 957/1000\n",
      "4068/4068 [==============================] - 0s 92us/step - loss: 0.1982 - accuracy: 0.9235 - val_loss: 0.4940 - val_accuracy: 0.8940\n",
      "Epoch 958/1000\n",
      "4068/4068 [==============================] - 0s 93us/step - loss: 0.2148 - accuracy: 0.9169 - val_loss: 0.3767 - val_accuracy: 0.8918\n",
      "Epoch 959/1000\n",
      "4068/4068 [==============================] - 0s 83us/step - loss: 0.1865 - accuracy: 0.9206 - val_loss: 0.3910 - val_accuracy: 0.8940\n",
      "Epoch 960/1000\n",
      "4068/4068 [==============================] - 0s 96us/step - loss: 0.1980 - accuracy: 0.9189 - val_loss: 0.4271 - val_accuracy: 0.8631\n",
      "Epoch 961/1000\n",
      "4068/4068 [==============================] - 0s 85us/step - loss: 0.1930 - accuracy: 0.9196 - val_loss: 0.3870 - val_accuracy: 0.9007\n",
      "Epoch 962/1000\n",
      "4068/4068 [==============================] - 0s 83us/step - loss: 0.1859 - accuracy: 0.9191 - val_loss: 0.4127 - val_accuracy: 0.8852\n",
      "Epoch 963/1000\n",
      "4068/4068 [==============================] - 0s 92us/step - loss: 0.2037 - accuracy: 0.9213 - val_loss: 0.4412 - val_accuracy: 0.9007\n",
      "Epoch 964/1000\n",
      "4068/4068 [==============================] - 0s 92us/step - loss: 0.1855 - accuracy: 0.9245 - val_loss: 0.4090 - val_accuracy: 0.9007\n",
      "Epoch 965/1000\n",
      "4068/4068 [==============================] - 0s 98us/step - loss: 0.1880 - accuracy: 0.9204 - val_loss: 0.4532 - val_accuracy: 0.8985\n",
      "Epoch 966/1000\n",
      "4068/4068 [==============================] - 0s 102us/step - loss: 0.2027 - accuracy: 0.9221 - val_loss: 0.4041 - val_accuracy: 0.8720\n",
      "Epoch 967/1000\n",
      "4068/4068 [==============================] - 0s 90us/step - loss: 0.1895 - accuracy: 0.9238 - val_loss: 0.4959 - val_accuracy: 0.8985\n",
      "Epoch 968/1000\n",
      "4068/4068 [==============================] - 0s 81us/step - loss: 0.2044 - accuracy: 0.9196 - val_loss: 0.4213 - val_accuracy: 0.8764\n",
      "Epoch 969/1000\n",
      "4068/4068 [==============================] - 0s 82us/step - loss: 0.2202 - accuracy: 0.9162 - val_loss: 0.4052 - val_accuracy: 0.8918\n",
      "Epoch 970/1000\n",
      "4068/4068 [==============================] - 0s 85us/step - loss: 0.1892 - accuracy: 0.9194 - val_loss: 0.3966 - val_accuracy: 0.8896\n",
      "Epoch 971/1000\n",
      "4068/4068 [==============================] - 0s 83us/step - loss: 0.1910 - accuracy: 0.9226 - val_loss: 0.4799 - val_accuracy: 0.9007\n",
      "Epoch 972/1000\n",
      "4068/4068 [==============================] - 0s 95us/step - loss: 0.1957 - accuracy: 0.9149 - val_loss: 0.4290 - val_accuracy: 0.8631\n",
      "Epoch 973/1000\n",
      "4068/4068 [==============================] - 0s 102us/step - loss: 0.2005 - accuracy: 0.9231 - val_loss: 0.4033 - val_accuracy: 0.8896\n",
      "Epoch 974/1000\n",
      "4068/4068 [==============================] - 0s 109us/step - loss: 0.2035 - accuracy: 0.9149 - val_loss: 0.4238 - val_accuracy: 0.8653\n",
      "Epoch 975/1000\n",
      "4068/4068 [==============================] - 0s 99us/step - loss: 0.1812 - accuracy: 0.9235 - val_loss: 0.4119 - val_accuracy: 0.8985\n",
      "Epoch 976/1000\n",
      "4068/4068 [==============================] - 0s 91us/step - loss: 0.1881 - accuracy: 0.9199 - val_loss: 0.5004 - val_accuracy: 0.8278\n",
      "Epoch 977/1000\n",
      "4068/4068 [==============================] - 0s 106us/step - loss: 0.1945 - accuracy: 0.9199 - val_loss: 0.4101 - val_accuracy: 0.8808\n",
      "Epoch 978/1000\n",
      "4068/4068 [==============================] - 0s 99us/step - loss: 0.1960 - accuracy: 0.9191 - val_loss: 0.4257 - val_accuracy: 0.8786\n",
      "Epoch 979/1000\n",
      "4068/4068 [==============================] - 0s 85us/step - loss: 0.1852 - accuracy: 0.9240 - val_loss: 0.4059 - val_accuracy: 0.8852\n",
      "Epoch 980/1000\n",
      "4068/4068 [==============================] - 0s 98us/step - loss: 0.1967 - accuracy: 0.9189 - val_loss: 0.4340 - val_accuracy: 0.8918\n",
      "Epoch 981/1000\n",
      "4068/4068 [==============================] - 0s 82us/step - loss: 0.1961 - accuracy: 0.9233 - val_loss: 0.4459 - val_accuracy: 0.8786\n",
      "Epoch 982/1000\n",
      "4068/4068 [==============================] - 0s 89us/step - loss: 0.1831 - accuracy: 0.9282 - val_loss: 0.4639 - val_accuracy: 0.8455\n",
      "Epoch 983/1000\n",
      "4068/4068 [==============================] - 0s 91us/step - loss: 0.1888 - accuracy: 0.9221 - val_loss: 0.4015 - val_accuracy: 0.8940\n",
      "Epoch 984/1000\n",
      "4068/4068 [==============================] - 0s 97us/step - loss: 0.1933 - accuracy: 0.9201 - val_loss: 0.5060 - val_accuracy: 0.8874\n",
      "Epoch 985/1000\n",
      "4068/4068 [==============================] - 0s 97us/step - loss: 0.1918 - accuracy: 0.9228 - val_loss: 0.4658 - val_accuracy: 0.9007\n",
      "Epoch 986/1000\n",
      "4068/4068 [==============================] - 0s 112us/step - loss: 0.1867 - accuracy: 0.9228 - val_loss: 0.4115 - val_accuracy: 0.8896\n",
      "Epoch 987/1000\n",
      "4068/4068 [==============================] - 0s 102us/step - loss: 0.2038 - accuracy: 0.9194 - val_loss: 0.4113 - val_accuracy: 0.8742\n",
      "Epoch 988/1000\n",
      "4068/4068 [==============================] - 0s 98us/step - loss: 0.1887 - accuracy: 0.9235 - val_loss: 0.4158 - val_accuracy: 0.8720\n",
      "Epoch 989/1000\n",
      "4068/4068 [==============================] - 0s 92us/step - loss: 0.1868 - accuracy: 0.9211 - val_loss: 0.4390 - val_accuracy: 0.8433\n",
      "Epoch 990/1000\n",
      "4068/4068 [==============================] - 0s 88us/step - loss: 0.1853 - accuracy: 0.9226 - val_loss: 0.4212 - val_accuracy: 0.8874\n",
      "Epoch 991/1000\n",
      "4068/4068 [==============================] - 0s 85us/step - loss: 0.1923 - accuracy: 0.9231 - val_loss: 0.4101 - val_accuracy: 0.8918\n",
      "Epoch 992/1000\n",
      "4068/4068 [==============================] - 0s 79us/step - loss: 0.1827 - accuracy: 0.9233 - val_loss: 0.4204 - val_accuracy: 0.9007\n",
      "Epoch 993/1000\n",
      "4068/4068 [==============================] - 0s 92us/step - loss: 0.1907 - accuracy: 0.9213 - val_loss: 0.4073 - val_accuracy: 0.8896\n",
      "Epoch 994/1000\n",
      "4068/4068 [==============================] - 0s 102us/step - loss: 0.1889 - accuracy: 0.9189 - val_loss: 0.4177 - val_accuracy: 0.8830\n",
      "Epoch 995/1000\n",
      "4068/4068 [==============================] - 0s 95us/step - loss: 0.1938 - accuracy: 0.9204 - val_loss: 0.4002 - val_accuracy: 0.8852\n",
      "Epoch 996/1000\n",
      "4068/4068 [==============================] - 0s 86us/step - loss: 0.1951 - accuracy: 0.9184 - val_loss: 0.4392 - val_accuracy: 0.8985\n",
      "Epoch 997/1000\n",
      "4068/4068 [==============================] - 0s 94us/step - loss: 0.1903 - accuracy: 0.9255 - val_loss: 0.4420 - val_accuracy: 0.8940\n",
      "Epoch 998/1000\n",
      "4068/4068 [==============================] - 0s 89us/step - loss: 0.1944 - accuracy: 0.9223 - val_loss: 0.3969 - val_accuracy: 0.8874\n",
      "Epoch 999/1000\n",
      "4068/4068 [==============================] - 0s 85us/step - loss: 0.1839 - accuracy: 0.9233 - val_loss: 0.4150 - val_accuracy: 0.8852\n",
      "Epoch 1000/1000\n",
      "4068/4068 [==============================] - 0s 99us/step - loss: 0.1923 - accuracy: 0.9235 - val_loss: 0.4407 - val_accuracy: 0.8587\n"
     ]
    }
   ],
   "source": [
    "acc_training = model.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=16, \n",
    "                         epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perhitungan Akurasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453/453 [==============================] - 0s 17us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4406925284573048, 0.8587196469306946]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_testing  = model.evaluate(x_test, y_test) \n",
    "acc_testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluasi Model dengan Melihat Plot\n",
    "\n",
    "Kasus : \n",
    "\n",
    "1. `Fit`\n",
    "    Fit apabila hasil akurasi dari data testing *tidak berbeda jauh* daripada data training. *Kasus yang inilah yang paling bagus*.\n",
    "\n",
    "2. `Underfitting`\n",
    "    Underfitting terjadi apabila hasil akurasi dari data testing itu *lebih rendah* daripada data training.\n",
    "\n",
    "3. `Overfitting`\n",
    "    Overfitting terjadi apabila hasil akurasi dari data testing itu *lebih tinggi* daripada data training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot hasil Akurasi antara Training dan Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(acc_training.history['accuracy'])\n",
    "plt.plot(acc_training.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('Akurasi')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Data training', 'Data Testing'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot hasil Loss antara Training dan Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(acc_training.history['loss'])\n",
    "plt.plot(acc_training.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Data training', 'Data Testing'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix\n",
    "    Confusion Matrix adalah sebuah tabel yang berisi 2 jenis nilai yaitu Nilai Dataset(nilai yang benar) dan  Nilai prediksi. Nilai tersebut dipecah menjadi 4 dan akan dibandingkan untuk mendapatkan informasi tentang performa model learning.\n",
    "    \n",
    "  Misal : `0` ( positive ) dan `1` ( negative ).\n",
    "    1. True Positive = Mesin memprediksi positive dan benar\n",
    "    2. True Negative = Mesin memprediksi negative dan benar\n",
    "    3. False Positive = Mesin memprediksi Positive dan negative \n",
    "    4. False Negative = Mesin memprediksi negative dan positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict_classes(x_test)\n",
    "cm = confusion_matrix(y_test, prediction)\n",
    "tp = cm[0,0]\n",
    "tn = cm[1,1]\n",
    "fp = cm[0,1]\n",
    "fn = cm[1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confussion matriks:\n",
      " [[409   2]\n",
      " [ 42   0]]\n"
     ]
    }
   ],
   "source": [
    "print (\"Confussion matriks:\\n\",cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluasi Model menggunakan Confusion Matrix\n",
    "\n",
    "Confusion matrik ada banyak penilaian yang bisa diambil untuk mengukur performa pada model learning. Namun yang paling sering dipakai ialah :\n",
    "\n",
    "1. `Presisi`\n",
    "    Perbandingan prediksi kelas yang `positif yang benar` dengan semua prediksi kelas `positif`.\n",
    "\n",
    "            Presisi = TP/(TP+FP)\n",
    "\n",
    "2. `Recall`\n",
    "    Seberapa akurat mesin untuk mengklasifikasi kelas yang `positif`.\n",
    "\n",
    "            Recall = TP/(TP+FN)\n",
    "\n",
    "3. `F1`\n",
    "    Nilai rata-rata dari Recall dan Presisi \n",
    "        \n",
    "            F1 = (2*Presisi*recall) / (presisi+recall)\n",
    "\n",
    "        \n",
    "4. `Akurasi`\n",
    "    Perbandingan Seberapa banyak model mengklasifikasi kelas yang benar dengan total kelas yang ada.\n",
    "\n",
    "            Akurasi = (TP+TN)/(TP+TN+FP+FN)\n",
    "\n",
    "5. `Sensitivity`\n",
    "            `Sensitivity` tp/(tp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = tp / ( tp + fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1=(2*precision*recall)/ (precision+recall)\n",
    "akurasi = (tp+tn)/(tp+tn+fp+fn)\n",
    "Sensitivity = tp/(tp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precesion :  0.9951338199513382\n",
      "recall :  0.9068736141906873\n",
      "F1 :  0.9489559164733178\n",
      "sensitivity :  0.9068736141906873\n",
      "akurasi :  0.9028697571743929\n"
     ]
    }
   ],
   "source": [
    "print (\"precesion : \",precision)\n",
    "print(\"recall : \",recall)\n",
    "print (\"F1 : \",f1)\n",
    "print(\"sensitivity : \",Sensitivity )\n",
    "print(\"akurasi : \",akurasi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
