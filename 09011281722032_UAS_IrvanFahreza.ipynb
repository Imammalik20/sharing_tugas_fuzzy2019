{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breast Cancer\n",
    "\n",
    "Di seluruh dunia, kanker payudara adalah jenis kanker yang paling umum pada wanita dan tertinggi kedua dalam hal tingkat kematian. Diagnosis kanker payudara dilakukan ketika benjolan abnormal ditemukan (dari pemeriksaan sendiri atau x-ray) atau setitik kecil dari kalsium terlihat (pada x-ray). Setelah benjolan yang mencurigakan ditemukan, dokter akan melakukan diagnosis untuk menentukan apakah itu adalah kanker dan, jika demikian, apakah telah menyebar ke bagian lain dari tubuh.\n",
    "\n",
    "Dataset kanker payudara ini diperoleh dari University of Wisconsin Hospitals, Madison dari Dr. William H. Wolberg."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset breast cancer ini terdiri dari 569 data yang menjelaskan tentang kanker payudara. Pada Dataset ini terdapat 6 variabel yang terdiri dari :\n",
    "1. mean_radius\n",
    "2. mean_texture\n",
    "3. mean_perimeter\n",
    "4. mean_area\n",
    "5. mean_smoothness\n",
    "6. diagnosis\n",
    "    - 0 = Tidak terdiagnosa Kanker Payudara\n",
    "    - 1 = terdiagnosa Kanker Payudara\n",
    "\n",
    "Dataset ini dapat diunduh melalui link : https://www.kaggle.com/merishnasuwal/breast-cancer-prediction-dataset/download/nHi1uj8ANJ3EVQrbUCcP%2Fversions%2FH2oqekzDXcgRFUS9pcpI%2Ffiles%2FBreast_cancer_data.csv?datasetVersionNumber=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Dataset\n",
    "\n",
    "Menampilkan 5 data pertama dan 5 data terakhir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_radius</th>\n",
       "      <th>mean_texture</th>\n",
       "      <th>mean_perimeter</th>\n",
       "      <th>mean_area</th>\n",
       "      <th>mean_smoothness</th>\n",
       "      <th>diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_radius  mean_texture  mean_perimeter  mean_area  mean_smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   diagnosis  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          0  \n",
       "4          0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(r'C:\\Users\\Breast_cancer_data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_radius</th>\n",
       "      <th>mean_texture</th>\n",
       "      <th>mean_perimeter</th>\n",
       "      <th>mean_area</th>\n",
       "      <th>mean_smoothness</th>\n",
       "      <th>diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_radius  mean_texture  mean_perimeter  mean_area  mean_smoothness  \\\n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     diagnosis  \n",
       "564          0  \n",
       "565          0  \n",
       "566          0  \n",
       "567          0  \n",
       "568          1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 6 columns):\n",
      "mean_radius        569 non-null float64\n",
      "mean_texture       569 non-null float64\n",
      "mean_perimeter     569 non-null float64\n",
      "mean_area          569 non-null float64\n",
      "mean_smoothness    569 non-null float64\n",
      "diagnosis          569 non-null int64\n",
      "dtypes: float64(5), int64(1)\n",
      "memory usage: 26.8 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset csv ini terdiri dari 569 baris dan 6 kolom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Processing\n",
    "\n",
    "Preprocessing data adalah tahap penting dalam pembelajaran mesin, karena data masukan yang baik dan tepat (harusnya) akan membuat estimator mampu menghasilkan keluaran yang baik pula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data kemudian dibagi menjadi dua jenis: data independen/prediktor (X) dan data dependen/target (y). Pembagian data ini menggunakan pandas.DataFrame.iloc seperti di bawah. Karena dataset adalah matriks M*N, kita menggunakan slicing dengan format [baris, kolom].\n",
    "\n",
    "Atribut values akan mengembalikan numpy array. Tanpa atribut ini, iloc akan mengembalikan obyek DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,0:5].values\n",
    "Y = data.iloc[:,5].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sebaran Fitur Diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x24b8e6dc198>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.countplot(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Membagi dataset ke dalam training set dan test set\n",
    "Model machine learning umumnya membutuhkan dua set data untuk \"belajar\" dan menghasilkan estimasi: training set dan test set. Biasanya training set ini memiliki proporsi lebih besar dibandingkan test set, misal 70%.\n",
    "\n",
    "Untuk dataset yang digunakan dalam tulisan ini:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untuk parameter:\n",
    "\n",
    "- test_size : proporsi test set, dalam hal ini 0.3.\n",
    "- train_size: proporsi train size. Jika tidak di set, maka akan menyesuaikan dengan test size (dalam kasus ini 0.7). Berlaku kebalikannya.\n",
    "- random_state : konstan ini akan membuat hasil splitting tetap sama antar runtime atau antar mesin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scalling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kita perlu menskalakan kolom-kolom yang dibutuhkan. Perbedaan skala dapat menyebabkan kendala dengan estimator. Ingat euclidean distance.\n",
    "\n",
    "(Pada contoh ini, hanya X yang diskalakan. Untuk kasus tertentu, variabel independen dan dependen harus diskala)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**StandardScaler** menghilangkan mean (terpusat pada 0) dan menskalakan ke variansi (deviasi standar = 1), dengan asumsi data terdistribusi normal (gauss) untuk semua fitur. Formulanya\n",
    "\n",
    "\\begin{split}z = \\frac{x − u} {s}\\end{split}\n",
    "\n",
    "dengan *u* adalah mean sampel dan *s* adalah deviasi standar (DS) sampel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(x_train)\n",
    "X_test = sc.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sebenarnya ada dua parameter boolean yang bisa diatur untuk *StandardScaler()*, yaitu *with_mean* dan *with_std*. Umumnya biarkan saja ke default, yaitu True.\n",
    "\n",
    "Terlihat pada potongan kode di atas, fitting untuk menghitung mean dan DS hanya dilakukan pada training set (lalu dilakukan transformasi *(fit_transform)*). Gunakan mean dan DS yang didapat tadi untuk test set (sehingga cukup *transform()* saja)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Network (ANN)\n",
    "\n",
    "- ANN adalah salah satu main tools yang digunakan dalam Machine Learning\n",
    "- Seperti yang ditunjukkan oleh bagian \"Neural\" dari namanya, mereka adalah sistem \"Brain-Inspired\" yang dimaksudkan untuk mereplikasi cara  manusia belajar.\n",
    "- Neural Network terdiri dari input dan output layer, serta (dalam kebanyakan kasus) hidden layer yang terdiri dari unit-unit yang mengubah input menjadi sesuatu yang dapat digunakan oleh output layer.\n",
    "- ANN adalah tools yang sangat baik untuk menemukan pola yang terlalu rumit atau banyak bagi seorang programmer untuk mengekstraksi dan mengajarkan mesin untuk mengenali sesuatu.\n",
    "- Neural Network juga disebut \"perceptrons\".\n",
    "- Neural Network telah menjadi bagian utama dari kecerdasan buatan dalam beberapa dekade terakhir. Ini disebabkan oleh kedatangan teknik yang disebut \"backpropagation,\" yang memungkinkan network untuk menyesuaikan hidden layer neuron mereka dalam situasi di mana hasilnya tidak sesuai dengan apa yang diharapkan creator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.activations import relu, sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers\n",
    "\n",
    "Layer menerima input weight dan mengubah nilai tersebut yang umumnya fungsi non-linear dan meneruskannya sebagai output ke layer selanjutnya. Layer terdiri dari input layer, hidden layer, dan output layer.\n",
    "\n",
    "### Epochs\n",
    "Epoch adalah ketika seluruh dataset sudah melalui proses training pada Neural Netwok sampai dikembalikan ke awal untuk sekali putaran, karena satu Epoch terlalu besar untuk dimasukkan (feeding) kedalam komputer maka dari itu kita perlu membaginya kedalam satuan kecil (batches).\n",
    "\n",
    "Seiring bertambahnya jumlah epoch, semakin banyak pula weight (bobot) yang berubah dalam Neural Network dan kurvanya melengkung dari kurva yang kurang sesuai hingga selaras dengan kurva yang overfitting.\n",
    "\n",
    "### Batch Size\n",
    "Batch Size adalah jumlah sampel data yang disebarkan ke Neural Network. Contoh: jika kita mempunyai 100 dataset dan batch size kita adalah 5 maka algoritma ini akan menggunakan 5 sempel data pertama dari 100 data yang kita miliki (ke1, ke2, ke3, ke4, dan ke5) lalu disebarkankan atau ditraining oleh Neural Network sampai selesai kemudian mengambil kembali 5 sampel data kedua dari 100 data (ke6, ke7, ke8, ke9, dan ke10), dan begitu seterusnya sampai 5 sampel data ke 20 (100⁄5=20).\n",
    "\n",
    "## Activation Function\n",
    "**Activation function** befungsi untuk menentukan apakah neuron tersebut harus “aktif” atau tidak berdasarkan dari weighted sum dari input. Secara umum terdapat 2 jenis activation function, Linear dan Non-Linear Activation function.\n",
    "\n",
    "**Relu** melakukan “treshold” dari 0 hingga infinity. ReLU juga dapat menutupi kelemahan yang dimiliki oleh Sigmoid.\n",
    "Karna ReLU pada intinya hanya membuat pembatas pada bilangan nol, artinya apabila x ≤ 0 maka x = 0 dan apabila x > 0 maka x = x\n",
    "Ada beberapa pro dan kontra ketika kita menggunakan ReLU:\n",
    "\n",
    "Kelebihan :\n",
    "- ReLU sangat mempercepat proses konvergensi yang dilakukan dengan stochastic gradient descent jika dibandingkan dengan sigmoid / tanh.\n",
    "- Jika kita bandingan dengan sigmoid/tanh yang memiliki operasi-operasi yang “expensive” (exponentials, etc.), ReLU bisa kita implementasikan hanya dengan membuat pembatas(threshold) pada bilangan nol.\n",
    "\n",
    "Kekurangan :\n",
    "- Sayangnya, unit ReLU bisa menjadi rapuh pada saat proses training dan bisa membuat unit tersebut “mati”. Sebagai contohnya, kita mungkin bisa menemukan bahwa 40% dari network kita “mati” (neuron yang tidak akan pernah aktif selama proses training) apabila learning rate yang kita inisialisasi terlalu tinggi. Namun apabila kita menginisialisasi learning rate kita secara tepat maka hal seperti ini jarang menjadi masalah.\n",
    "\n",
    "\n",
    "**Sigmoid** function mempunyai rentang antara 0 hingga 1 sedangkan rentang dari Tanh adalah -1 hingga 1.\n",
    "Sigmoid memiliki bentuk formula sebagai berikut :\n",
    "\\begin{split}S(x) = \\frac{1} { 1+e-x }\\end{split}\n",
    "Sigmoid akan menerima angka tunggal dan mengubah nilai x menjadi sebuah nilai yang memiliki range mulai dari 0 sampai 1. Belakangan ini Sigmoid tidak disukai dan jarang digunakan, sigmoid memiliki kekurangan berupa:\n",
    "- Sigmoid mematikan gradient, property yang paling tidak diinginkan dari Sigmoid adalah ketika activation dari neuron mengeluarkan nilai yang berada pada ekor 0 atau 1, dimana gradient di wilayah ini hampir nol. Karena itu, jika gradient memiliki nilai yang sangat kecil, Sigmoid akan “mematikan” gradient dan kita sangat tidak menginginkan hal ini terjadi saat melakukan backpropagation.\n",
    "- Output dari Sigmoid tidak zero-centered. Hal ini berimplikasi pada kedinamisan saat melakukan gradient descent, karna apabila data yang datang ke neuron selalu positif maka gradient pada weights selama backpropagation akan menjadi semua positif atau semua negatif. Hal seperti ini dapat mengganggu proses training, tetapi hal ini tidak separah seperti hal yang ada pada point pertama(mematikan gradient).\n",
    "\n",
    "## Loss Function\n",
    "**Loss Function** digunakan untuk mengukur seberapa bagus performa dari neural network kita dalam melakukan prediksi terhadap target.\n",
    "\n",
    "- *Loss = (Target - Prediction)*<sup>2</sup>\n",
    "\n",
    "Ada berbagai macam loss function, namun yang paling sering digunakan adalah Squared Error (L2 Loss) untuk regresi. Sedangkan untuk klasifikasi yang biasa digunakan adalah **Cross-Entropy**.\n",
    "\n",
    "**Cross-Entropy** mengukur kinerja model klasifikasi yang outputnya merupakan nilai probabilitas antara 0 dan 1. Kehilangan lintas-entropi meningkat karena probabilitas yang diprediksi menyimpang dari label aktual. Jadi memprediksi probabilitas 0,012 ketika label observasi aktual adalah 1 akan buruk dan menghasilkan nilai kerugian yang tinggi. Model yang sempurna akan kehilangan log 0.\n",
    "\n",
    "**Optimizers** digunakan untuk memperbarui bobot dan bias yaitu parameter internal model untuk mengurangi kesalahan.\n",
    "Teknik yang paling penting dan dasar bagaimana kita melatih dan mengoptimalkan model kita adalah menggunakan Gradient Descent.digunakan untuk memperbarui bobot dan bias yaitu parameter internal model untuk mengurangi kesalahan.\n",
    "Teknik yang paling penting dan dasar bagaimana kita melatih dan mengoptimalkan model kita adalah menggunakan Gradient Descent.\n",
    "\n",
    "**Adam** merupakan cara lain untuk menggunakan gradien masa sebelumnya untuk menghitung gradien saat ini. Adam juga memanfaatkan konsep momentum dengan menambahkan pecahan dari gradien sebelumnya ke yang sekarang. Pengoptimal ini telah menjadi sangat luas, dan secara praktis diterima untuk digunakan dalam test neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From E:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Train on 398 samples, validate on 171 samples\n",
      "Epoch 1/200\n",
      "398/398 [==============================] - 1s 3ms/step - loss: 0.6922 - acc: 0.6482 - val_loss: 0.6917 - val_acc: 0.6023\n",
      "Epoch 2/200\n",
      "398/398 [==============================] - 0s 113us/step - loss: 0.6904 - acc: 0.6382 - val_loss: 0.6903 - val_acc: 0.6023\n",
      "Epoch 3/200\n",
      "398/398 [==============================] - 0s 103us/step - loss: 0.6884 - acc: 0.6382 - val_loss: 0.6886 - val_acc: 0.6023\n",
      "Epoch 4/200\n",
      "398/398 [==============================] - ETA: 0s - loss: 0.6903 - acc: 0.562 - 0s 110us/step - loss: 0.6860 - acc: 0.6382 - val_loss: 0.6865 - val_acc: 0.6023\n",
      "Epoch 5/200\n",
      "398/398 [==============================] - 0s 108us/step - loss: 0.6831 - acc: 0.6382 - val_loss: 0.6833 - val_acc: 0.6023\n",
      "Epoch 6/200\n",
      "398/398 [==============================] - 0s 98us/step - loss: 0.6789 - acc: 0.6382 - val_loss: 0.6790 - val_acc: 0.6023\n",
      "Epoch 7/200\n",
      "398/398 [==============================] - 0s 100us/step - loss: 0.6732 - acc: 0.6382 - val_loss: 0.6731 - val_acc: 0.6023\n",
      "Epoch 8/200\n",
      "398/398 [==============================] - 0s 100us/step - loss: 0.6658 - acc: 0.6382 - val_loss: 0.6653 - val_acc: 0.6023\n",
      "Epoch 9/200\n",
      "398/398 [==============================] - 0s 100us/step - loss: 0.6563 - acc: 0.6382 - val_loss: 0.6553 - val_acc: 0.6023\n",
      "Epoch 10/200\n",
      "398/398 [==============================] - 0s 100us/step - loss: 0.6447 - acc: 0.6382 - val_loss: 0.6432 - val_acc: 0.6023\n",
      "Epoch 11/200\n",
      "398/398 [==============================] - 0s 100us/step - loss: 0.6310 - acc: 0.6382 - val_loss: 0.6292 - val_acc: 0.6023\n",
      "Epoch 12/200\n",
      "398/398 [==============================] - 0s 98us/step - loss: 0.6152 - acc: 0.6382 - val_loss: 0.6131 - val_acc: 0.6023\n",
      "Epoch 13/200\n",
      "398/398 [==============================] - 0s 100us/step - loss: 0.5975 - acc: 0.6382 - val_loss: 0.5956 - val_acc: 0.6023\n",
      "Epoch 14/200\n",
      "398/398 [==============================] - 0s 98us/step - loss: 0.5785 - acc: 0.6382 - val_loss: 0.5767 - val_acc: 0.6023\n",
      "Epoch 15/200\n",
      "398/398 [==============================] - 0s 100us/step - loss: 0.5586 - acc: 0.6382 - val_loss: 0.5576 - val_acc: 0.6023\n",
      "Epoch 16/200\n",
      "398/398 [==============================] - 0s 103us/step - loss: 0.5388 - acc: 0.6382 - val_loss: 0.5384 - val_acc: 0.6023\n",
      "Epoch 17/200\n",
      "398/398 [==============================] - 0s 100us/step - loss: 0.5191 - acc: 0.6382 - val_loss: 0.5200 - val_acc: 0.6023\n",
      "Epoch 18/200\n",
      "398/398 [==============================] - 0s 100us/step - loss: 0.5002 - acc: 0.6382 - val_loss: 0.5029 - val_acc: 0.6023\n",
      "Epoch 19/200\n",
      "398/398 [==============================] - 0s 103us/step - loss: 0.4835 - acc: 0.6382 - val_loss: 0.4868 - val_acc: 0.6023\n",
      "Epoch 20/200\n",
      "398/398 [==============================] - 0s 108us/step - loss: 0.4672 - acc: 0.6382 - val_loss: 0.4725 - val_acc: 0.6023\n",
      "Epoch 21/200\n",
      "398/398 [==============================] - 0s 100us/step - loss: 0.4535 - acc: 0.6382 - val_loss: 0.4595 - val_acc: 0.6023\n",
      "Epoch 22/200\n",
      "398/398 [==============================] - 0s 107us/step - loss: 0.4408 - acc: 0.6382 - val_loss: 0.4483 - val_acc: 0.6023\n",
      "Epoch 23/200\n",
      "398/398 [==============================] - 0s 108us/step - loss: 0.4299 - acc: 0.6382 - val_loss: 0.4383 - val_acc: 0.6023\n",
      "Epoch 24/200\n",
      "398/398 [==============================] - 0s 103us/step - loss: 0.4203 - acc: 0.6382 - val_loss: 0.4292 - val_acc: 0.6023\n",
      "Epoch 25/200\n",
      "398/398 [==============================] - 0s 105us/step - loss: 0.4118 - acc: 0.6382 - val_loss: 0.4211 - val_acc: 0.6023\n",
      "Epoch 26/200\n",
      "398/398 [==============================] - 0s 105us/step - loss: 0.4038 - acc: 0.6382 - val_loss: 0.4139 - val_acc: 0.6023\n",
      "Epoch 27/200\n",
      "398/398 [==============================] - 0s 103us/step - loss: 0.3972 - acc: 0.6382 - val_loss: 0.4072 - val_acc: 0.6023\n",
      "Epoch 28/200\n",
      "398/398 [==============================] - 0s 105us/step - loss: 0.3910 - acc: 0.6382 - val_loss: 0.4012 - val_acc: 0.6023\n",
      "Epoch 29/200\n",
      "398/398 [==============================] - 0s 105us/step - loss: 0.3856 - acc: 0.6382 - val_loss: 0.3959 - val_acc: 0.6023\n",
      "Epoch 30/200\n",
      "398/398 [==============================] - 0s 100us/step - loss: 0.3805 - acc: 0.6382 - val_loss: 0.3910 - val_acc: 0.6023\n",
      "Epoch 31/200\n",
      "398/398 [==============================] - 0s 103us/step - loss: 0.3760 - acc: 0.6382 - val_loss: 0.3865 - val_acc: 0.6023\n",
      "Epoch 32/200\n",
      "398/398 [==============================] - 0s 103us/step - loss: 0.3718 - acc: 0.7085 - val_loss: 0.3820 - val_acc: 0.8830\n",
      "Epoch 33/200\n",
      "398/398 [==============================] - 0s 98us/step - loss: 0.3677 - acc: 0.9121 - val_loss: 0.3780 - val_acc: 0.8947\n",
      "Epoch 34/200\n",
      "398/398 [==============================] - 0s 100us/step - loss: 0.3640 - acc: 0.9121 - val_loss: 0.3743 - val_acc: 0.9006\n",
      "Epoch 35/200\n",
      "398/398 [==============================] - 0s 98us/step - loss: 0.3605 - acc: 0.9171 - val_loss: 0.3708 - val_acc: 0.9123\n",
      "Epoch 36/200\n",
      "398/398 [==============================] - 0s 95us/step - loss: 0.3572 - acc: 0.9246 - val_loss: 0.3674 - val_acc: 0.9123\n",
      "Epoch 37/200\n",
      "398/398 [==============================] - 0s 100us/step - loss: 0.3540 - acc: 0.9246 - val_loss: 0.3642 - val_acc: 0.9123\n",
      "Epoch 38/200\n",
      "398/398 [==============================] - 0s 103us/step - loss: 0.3510 - acc: 0.9271 - val_loss: 0.3611 - val_acc: 0.9123\n",
      "Epoch 39/200\n",
      "398/398 [==============================] - 0s 108us/step - loss: 0.3482 - acc: 0.9271 - val_loss: 0.3581 - val_acc: 0.9181\n",
      "Epoch 40/200\n",
      "398/398 [==============================] - 0s 105us/step - loss: 0.3455 - acc: 0.9271 - val_loss: 0.3550 - val_acc: 0.9240\n",
      "Epoch 41/200\n",
      "398/398 [==============================] - 0s 98us/step - loss: 0.3428 - acc: 0.9271 - val_loss: 0.3523 - val_acc: 0.9240\n",
      "Epoch 42/200\n",
      "398/398 [==============================] - 0s 98us/step - loss: 0.3403 - acc: 0.9271 - val_loss: 0.3497 - val_acc: 0.9240\n",
      "Epoch 43/200\n",
      "398/398 [==============================] - 0s 105us/step - loss: 0.3380 - acc: 0.9271 - val_loss: 0.3471 - val_acc: 0.9298\n",
      "Epoch 44/200\n",
      "398/398 [==============================] - 0s 95us/step - loss: 0.3356 - acc: 0.9271 - val_loss: 0.3447 - val_acc: 0.9298\n",
      "Epoch 45/200\n",
      "398/398 [==============================] - 0s 100us/step - loss: 0.3333 - acc: 0.9271 - val_loss: 0.3422 - val_acc: 0.9298\n",
      "Epoch 46/200\n",
      "398/398 [==============================] - 0s 103us/step - loss: 0.3313 - acc: 0.9271 - val_loss: 0.3395 - val_acc: 0.9298\n",
      "Epoch 47/200\n",
      "398/398 [==============================] - 0s 100us/step - loss: 0.3290 - acc: 0.9246 - val_loss: 0.3373 - val_acc: 0.9298\n",
      "Epoch 48/200\n",
      "398/398 [==============================] - 0s 103us/step - loss: 0.3269 - acc: 0.9246 - val_loss: 0.3350 - val_acc: 0.9298\n",
      "Epoch 49/200\n",
      "398/398 [==============================] - 0s 103us/step - loss: 0.3248 - acc: 0.9246 - val_loss: 0.3331 - val_acc: 0.9298\n",
      "Epoch 50/200\n",
      "398/398 [==============================] - 0s 105us/step - loss: 0.3228 - acc: 0.9246 - val_loss: 0.3309 - val_acc: 0.9298\n",
      "Epoch 51/200\n",
      "398/398 [==============================] - 0s 107us/step - loss: 0.3209 - acc: 0.9246 - val_loss: 0.3289 - val_acc: 0.9298\n",
      "Epoch 52/200\n",
      "398/398 [==============================] - 0s 103us/step - loss: 0.3191 - acc: 0.9246 - val_loss: 0.3272 - val_acc: 0.9298\n",
      "Epoch 53/200\n",
      "398/398 [==============================] - 0s 113us/step - loss: 0.3172 - acc: 0.9246 - val_loss: 0.3251 - val_acc: 0.9357\n",
      "Epoch 54/200\n",
      "398/398 [==============================] - 0s 118us/step - loss: 0.3156 - acc: 0.9246 - val_loss: 0.3232 - val_acc: 0.9357\n",
      "Epoch 55/200\n",
      "398/398 [==============================] - 0s 113us/step - loss: 0.3137 - acc: 0.9246 - val_loss: 0.3211 - val_acc: 0.9357\n",
      "Epoch 56/200\n",
      "398/398 [==============================] - 0s 105us/step - loss: 0.3121 - acc: 0.9246 - val_loss: 0.3193 - val_acc: 0.9357\n",
      "Epoch 57/200\n",
      "398/398 [==============================] - 0s 120us/step - loss: 0.3103 - acc: 0.9246 - val_loss: 0.3175 - val_acc: 0.9357\n",
      "Epoch 58/200\n",
      "398/398 [==============================] - 0s 123us/step - loss: 0.3085 - acc: 0.9246 - val_loss: 0.3155 - val_acc: 0.9357\n",
      "Epoch 59/200\n",
      "398/398 [==============================] - 0s 118us/step - loss: 0.3069 - acc: 0.9271 - val_loss: 0.3138 - val_acc: 0.9357\n",
      "Epoch 60/200\n",
      "398/398 [==============================] - 0s 119us/step - loss: 0.3053 - acc: 0.9271 - val_loss: 0.3120 - val_acc: 0.9415\n",
      "Epoch 61/200\n",
      "398/398 [==============================] - 0s 103us/step - loss: 0.3038 - acc: 0.9296 - val_loss: 0.3101 - val_acc: 0.9415\n",
      "Epoch 62/200\n",
      "398/398 [==============================] - 0s 103us/step - loss: 0.3022 - acc: 0.9296 - val_loss: 0.3085 - val_acc: 0.9357\n",
      "Epoch 63/200\n",
      "398/398 [==============================] - 0s 108us/step - loss: 0.3007 - acc: 0.9296 - val_loss: 0.3070 - val_acc: 0.9357\n",
      "Epoch 64/200\n",
      "398/398 [==============================] - 0s 98us/step - loss: 0.2993 - acc: 0.9271 - val_loss: 0.3053 - val_acc: 0.9357\n",
      "Epoch 65/200\n",
      "398/398 [==============================] - 0s 98us/step - loss: 0.2979 - acc: 0.9271 - val_loss: 0.3038 - val_acc: 0.9357\n",
      "Epoch 66/200\n",
      "398/398 [==============================] - 0s 100us/step - loss: 0.2965 - acc: 0.9271 - val_loss: 0.3024 - val_acc: 0.9357\n",
      "Epoch 67/200\n",
      "398/398 [==============================] - 0s 103us/step - loss: 0.2951 - acc: 0.9271 - val_loss: 0.3008 - val_acc: 0.9357\n",
      "Epoch 68/200\n",
      "398/398 [==============================] - 0s 103us/step - loss: 0.2937 - acc: 0.9271 - val_loss: 0.2994 - val_acc: 0.9357\n",
      "Epoch 69/200\n",
      "398/398 [==============================] - 0s 100us/step - loss: 0.2925 - acc: 0.9271 - val_loss: 0.2980 - val_acc: 0.9357\n",
      "Epoch 70/200\n",
      "398/398 [==============================] - 0s 100us/step - loss: 0.2910 - acc: 0.9271 - val_loss: 0.2965 - val_acc: 0.9298\n",
      "Epoch 71/200\n",
      "398/398 [==============================] - 0s 108us/step - loss: 0.2899 - acc: 0.9296 - val_loss: 0.2951 - val_acc: 0.9298\n",
      "Epoch 72/200\n",
      "398/398 [==============================] - 0s 105us/step - loss: 0.2886 - acc: 0.9296 - val_loss: 0.2937 - val_acc: 0.9298\n",
      "Epoch 73/200\n",
      "398/398 [==============================] - 0s 105us/step - loss: 0.2874 - acc: 0.9296 - val_loss: 0.2924 - val_acc: 0.9298\n",
      "Epoch 74/200\n",
      "398/398 [==============================] - 0s 105us/step - loss: 0.2862 - acc: 0.9296 - val_loss: 0.2911 - val_acc: 0.9298\n",
      "Epoch 75/200\n",
      "398/398 [==============================] - 0s 110us/step - loss: 0.2849 - acc: 0.9296 - val_loss: 0.2898 - val_acc: 0.9298\n",
      "Epoch 76/200\n",
      "398/398 [==============================] - 0s 130us/step - loss: 0.2837 - acc: 0.9271 - val_loss: 0.2884 - val_acc: 0.9298\n",
      "Epoch 77/200\n",
      "398/398 [==============================] - 0s 138us/step - loss: 0.2826 - acc: 0.9296 - val_loss: 0.2874 - val_acc: 0.9298\n",
      "Epoch 78/200\n",
      "398/398 [==============================] - 0s 140us/step - loss: 0.2815 - acc: 0.9296 - val_loss: 0.2861 - val_acc: 0.9298\n",
      "Epoch 79/200\n",
      "398/398 [==============================] - 0s 110us/step - loss: 0.2804 - acc: 0.9296 - val_loss: 0.2851 - val_acc: 0.9298\n",
      "Epoch 80/200\n",
      "398/398 [==============================] - 0s 98us/step - loss: 0.2792 - acc: 0.9296 - val_loss: 0.2838 - val_acc: 0.9298\n",
      "Epoch 81/200\n",
      "398/398 [==============================] - 0s 103us/step - loss: 0.2781 - acc: 0.9322 - val_loss: 0.2826 - val_acc: 0.9298\n",
      "Epoch 82/200\n",
      "398/398 [==============================] - 0s 105us/step - loss: 0.2770 - acc: 0.9322 - val_loss: 0.2814 - val_acc: 0.9357\n",
      "Epoch 83/200\n",
      "398/398 [==============================] - 0s 104us/step - loss: 0.2759 - acc: 0.9296 - val_loss: 0.2801 - val_acc: 0.9357\n",
      "Epoch 84/200\n",
      "398/398 [==============================] - 0s 108us/step - loss: 0.2754 - acc: 0.9271 - val_loss: 0.2787 - val_acc: 0.9357\n",
      "Epoch 85/200\n",
      "398/398 [==============================] - 0s 110us/step - loss: 0.2738 - acc: 0.9271 - val_loss: 0.2778 - val_acc: 0.9357\n",
      "Epoch 86/200\n",
      "398/398 [==============================] - 0s 103us/step - loss: 0.2730 - acc: 0.9271 - val_loss: 0.2770 - val_acc: 0.9357\n",
      "Epoch 87/200\n",
      "398/398 [==============================] - 0s 103us/step - loss: 0.2719 - acc: 0.9271 - val_loss: 0.2760 - val_acc: 0.9357\n",
      "Epoch 88/200\n",
      "398/398 [==============================] - 0s 103us/step - loss: 0.2709 - acc: 0.9271 - val_loss: 0.2749 - val_acc: 0.9357\n",
      "Epoch 89/200\n",
      "398/398 [==============================] - 0s 103us/step - loss: 0.2699 - acc: 0.9271 - val_loss: 0.2738 - val_acc: 0.9357\n",
      "Epoch 90/200\n",
      "398/398 [==============================] - 0s 100us/step - loss: 0.2692 - acc: 0.9271 - val_loss: 0.2729 - val_acc: 0.9357\n",
      "Epoch 91/200\n",
      "398/398 [==============================] - 0s 115us/step - loss: 0.2682 - acc: 0.9271 - val_loss: 0.2717 - val_acc: 0.9357\n",
      "Epoch 92/200\n",
      "398/398 [==============================] - 0s 120us/step - loss: 0.2673 - acc: 0.9271 - val_loss: 0.2708 - val_acc: 0.9357\n",
      "Epoch 93/200\n",
      "398/398 [==============================] - 0s 123us/step - loss: 0.2663 - acc: 0.9271 - val_loss: 0.2698 - val_acc: 0.9357\n",
      "Epoch 94/200\n",
      "398/398 [==============================] - 0s 120us/step - loss: 0.2654 - acc: 0.9271 - val_loss: 0.2690 - val_acc: 0.9357\n",
      "Epoch 95/200\n",
      "398/398 [==============================] - 0s 98us/step - loss: 0.2645 - acc: 0.9271 - val_loss: 0.2683 - val_acc: 0.9357\n",
      "Epoch 96/200\n",
      "398/398 [==============================] - 0s 103us/step - loss: 0.2638 - acc: 0.9271 - val_loss: 0.2675 - val_acc: 0.9357\n",
      "Epoch 97/200\n",
      "398/398 [==============================] - 0s 105us/step - loss: 0.2631 - acc: 0.9271 - val_loss: 0.2665 - val_acc: 0.9357\n",
      "Epoch 98/200\n",
      "398/398 [==============================] - 0s 108us/step - loss: 0.2620 - acc: 0.9271 - val_loss: 0.2654 - val_acc: 0.9357\n",
      "Epoch 99/200\n",
      "398/398 [==============================] - 0s 100us/step - loss: 0.2612 - acc: 0.9271 - val_loss: 0.2644 - val_acc: 0.9357\n",
      "Epoch 100/200\n",
      "398/398 [==============================] - 0s 105us/step - loss: 0.2602 - acc: 0.9271 - val_loss: 0.2634 - val_acc: 0.9357\n",
      "Epoch 101/200\n",
      "398/398 [==============================] - 0s 103us/step - loss: 0.2595 - acc: 0.9271 - val_loss: 0.2626 - val_acc: 0.9357\n",
      "Epoch 102/200\n",
      "398/398 [==============================] - 0s 108us/step - loss: 0.2586 - acc: 0.9271 - val_loss: 0.2615 - val_acc: 0.9357\n",
      "Epoch 103/200\n",
      "398/398 [==============================] - 0s 103us/step - loss: 0.2578 - acc: 0.9271 - val_loss: 0.2606 - val_acc: 0.9357\n",
      "Epoch 104/200\n",
      "398/398 [==============================] - 0s 105us/step - loss: 0.2570 - acc: 0.9271 - val_loss: 0.2598 - val_acc: 0.9357\n",
      "Epoch 105/200\n",
      "398/398 [==============================] - 0s 110us/step - loss: 0.2563 - acc: 0.9271 - val_loss: 0.2589 - val_acc: 0.9298\n",
      "Epoch 106/200\n",
      "398/398 [==============================] - 0s 103us/step - loss: 0.2555 - acc: 0.9271 - val_loss: 0.2581 - val_acc: 0.9357\n",
      "Epoch 107/200\n",
      "398/398 [==============================] - 0s 100us/step - loss: 0.2546 - acc: 0.9271 - val_loss: 0.2572 - val_acc: 0.9357\n",
      "Epoch 108/200\n",
      "398/398 [==============================] - 0s 103us/step - loss: 0.2540 - acc: 0.9271 - val_loss: 0.2562 - val_acc: 0.9357\n",
      "Epoch 109/200\n",
      "398/398 [==============================] - 0s 98us/step - loss: 0.2531 - acc: 0.9271 - val_loss: 0.2555 - val_acc: 0.9357\n",
      "Epoch 110/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398/398 [==============================] - 0s 108us/step - loss: 0.2524 - acc: 0.9271 - val_loss: 0.2549 - val_acc: 0.9357\n",
      "Epoch 111/200\n",
      "398/398 [==============================] - 0s 103us/step - loss: 0.2517 - acc: 0.9271 - val_loss: 0.2541 - val_acc: 0.9357\n",
      "Epoch 112/200\n",
      "398/398 [==============================] - 0s 110us/step - loss: 0.2509 - acc: 0.9271 - val_loss: 0.2533 - val_acc: 0.9298\n",
      "Epoch 113/200\n",
      "398/398 [==============================] - 0s 120us/step - loss: 0.2502 - acc: 0.9271 - val_loss: 0.2526 - val_acc: 0.9298\n",
      "Epoch 114/200\n",
      "398/398 [==============================] - 0s 120us/step - loss: 0.2496 - acc: 0.9271 - val_loss: 0.2519 - val_acc: 0.9298\n",
      "Epoch 115/200\n",
      "398/398 [==============================] - 0s 118us/step - loss: 0.2488 - acc: 0.9271 - val_loss: 0.2510 - val_acc: 0.9298\n",
      "Epoch 116/200\n",
      "398/398 [==============================] - 0s 108us/step - loss: 0.2480 - acc: 0.9271 - val_loss: 0.2501 - val_acc: 0.9298\n",
      "Epoch 117/200\n",
      "398/398 [==============================] - 0s 129us/step - loss: 0.2473 - acc: 0.9271 - val_loss: 0.2494 - val_acc: 0.9298\n",
      "Epoch 118/200\n",
      "398/398 [==============================] - 0s 115us/step - loss: 0.2467 - acc: 0.9271 - val_loss: 0.2488 - val_acc: 0.9298\n",
      "Epoch 119/200\n",
      "398/398 [==============================] - 0s 115us/step - loss: 0.2462 - acc: 0.9271 - val_loss: 0.2482 - val_acc: 0.9298\n",
      "Epoch 120/200\n",
      "398/398 [==============================] - 0s 103us/step - loss: 0.2453 - acc: 0.9271 - val_loss: 0.2476 - val_acc: 0.9298\n",
      "Epoch 121/200\n",
      "398/398 [==============================] - 0s 113us/step - loss: 0.2447 - acc: 0.9271 - val_loss: 0.2470 - val_acc: 0.9298\n",
      "Epoch 122/200\n",
      "398/398 [==============================] - 0s 103us/step - loss: 0.2440 - acc: 0.9271 - val_loss: 0.2461 - val_acc: 0.9298\n",
      "Epoch 123/200\n",
      "398/398 [==============================] - 0s 98us/step - loss: 0.2434 - acc: 0.9271 - val_loss: 0.2454 - val_acc: 0.9298\n",
      "Epoch 124/200\n",
      "398/398 [==============================] - 0s 115us/step - loss: 0.2427 - acc: 0.9271 - val_loss: 0.2447 - val_acc: 0.9298\n",
      "Epoch 125/200\n",
      "398/398 [==============================] - 0s 123us/step - loss: 0.2421 - acc: 0.9271 - val_loss: 0.2440 - val_acc: 0.9298\n",
      "Epoch 126/200\n",
      "398/398 [==============================] - 0s 123us/step - loss: 0.2415 - acc: 0.9271 - val_loss: 0.2434 - val_acc: 0.9298\n",
      "Epoch 127/200\n",
      "398/398 [==============================] - 0s 120us/step - loss: 0.2408 - acc: 0.9271 - val_loss: 0.2428 - val_acc: 0.9298\n",
      "Epoch 128/200\n",
      "398/398 [==============================] - 0s 100us/step - loss: 0.2402 - acc: 0.9271 - val_loss: 0.2423 - val_acc: 0.9298\n",
      "Epoch 129/200\n",
      "398/398 [==============================] - 0s 108us/step - loss: 0.2397 - acc: 0.9271 - val_loss: 0.2417 - val_acc: 0.9298\n",
      "Epoch 130/200\n",
      "398/398 [==============================] - 0s 115us/step - loss: 0.2390 - acc: 0.9271 - val_loss: 0.2412 - val_acc: 0.9298\n",
      "Epoch 131/200\n",
      "398/398 [==============================] - 0s 108us/step - loss: 0.2385 - acc: 0.9271 - val_loss: 0.2405 - val_acc: 0.9298\n",
      "Epoch 132/200\n",
      "398/398 [==============================] - 0s 145us/step - loss: 0.2379 - acc: 0.9271 - val_loss: 0.2398 - val_acc: 0.9298\n",
      "Epoch 133/200\n",
      "398/398 [==============================] - 0s 140us/step - loss: 0.2374 - acc: 0.9271 - val_loss: 0.2392 - val_acc: 0.9298\n",
      "Epoch 134/200\n",
      "398/398 [==============================] - 0s 140us/step - loss: 0.2368 - acc: 0.9271 - val_loss: 0.2388 - val_acc: 0.9298\n",
      "Epoch 135/200\n",
      "398/398 [==============================] - 0s 138us/step - loss: 0.2363 - acc: 0.9271 - val_loss: 0.2382 - val_acc: 0.9298\n",
      "Epoch 136/200\n",
      "398/398 [==============================] - 0s 118us/step - loss: 0.2357 - acc: 0.9271 - val_loss: 0.2374 - val_acc: 0.9298\n",
      "Epoch 137/200\n",
      "398/398 [==============================] - 0s 113us/step - loss: 0.2352 - acc: 0.9271 - val_loss: 0.2367 - val_acc: 0.9298\n",
      "Epoch 138/200\n",
      "398/398 [==============================] - 0s 103us/step - loss: 0.2346 - acc: 0.9271 - val_loss: 0.2361 - val_acc: 0.9298\n",
      "Epoch 139/200\n",
      "398/398 [==============================] - 0s 95us/step - loss: 0.2340 - acc: 0.9271 - val_loss: 0.2356 - val_acc: 0.9298\n",
      "Epoch 140/200\n",
      "398/398 [==============================] - 0s 118us/step - loss: 0.2335 - acc: 0.9271 - val_loss: 0.2354 - val_acc: 0.9298\n",
      "Epoch 141/200\n",
      "398/398 [==============================] - 0s 170us/step - loss: 0.2329 - acc: 0.9271 - val_loss: 0.2348 - val_acc: 0.9298\n",
      "Epoch 142/200\n",
      "398/398 [==============================] - 0s 130us/step - loss: 0.2324 - acc: 0.9271 - val_loss: 0.2342 - val_acc: 0.9298\n",
      "Epoch 143/200\n",
      "398/398 [==============================] - 0s 138us/step - loss: 0.2319 - acc: 0.9271 - val_loss: 0.2337 - val_acc: 0.9298\n",
      "Epoch 144/200\n",
      "398/398 [==============================] - 0s 120us/step - loss: 0.2314 - acc: 0.9271 - val_loss: 0.2331 - val_acc: 0.9298\n",
      "Epoch 145/200\n",
      "398/398 [==============================] - 0s 100us/step - loss: 0.2310 - acc: 0.9271 - val_loss: 0.2328 - val_acc: 0.9298\n",
      "Epoch 146/200\n",
      "398/398 [==============================] - 0s 100us/step - loss: 0.2306 - acc: 0.9271 - val_loss: 0.2318 - val_acc: 0.9298\n",
      "Epoch 147/200\n",
      "398/398 [==============================] - 0s 110us/step - loss: 0.2299 - acc: 0.9271 - val_loss: 0.2314 - val_acc: 0.9298\n",
      "Epoch 148/200\n",
      "398/398 [==============================] - 0s 98us/step - loss: 0.2294 - acc: 0.9271 - val_loss: 0.2306 - val_acc: 0.9298\n",
      "Epoch 149/200\n",
      "398/398 [==============================] - 0s 105us/step - loss: 0.2289 - acc: 0.9271 - val_loss: 0.2303 - val_acc: 0.9298\n",
      "Epoch 150/200\n",
      "398/398 [==============================] - 0s 133us/step - loss: 0.2286 - acc: 0.9271 - val_loss: 0.2295 - val_acc: 0.9298\n",
      "Epoch 151/200\n",
      "398/398 [==============================] - 0s 120us/step - loss: 0.2279 - acc: 0.9271 - val_loss: 0.2293 - val_acc: 0.9298\n",
      "Epoch 152/200\n",
      "398/398 [==============================] - 0s 118us/step - loss: 0.2274 - acc: 0.9271 - val_loss: 0.2287 - val_acc: 0.9298\n",
      "Epoch 153/200\n",
      "398/398 [==============================] - 0s 119us/step - loss: 0.2271 - acc: 0.9271 - val_loss: 0.2284 - val_acc: 0.9298\n",
      "Epoch 154/200\n",
      "398/398 [==============================] - 0s 103us/step - loss: 0.2266 - acc: 0.9271 - val_loss: 0.2279 - val_acc: 0.9298\n",
      "Epoch 155/200\n",
      "398/398 [==============================] - 0s 104us/step - loss: 0.2262 - acc: 0.9271 - val_loss: 0.2272 - val_acc: 0.9298\n",
      "Epoch 156/200\n",
      "398/398 [==============================] - 0s 105us/step - loss: 0.2255 - acc: 0.9271 - val_loss: 0.2267 - val_acc: 0.9298\n",
      "Epoch 157/200\n",
      "398/398 [==============================] - 0s 110us/step - loss: 0.2251 - acc: 0.9271 - val_loss: 0.2262 - val_acc: 0.9298\n",
      "Epoch 158/200\n",
      "398/398 [==============================] - 0s 98us/step - loss: 0.2248 - acc: 0.9296 - val_loss: 0.2256 - val_acc: 0.9298\n",
      "Epoch 159/200\n",
      "398/398 [==============================] - 0s 103us/step - loss: 0.2242 - acc: 0.9296 - val_loss: 0.2251 - val_acc: 0.9298\n",
      "Epoch 160/200\n",
      "398/398 [==============================] - 0s 118us/step - loss: 0.2239 - acc: 0.9296 - val_loss: 0.2249 - val_acc: 0.9298\n",
      "Epoch 161/200\n",
      "398/398 [==============================] - 0s 115us/step - loss: 0.2234 - acc: 0.9271 - val_loss: 0.2246 - val_acc: 0.9298\n",
      "Epoch 162/200\n",
      "398/398 [==============================] - 0s 120us/step - loss: 0.2230 - acc: 0.9271 - val_loss: 0.2241 - val_acc: 0.9298\n",
      "Epoch 163/200\n",
      "398/398 [==============================] - 0s 120us/step - loss: 0.2225 - acc: 0.9271 - val_loss: 0.2238 - val_acc: 0.9298\n",
      "Epoch 164/200\n",
      "398/398 [==============================] - 0s 113us/step - loss: 0.2222 - acc: 0.9271 - val_loss: 0.2232 - val_acc: 0.9298\n",
      "Epoch 165/200\n",
      "398/398 [==============================] - 0s 118us/step - loss: 0.2219 - acc: 0.9271 - val_loss: 0.2231 - val_acc: 0.9298\n",
      "Epoch 166/200\n",
      "398/398 [==============================] - 0s 125us/step - loss: 0.2213 - acc: 0.9271 - val_loss: 0.2222 - val_acc: 0.9298\n",
      "Epoch 167/200\n",
      "398/398 [==============================] - 0s 118us/step - loss: 0.2208 - acc: 0.9296 - val_loss: 0.2217 - val_acc: 0.9298\n",
      "Epoch 168/200\n",
      "398/398 [==============================] - 0s 110us/step - loss: 0.2205 - acc: 0.9296 - val_loss: 0.2213 - val_acc: 0.9298\n",
      "Epoch 169/200\n",
      "398/398 [==============================] - 0s 103us/step - loss: 0.2201 - acc: 0.9296 - val_loss: 0.2206 - val_acc: 0.9298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/200\n",
      "398/398 [==============================] - 0s 108us/step - loss: 0.2197 - acc: 0.9296 - val_loss: 0.2203 - val_acc: 0.9298\n",
      "Epoch 171/200\n",
      "398/398 [==============================] - 0s 105us/step - loss: 0.2193 - acc: 0.9296 - val_loss: 0.2199 - val_acc: 0.9298\n",
      "Epoch 172/200\n",
      "398/398 [==============================] - 0s 103us/step - loss: 0.2190 - acc: 0.9296 - val_loss: 0.2194 - val_acc: 0.9298\n",
      "Epoch 173/200\n",
      "398/398 [==============================] - 0s 98us/step - loss: 0.2185 - acc: 0.9296 - val_loss: 0.2191 - val_acc: 0.9298\n",
      "Epoch 174/200\n",
      "398/398 [==============================] - 0s 98us/step - loss: 0.2182 - acc: 0.9296 - val_loss: 0.2189 - val_acc: 0.9298\n",
      "Epoch 175/200\n",
      "398/398 [==============================] - 0s 103us/step - loss: 0.2178 - acc: 0.9296 - val_loss: 0.2185 - val_acc: 0.9298\n",
      "Epoch 176/200\n",
      "398/398 [==============================] - 0s 103us/step - loss: 0.2174 - acc: 0.9296 - val_loss: 0.2181 - val_acc: 0.9298\n",
      "Epoch 177/200\n",
      "398/398 [==============================] - 0s 98us/step - loss: 0.2171 - acc: 0.9296 - val_loss: 0.2178 - val_acc: 0.9298\n",
      "Epoch 178/200\n",
      "398/398 [==============================] - 0s 103us/step - loss: 0.2167 - acc: 0.9271 - val_loss: 0.2176 - val_acc: 0.9298\n",
      "Epoch 179/200\n",
      "398/398 [==============================] - 0s 100us/step - loss: 0.2164 - acc: 0.9271 - val_loss: 0.2170 - val_acc: 0.9298\n",
      "Epoch 180/200\n",
      "398/398 [==============================] - 0s 100us/step - loss: 0.2160 - acc: 0.9296 - val_loss: 0.2165 - val_acc: 0.9298\n",
      "Epoch 181/200\n",
      "398/398 [==============================] - 0s 100us/step - loss: 0.2157 - acc: 0.9296 - val_loss: 0.2165 - val_acc: 0.9298\n",
      "Epoch 182/200\n",
      "398/398 [==============================] - 0s 103us/step - loss: 0.2152 - acc: 0.9271 - val_loss: 0.2159 - val_acc: 0.9298\n",
      "Epoch 183/200\n",
      "398/398 [==============================] - 0s 99us/step - loss: 0.2149 - acc: 0.9271 - val_loss: 0.2156 - val_acc: 0.9298\n",
      "Epoch 184/200\n",
      "398/398 [==============================] - 0s 95us/step - loss: 0.2147 - acc: 0.9271 - val_loss: 0.2157 - val_acc: 0.9298\n",
      "Epoch 185/200\n",
      "398/398 [==============================] - 0s 106us/step - loss: 0.2141 - acc: 0.9271 - val_loss: 0.2154 - val_acc: 0.9298\n",
      "Epoch 186/200\n",
      "398/398 [==============================] - 0s 100us/step - loss: 0.2138 - acc: 0.9271 - val_loss: 0.2149 - val_acc: 0.9298\n",
      "Epoch 187/200\n",
      "398/398 [==============================] - 0s 102us/step - loss: 0.2135 - acc: 0.9296 - val_loss: 0.2146 - val_acc: 0.9298\n",
      "Epoch 188/200\n",
      "398/398 [==============================] - 0s 100us/step - loss: 0.2131 - acc: 0.9296 - val_loss: 0.2143 - val_acc: 0.9298\n",
      "Epoch 189/200\n",
      "398/398 [==============================] - 0s 98us/step - loss: 0.2129 - acc: 0.9271 - val_loss: 0.2144 - val_acc: 0.9298\n",
      "Epoch 190/200\n",
      "398/398 [==============================] - 0s 105us/step - loss: 0.2127 - acc: 0.9271 - val_loss: 0.2136 - val_acc: 0.9298\n",
      "Epoch 191/200\n",
      "398/398 [==============================] - 0s 103us/step - loss: 0.2121 - acc: 0.9296 - val_loss: 0.2133 - val_acc: 0.9298\n",
      "Epoch 192/200\n",
      "398/398 [==============================] - 0s 108us/step - loss: 0.2119 - acc: 0.9296 - val_loss: 0.2128 - val_acc: 0.9298\n",
      "Epoch 193/200\n",
      "398/398 [==============================] - 0s 105us/step - loss: 0.2115 - acc: 0.9296 - val_loss: 0.2128 - val_acc: 0.9298\n",
      "Epoch 194/200\n",
      "398/398 [==============================] - 0s 108us/step - loss: 0.2111 - acc: 0.9296 - val_loss: 0.2124 - val_acc: 0.9298\n",
      "Epoch 195/200\n",
      "398/398 [==============================] - 0s 108us/step - loss: 0.2109 - acc: 0.9296 - val_loss: 0.2121 - val_acc: 0.9298\n",
      "Epoch 196/200\n",
      "398/398 [==============================] - 0s 120us/step - loss: 0.2109 - acc: 0.9296 - val_loss: 0.2115 - val_acc: 0.9298\n",
      "Epoch 197/200\n",
      "398/398 [==============================] - 0s 103us/step - loss: 0.2102 - acc: 0.9296 - val_loss: 0.2112 - val_acc: 0.9298\n",
      "Epoch 198/200\n",
      "398/398 [==============================] - 0s 100us/step - loss: 0.2100 - acc: 0.9296 - val_loss: 0.2111 - val_acc: 0.9298\n",
      "Epoch 199/200\n",
      "398/398 [==============================] - 0s 100us/step - loss: 0.2098 - acc: 0.9271 - val_loss: 0.2110 - val_acc: 0.9298\n",
      "Epoch 200/200\n",
      "398/398 [==============================] - 0s 103us/step - loss: 0.2095 - acc: 0.9271 - val_loss: 0.2107 - val_acc: 0.9298\n",
      "171/171 [==============================] - 0s 41us/step\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Hidden layer dan Input Layer\n",
    "model.add(Dense(3, kernel_initializer=\"uniform\", activation = 'relu', input_dim = 5))\n",
    "model.add(Dense(3, kernel_initializer=\"uniform\", activation = 'relu'))\n",
    "\n",
    "# Output Layer\n",
    "model.add(Dense(1, kernel_initializer=\"uniform\", activation = 'sigmoid'))\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "acc_training = model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=32, epochs=200)\n",
    "acc_testing  = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hasil Akurasi Prediksi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi data Testing = 92.98%\n"
     ]
    }
   ],
   "source": [
    "print(\"Akurasi data Testing = %.2f%%\" % (acc_testing[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Model Akurasi dan Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(acc_training.history['acc'])\n",
    "plt.plot(acc_training.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(acc_training.history['loss'])\n",
    "plt.plot(acc_training.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluasi Model\n",
    "\n",
    "**Confusion Matrix** adalah teknik untuk meringkas perfomance dari algoritma klasifikasinya.\n",
    "\n",
    "Klasifikasi akurasi bisa saja *misleading* jika memiliki jumlah pengamatan yang tidak sama di setiap kelas atau jika  memiliki lebih dari dua kelas dalam dataset Anda.\n",
    "\n",
    "Menghitung **Confusion Matrix** dapat memberi ide yang lebih baik tentang apakah model klasifikasinya benar dan jenis kesalahan apa yang dibuatnya.\n",
    "\n",
    "- True Positive (TP), yaitu jumlah dari kelas 1 yang benar dan diklasifikasikan sebagai kelas 1.\n",
    "- True Negative (TN), yaitu jumlah dari kelas 0 yang benar diklasifikasikan sebagai kelas 0.\n",
    "- False Positive (FP), yaitu jumlah dari kelas 0 yang salah diklasifikasikan sebagai kelas 1.\n",
    "- False Negative (FN), yaitu jumlah dari kelas 1 yang salah diklasifikasikan sebagai kelas 0.\n",
    "\n",
    "\n",
    "1. Akurasi adalah rasio prediksi Benar (positif dan negatif) dengan keseluruhan data.\n",
    "\n",
    "\\begin{split}Akurasi = \\frac{TP+TN} {TP+FP+FN+TN}\\end{split}\n",
    "\n",
    "2. Presisi adalah rasio prediksi benar positif dibandingkan dengan keseluruhan hasil yang diprediksi positif.\n",
    "\\begin{split}Presisi = \\frac{TP} {TP+FP}\\end{split}\n",
    "\n",
    "3. Recall (Sensitifitas) adalah rasio prediksi benar positif dibandingkan dengan keseluruhan data yang benar positif.\n",
    "\\begin{split}Recall = \\frac{TP} {TP+FN}\\end{split}\n",
    "\n",
    "4. Specificity adalah kebenaran memprediksi negatif dibandingkan dengan keseluruhan data negatif.\n",
    "\\begin{split}Specificity = \\frac{TN} {TN+FP}\\end{split}\n",
    "5. F1 Score adalah perbandingan rata-rata presisi dan recall yang dibobotkan.\n",
    "\\begin{split}F1 Score = \\frac {2 \\times Recall \\times Presisi} {Recall + Presisi}\\end{split}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediksi\n",
    "y_pred_test = model.predict(X_test)\n",
    "y_pred_test = (y_pred_test > 0.5)\n",
    "\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_train= (y_pred_train > 0.5)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "cm_train = confusion_matrix(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[61,  7],\n",
       "       [ 5, 98]], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy test 0.9298245614035088\n",
      "Specificity test 0.9514563106796117\n",
      "Sensitivity test 0.8970588235294118\n",
      "Precision test 0.9242424242424242\n",
      "F1 Score test 0.9104477611940298\n"
     ]
    }
   ],
   "source": [
    "tp = cm_test[0][0]\n",
    "fn = cm_test[0][1]\n",
    "fp = cm_test[1][0]\n",
    "tn = cm_test[1][1]\n",
    "\n",
    "accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
    "specificity = tn/(tn+fp)\n",
    "sensitivity = tp/(tp+fn)\n",
    "precision = tp/(tp+fp)\n",
    "f1_score = (2*sensitivity*precision)/(sensitivity+precision)\n",
    "\n",
    "print(\"Accuracy test {0}\".format(accuracy))\n",
    "print(\"Specificity test {0}\".format(specificity))\n",
    "print(\"Sensitivity test {0}\".format(sensitivity))\n",
    "print(\"Precision test {0}\".format(precision))\n",
    "print(\"F1 Score test {0}\".format(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[127,  17],\n",
       "       [ 11, 243]], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy training 0.9296482412060302\n",
      "Specificity training 0.9566929133858267\n",
      "Sensitivity training 0.8819444444444444\n",
      "Precision training 0.9202898550724637\n",
      "F1 Score training 0.900709219858156\n"
     ]
    }
   ],
   "source": [
    "tp = cm_train[0][0]\n",
    "fn = cm_train[0][1]\n",
    "fp = cm_train[1][0]\n",
    "tn = cm_train[1][1]\n",
    "\n",
    "accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
    "specificity = tn/(tn+fp)\n",
    "sensitivity = tp/(tp+fn)\n",
    "precision = tp/(tp+fp)\n",
    "f1_score = (2*sensitivity*precision)/(sensitivity+precision)\n",
    "\n",
    "print(\"Accuracy training {0}\".format(accuracy))\n",
    "print(\"Specificity training {0}\".format(specificity))\n",
    "print(\"Sensitivity training {0}\".format(sensitivity))\n",
    "print(\"Precision training {0}\".format(precision))\n",
    "print(\"F1 Score training {0}\".format(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
