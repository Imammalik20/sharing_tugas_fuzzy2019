{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural network\n",
    "### Nama : Annisa Karima R. Harahap\n",
    "### NIM    : 09011181722029\n",
    "### Kelas : SK5A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dalam kesempeatan kali ini, saya telah menerapkan artificial intelegencia untuk menentukan hasil akurasi dari dataset yang disediakan. Saya menggunakan dataset yang bersumber dari : https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection. Di dalam data tersebut terdapat dua atribut yaitu v1 dan v2. Atribut v1 adalah kelas atau label dan v2 adalah kumpulan pesan bahasa inggris sebanyak 5571 pesan. Kemudian akan ditentukan mana yang merupakan pesan yang **sah/legitimate (HAM)** dan pesan yang merupakan **SPAM**\n",
    "- HAM = Berada di kelas 0 yang berarti bukan spam \n",
    "- SPAM = Berada di kelas 1 yang berarti memang SPAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach\n",
    "- Import Library\n",
    "- Read Dataset\n",
    "- Text Processing (Pembersihan data)\n",
    "- Feature Extraction\n",
    "- Artificial Neural Network\n",
    "- Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.activations import relu\n",
    "from keras.activations import selu\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,precision_score,recall_score,f1_score,matthews_corrcoef,classification_report,roc_curve\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Read Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset terdiri dari dua atribut yaitu v1 dan v2. Atribut v1 berisikan label yang yang terdiri dari **HAM** dan **SPAM** dan atribut v2 berisi kumpulan pesan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5567</td>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5568</td>\n",
       "      <td>ham</td>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5569</td>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5570</td>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5571</td>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        v1                                                 v2 Unnamed: 2  \\\n",
       "0      ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1      ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3      ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "...    ...                                                ...        ...   \n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...        NaN   \n",
       "5568   ham              Will Ì_ b going to esplanade fr home?        NaN   \n",
       "5569   ham  Pity, * was in mood for that. So...any other s...        NaN   \n",
       "5570   ham  The guy did some bitching but I acted like i'd...        NaN   \n",
       "5571   ham                         Rofl. Its true to its name        NaN   \n",
       "\n",
       "     Unnamed: 3 Unnamed: 4  \n",
       "0           NaN        NaN  \n",
       "1           NaN        NaN  \n",
       "2           NaN        NaN  \n",
       "3           NaN        NaN  \n",
       "4           NaN        NaN  \n",
       "...         ...        ...  \n",
       "5567        NaN        NaN  \n",
       "5568        NaN        NaN  \n",
       "5569        NaN        NaN  \n",
       "5570        NaN        NaN  \n",
       "5571        NaN        NaN  \n",
       "\n",
       "[5572 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('D:/Data/spam.csv', encoding='ISO-8859-1')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note :\n",
    "- pd.read_csv('lokasi data dan nama data') : Digunakan untuk membaca data yang akan digunakan\n",
    "- encoding='ISO-8859-1' : ISO 8859-1 digunakan untuk membaca unicode ASCII yang terdapat di dalam message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5567</td>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5568</td>\n",
       "      <td>ham</td>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5569</td>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5570</td>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5571</td>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                            message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham              Will Ì_ b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(df.columns[[2,3,4,]], axis = 1, inplace= True)\n",
    "df.columns=['label','message']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terdapat 5 kolom dan 5572 baris dalam dataset tersebut. Tiga kolom terakhir seluruhnya berisi NaN sehingga kita harus menghilangkannya karena tidak terdapat data yang penting. Kemudian saya mengganti nama atribut v1 menjadi label dan v2 menjadi message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note :\n",
    "- df.drop = digunakan untuk menghapus baris dan kolom yang tidak diinginkan\n",
    "- df.colums = digunakan untuk mennganti nama atribut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note :\n",
    "- df.shape = digunakan untuk menampilkan bentuk dari dataset yaitu jumlah dari kolom dan baris yang ada di dalam dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().any().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note :\n",
    "- df.isnull().any().sum() = Digunakan untuk menampilkan apakah dataset tersebut berisi NaN atau tidak. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "category_count=df.groupby('label').count() #jumlah Data yang dihitung adalah label\n",
    "plt.bar(category_count.index.values, category_count['message']) # data yang ada dilabel dihubungkan dengan data yang ada di message\n",
    "plt.xlabel('kelas')\n",
    "plt.ylabel('number') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dilihat dari histogram HAM lebih banyak daripada SPAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5567</td>\n",
       "      <td>1</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5568</td>\n",
       "      <td>0</td>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5569</td>\n",
       "      <td>0</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5570</td>\n",
       "      <td>0</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5571</td>\n",
       "      <td>0</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                            message\n",
       "0         0  Go until jurong point, crazy.. Available only ...\n",
       "1         0                      Ok lar... Joking wif u oni...\n",
       "2         1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3         0  U dun say so early hor... U c already then say...\n",
       "4         0  Nah I don't think he goes to usf, he lives aro...\n",
       "...     ...                                                ...\n",
       "5567      1  This is the 2nd time we have tried 2 contact u...\n",
       "5568      0              Will Ì_ b going to esplanade fr home?\n",
       "5569      0  Pity, * was in mood for that. So...any other s...\n",
       "5570      0  The guy did some bitching but I acted like i'd...\n",
       "5571      0                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Memberi label ham menjadi 0 dan spam menjadi 1\n",
    "df = df.replace(['ham','spam'],[0, 1])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di atribut label terdapat dua data yaitu HAM dan SPAM yang dijadikan sebagai label atau kelas. Untuk itu, saya menggantinya dengan 0 dan 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note :\n",
    "- df = df.replace(['\"data 1\"','\"data 2\"'],[\"label 1\", \"label 2\"])\n",
    "- df.replace = digunakan untuk memberi label pada data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4825\n",
       "1     747\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note :\n",
    "- df.label.value_counts() = digunakan untuk menghitung jumlah data dari masing-masing label\n",
    "- label merupakan tipe data integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "Ok lar... Joking wif u oni...\n",
      "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
      "U dun say so early hor... U c already then say...\n",
      "Nah I don't think he goes to usf, he lives around here though\n"
     ]
    }
   ],
   "source": [
    "print (df['message'][0])\n",
    "print (df['message'][1])\n",
    "print (df['message'][2])\n",
    "print (df['message'][3])\n",
    "print (df['message'][4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ini menampilkan keseluruhan isi pesan dari atribut message. Pesan yang ditampilkan merupakan 5 pesan teratas dari atribut message**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text preprocessing atau text mining adalah tehnik untuk mengolah data teks agar lebih tersruktur dan rapi. Text mining merupakan kegiatan membersihkan data yang berbentuk teks agar data tersebut hanya tinggal menyisahkan bagian yang dapat mewakili saja. Secara umum, text mining dibagi menjadi 4 bagian yaitu, case folding, Tokenize, stopword removal dan stemming. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Case Folding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case folding adalah kegiatan mengubah semua huruf kapital yang ada di dalam data menjadi huruf kecil. Penggunaan case folding juga dapat digunakan untuk menghilangkan punctuation(tanda baca). Dalam proses ini, saya akan menghilangkan tanda baca yang ada di dalam data atribut message. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(message):\n",
    "    new_message=''.join([char for char in message if char not in string.punctuation])\n",
    "    return new_message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kegiatan di atas merupakan kegiatan menghapus punctuation yang ada di atribut message dan membuat atribut baru dari hasil kegiatan tersebut. \n",
    "\n",
    "Note :\n",
    "**.** join = Digunakan untuk menggabungkan character menjadi sebuah yang diberi nama ('  '). Character yang digabungkan adalah karakter yang bukan sebuah **string.punctuation**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['new_message']=df['message'].apply(lambda row : remove_punctuation(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pengaplikasian fungsi lambda untuk membuat baris baru yang diberi nama new_message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amore wat'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.message[0])\n",
    "df.new_message[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pesan yang pertama adalah pesan yang ada atribut message dan pesan yang kedua adalah pesan yang ada di atribut new_message. Jika diamati, semua punctuation seperti tanda (.) dan (,) di atribut message telah hilang seperti yang ditampilkan di pesan kedua.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data yang berbentuk teka biasanya merupakan kumpulan beberapa kalimat. Kalimat tersebut dapat dipotong menjadi token-token dengan menggunakan teknik tokenize. Dalam proses ini saya akan membuat isi data di atribut new_message menjadi token-token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(message):\n",
    "    tokens=re.split('\\W+',message)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kegiatan di atas merupakan kegiatan memisahkan kalimat menjadi per kata yang ada di atribut new_message dan membuat atribut baru dari hasil kegiatan tersebut.\n",
    "\n",
    "Note : \n",
    "- re.split = mengimport fungsi split pada modul re(regular expression)\n",
    "- \\W+ = Membuka file untuk ditulis dan dibaca. File yang dibaca dimaksudkan adalah data yang ada pada atribut message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized_message']=df['new_message'].apply(lambda row : tokenize(row.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pengaplikasian fungsi lambda untuk membuat baris baru yang diberi nama new_message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amore wat\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['go',\n",
       " 'until',\n",
       " 'jurong',\n",
       " 'point',\n",
       " 'crazy',\n",
       " 'available',\n",
       " 'only',\n",
       " 'in',\n",
       " 'bugis',\n",
       " 'n',\n",
       " 'great',\n",
       " 'world',\n",
       " 'la',\n",
       " 'e',\n",
       " 'buffet',\n",
       " 'cine',\n",
       " 'there',\n",
       " 'got',\n",
       " 'amore',\n",
       " 'wat']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.new_message[0])\n",
    "df.tokenized_message[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pesan yang pertama merupakan contoh isi data dari atribut new_message dan pesan yang kedua merupakan contoh isi data dari atribut tokenized_message. Bisa dilihat, pada pesan yang pertama, data nya masih membentuk sebuah kalimat, sedangkan setelah dilakukan tokenizing, kalimat pada pesan pertama dipisah menjadi token-token seperti yang ditampilkan pada pesan yang kedua."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Filtering\n",
    "Filtering adalah kegiatan mengambil kata-kata yang penting dan membuang kata-kata yang sering muncul. Kata-kata yang sering muncul dapat berupa kata subjek, kata penghubung, dll. Stopword adalah sebutan untuk kata-kata yang tidak penting atau kurang desrkiptif tersebut. Filtering dilakukan untuk data yang berada di atribut new_message dimana data di dalamnya telah dilakukan tokenizing. Dalam tahap ini, saya akan menghapus stopword yang ada pada atribut new_message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords=nltk.corpus.stopwords.words('english')\n",
    "stopwords[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note :\n",
    "- nltk.corpus.stopwords.words('english') = digunakan untuk mengimport corpus yang ada pada platform nltk. English dimaksudkan untuk data teks yang digunakan adalah teks berbahasa inggris. Corpus tersebut dapat dijadikan acuan untuk melakukan filtering.\n",
    "- Kemudian ditampilkan 20 contoh dari corpus tersebut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(message):\n",
    "    clean_message=[word for word in message if word not in stopwords]\n",
    "    return clean_message "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kegiatan di atas merupakan kegiatan menghapus stopword yang yang ada pada atribut message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_message']=df['tokenized_message'].apply(lambda row : remove_stopwords(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pengaplikasian fungsi lambda untuk membuat baris baru yang diberi nama clean_message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['go', 'until', 'jurong', 'point', 'crazy', 'available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', 'cine', 'there', 'got', 'amore', 'wat']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['go',\n",
       " 'jurong',\n",
       " 'point',\n",
       " 'crazy',\n",
       " 'available',\n",
       " 'bugis',\n",
       " 'n',\n",
       " 'great',\n",
       " 'world',\n",
       " 'la',\n",
       " 'e',\n",
       " 'buffet',\n",
       " 'cine',\n",
       " 'got',\n",
       " 'amore',\n",
       " 'wat']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.tokenized_message[0])\n",
    "df.clean_message[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pesan pertama merupakan contoh isi data di atribut tokenized_message dan pesan kedua merupakan contoh isi data di atribut clean_message. Jika diamati, kata 'until' dihilangkan karena merupakan contoh dari stopword dan ditampilkan pada pesan kedua. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Stemming\n",
    "Stemming adalah teknik mentransformasi kata dalam sebuah kalimat menjadi kata dasar. Contohnya kata 'makanan' akan diubah menjadi 'makan'. Stemming akan menghilangkan sufix dan prefix pada kata. Dalam tahap ini akan dilakukan stemming pada atribut clean_message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note :\n",
    "- nltk.PorterStemmer() = Digunakan untuk menggunakan algortima porter untuk yang ada di dalam modul NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(clean_message):\n",
    "    stemmed_message=[ps.stem(word) for word in clean_message]\n",
    "    return stemmed_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['stemmed_message']=df.clean_message.apply(lambda row : stemming(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['go', 'jurong', 'point', 'crazy', 'available', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', 'cine', 'got', 'amore', 'wat']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['go',\n",
       " 'jurong',\n",
       " 'point',\n",
       " 'crazi',\n",
       " 'avail',\n",
       " 'bugi',\n",
       " 'n',\n",
       " 'great',\n",
       " 'world',\n",
       " 'la',\n",
       " 'e',\n",
       " 'buffet',\n",
       " 'cine',\n",
       " 'got',\n",
       " 'amor',\n",
       " 'wat']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.clean_message[0])\n",
    "df.stemmed_message[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pesan pertama merupakan contoh data di atribut clean_message dan pesan kedua merupakan contoh data di atribut stemmed_message. Jika dilihat kata 'available', dan 'amore' di pesan pertama telah berubah menjadi kata dasar seperti yang ditampilkan di pesan kedua."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Menampilkan Hasil dari Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_message(stemmed_message):\n",
    "    final_message=\" \".join([word for word in stemmed_message])\n",
    "    return final_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['final_message']=df.stemmed_message.apply(lambda row : get_final_message(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>new_message</th>\n",
       "      <th>tokenized_message</th>\n",
       "      <th>clean_message</th>\n",
       "      <th>stemmed_message</th>\n",
       "      <th>final_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>Go until jurong point crazy Available only in ...</td>\n",
       "      <td>[go, until, jurong, point, crazy, available, o...</td>\n",
       "      <td>[go, jurong, point, crazy, available, bugis, n...</td>\n",
       "      <td>[go, jurong, point, crazi, avail, bugi, n, gre...</td>\n",
       "      <td>go jurong point crazi avail bugi n great world...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>Ok lar Joking wif u oni</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joke, wif, u, oni]</td>\n",
       "      <td>ok lar joke wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "      <td>[free, entri, 2, wkli, comp, win, fa, cup, fin...</td>\n",
       "      <td>free entri 2 wkli comp win fa cup final tkt 21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>U dun say so early hor U c already then say</td>\n",
       "      <td>[u, dun, say, so, early, hor, u, c, already, t...</td>\n",
       "      <td>[u, dun, say, early, hor, u, c, already, say]</td>\n",
       "      <td>[u, dun, say, earli, hor, u, c, alreadi, say]</td>\n",
       "      <td>u dun say earli hor u c alreadi say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>Nah I dont think he goes to usf he lives aroun...</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, l...</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, t...</td>\n",
       "      <td>[nah, dont, think, goe, usf, live, around, tho...</td>\n",
       "      <td>nah dont think goe usf live around though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5567</td>\n",
       "      <td>1</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>[this, is, the, 2nd, time, we, have, tried, 2,...</td>\n",
       "      <td>[2nd, time, tried, 2, contact, u, u, å, 750, p...</td>\n",
       "      <td>[2nd, time, tri, 2, contact, u, u, å, 750, pou...</td>\n",
       "      <td>2nd time tri 2 contact u u å 750 pound prize 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5568</td>\n",
       "      <td>0</td>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "      <td>Will Ì b going to esplanade fr home</td>\n",
       "      <td>[will, ì, b, going, to, esplanade, fr, home]</td>\n",
       "      <td>[ì, b, going, esplanade, fr, home]</td>\n",
       "      <td>[ì, b, go, esplanad, fr, home]</td>\n",
       "      <td>ì b go esplanad fr home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5569</td>\n",
       "      <td>0</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>Pity  was in mood for that Soany other suggest...</td>\n",
       "      <td>[pity, was, in, mood, for, that, soany, other,...</td>\n",
       "      <td>[pity, mood, soany, suggestions]</td>\n",
       "      <td>[piti, mood, soani, suggest]</td>\n",
       "      <td>piti mood soani suggest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5570</td>\n",
       "      <td>0</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>The guy did some bitching but I acted like id ...</td>\n",
       "      <td>[the, guy, did, some, bitching, but, i, acted,...</td>\n",
       "      <td>[guy, bitching, acted, like, id, interested, b...</td>\n",
       "      <td>[guy, bitch, act, like, id, interest, buy, som...</td>\n",
       "      <td>guy bitch act like id interest buy someth els ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5571</td>\n",
       "      <td>0</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>Rofl Its true to its name</td>\n",
       "      <td>[rofl, its, true, to, its, name]</td>\n",
       "      <td>[rofl, true, name]</td>\n",
       "      <td>[rofl, true, name]</td>\n",
       "      <td>rofl true name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                            message  \\\n",
       "0         0  Go until jurong point, crazy.. Available only ...   \n",
       "1         0                      Ok lar... Joking wif u oni...   \n",
       "2         1  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3         0  U dun say so early hor... U c already then say...   \n",
       "4         0  Nah I don't think he goes to usf, he lives aro...   \n",
       "...     ...                                                ...   \n",
       "5567      1  This is the 2nd time we have tried 2 contact u...   \n",
       "5568      0              Will Ì_ b going to esplanade fr home?   \n",
       "5569      0  Pity, * was in mood for that. So...any other s...   \n",
       "5570      0  The guy did some bitching but I acted like i'd...   \n",
       "5571      0                         Rofl. Its true to its name   \n",
       "\n",
       "                                            new_message  \\\n",
       "0     Go until jurong point crazy Available only in ...   \n",
       "1                               Ok lar Joking wif u oni   \n",
       "2     Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3           U dun say so early hor U c already then say   \n",
       "4     Nah I dont think he goes to usf he lives aroun...   \n",
       "...                                                 ...   \n",
       "5567  This is the 2nd time we have tried 2 contact u...   \n",
       "5568                Will Ì b going to esplanade fr home   \n",
       "5569  Pity  was in mood for that Soany other suggest...   \n",
       "5570  The guy did some bitching but I acted like id ...   \n",
       "5571                          Rofl Its true to its name   \n",
       "\n",
       "                                      tokenized_message  \\\n",
       "0     [go, until, jurong, point, crazy, available, o...   \n",
       "1                        [ok, lar, joking, wif, u, oni]   \n",
       "2     [free, entry, in, 2, a, wkly, comp, to, win, f...   \n",
       "3     [u, dun, say, so, early, hor, u, c, already, t...   \n",
       "4     [nah, i, dont, think, he, goes, to, usf, he, l...   \n",
       "...                                                 ...   \n",
       "5567  [this, is, the, 2nd, time, we, have, tried, 2,...   \n",
       "5568       [will, ì, b, going, to, esplanade, fr, home]   \n",
       "5569  [pity, was, in, mood, for, that, soany, other,...   \n",
       "5570  [the, guy, did, some, bitching, but, i, acted,...   \n",
       "5571                   [rofl, its, true, to, its, name]   \n",
       "\n",
       "                                          clean_message  \\\n",
       "0     [go, jurong, point, crazy, available, bugis, n...   \n",
       "1                        [ok, lar, joking, wif, u, oni]   \n",
       "2     [free, entry, 2, wkly, comp, win, fa, cup, fin...   \n",
       "3         [u, dun, say, early, hor, u, c, already, say]   \n",
       "4     [nah, dont, think, goes, usf, lives, around, t...   \n",
       "...                                                 ...   \n",
       "5567  [2nd, time, tried, 2, contact, u, u, å, 750, p...   \n",
       "5568                 [ì, b, going, esplanade, fr, home]   \n",
       "5569                   [pity, mood, soany, suggestions]   \n",
       "5570  [guy, bitching, acted, like, id, interested, b...   \n",
       "5571                                 [rofl, true, name]   \n",
       "\n",
       "                                        stemmed_message  \\\n",
       "0     [go, jurong, point, crazi, avail, bugi, n, gre...   \n",
       "1                          [ok, lar, joke, wif, u, oni]   \n",
       "2     [free, entri, 2, wkli, comp, win, fa, cup, fin...   \n",
       "3         [u, dun, say, earli, hor, u, c, alreadi, say]   \n",
       "4     [nah, dont, think, goe, usf, live, around, tho...   \n",
       "...                                                 ...   \n",
       "5567  [2nd, time, tri, 2, contact, u, u, å, 750, pou...   \n",
       "5568                     [ì, b, go, esplanad, fr, home]   \n",
       "5569                       [piti, mood, soani, suggest]   \n",
       "5570  [guy, bitch, act, like, id, interest, buy, som...   \n",
       "5571                                 [rofl, true, name]   \n",
       "\n",
       "                                          final_message  \n",
       "0     go jurong point crazi avail bugi n great world...  \n",
       "1                                 ok lar joke wif u oni  \n",
       "2     free entri 2 wkli comp win fa cup final tkt 21...  \n",
       "3                   u dun say earli hor u c alreadi say  \n",
       "4             nah dont think goe usf live around though  \n",
       "...                                                 ...  \n",
       "5567  2nd time tri 2 contact u u å 750 pound prize 2...  \n",
       "5568                            ì b go esplanad fr home  \n",
       "5569                            piti mood soani suggest  \n",
       "5570  guy bitch act like id interest buy someth els ...  \n",
       "5571                                     rofl true name  \n",
       "\n",
       "[5572 rows x 7 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D. Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature extraction adalah tahapan yang berfungsi untuk mengubah data teks yang berbentuk kata menjadi vektor. Tujuan dari tahapan ini adalah membuat teks ke dalam bentuk pengklasifikasian kelas-kelas yang disediakan. Dalam data terdapat baris dengan teks sehingga kita perlu mengubah setiap kata dalam teks ini menjadi kolom atribut dengan menggunakan TfidfVectorizer. TF-IDF ni adalah singkatan dari singkatan dari “Term Frequency - Inverse Document” Frequency yang merupakan komponen dari skor yang dihasilkan yang ditetapkan untuk setiap kata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>8016</th>\n",
       "      <th>8017</th>\n",
       "      <th>8018</th>\n",
       "      <th>8019</th>\n",
       "      <th>8020</th>\n",
       "      <th>8021</th>\n",
       "      <th>8022</th>\n",
       "      <th>8023</th>\n",
       "      <th>8024</th>\n",
       "      <th>8025</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5567</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5568</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5569</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5570</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 8026 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1     2     3     4     5     6     7     8     9     ...  8016  \\\n",
       "0      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "1      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "2      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "3      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "4      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   ...   \n",
       "5567   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "5568   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "5569   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "5570   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "5571   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "\n",
       "      8017  8018  8019  8020  8021  8022  8023  8024  8025  \n",
       "0      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "5567   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "5568   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "5569   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "5570   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "5571   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5572 rows x 8026 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_model=TfidfVectorizer() # Membuat transform\n",
    "tfidf_vec=tfidf_model.fit_transform(df.final_message) # Membuat token dan kata-kata\n",
    "x = pd.DataFrame(tfidf_vec.toarray()) # Membuat encode dokumen\n",
    "x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    4825\n",
      "1     747\n",
      "Name: label, dtype: int64\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "y = df['label']\n",
    "print (y.value_counts())\n",
    "\n",
    "print(y[0])\n",
    "print(y[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "print(y[0])\n",
    "print(y[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note :\n",
    "- label encoder() = Digunakan untuk memberi label pada kelas agar bisa dibaca oleh komputer. \n",
    "- fit_transform(y) = membentuk kelas menjadi 0 dan 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E. Artificial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting\n",
    "Data yang ada akan dibagi secara acak, dengan persentase 80% untuk data training dan 20% untuk data testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size= 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change label to one hot encoder\n",
    "ytrain_one_hot = to_categorical(ytrain)\n",
    "ytest_one_hot = to_categorical(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dalam tahap ini, saya menggunakan 3 layer.\n",
    "- Layer pertama adalah hidden layer sekaligus input layer. Hidden layer yang digunakan sebanyak 50 dan input layer sebanyak 8026. Fungsi aktivasi yang digunakan adalah 'selu'\n",
    "- Layer kedua adalah hidden layer sebanyak 2 yang menggunakan fungsi aktivasi softmax\n",
    "- Layer ketiga adalah output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=8026, activation='selu'))#Input layer\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(optimizer=Adam(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4457 samples, validate on 1115 samples\n",
      "Epoch 1/200\n",
      "4457/4457 [==============================] - 21s 5ms/step - loss: 0.6838 - acc: 0.6493 - val_loss: 0.6754 - val_acc: 0.7587\n",
      "Epoch 2/200\n",
      "4457/4457 [==============================] - 4s 986us/step - loss: 0.6666 - acc: 0.8275 - val_loss: 0.6594 - val_acc: 0.8583\n",
      "Epoch 3/200\n",
      "4457/4457 [==============================] - 4s 824us/step - loss: 0.6496 - acc: 0.8878 - val_loss: 0.6436 - val_acc: 0.8628\n",
      "Epoch 4/200\n",
      "4457/4457 [==============================] - 3s 769us/step - loss: 0.6326 - acc: 0.8867 - val_loss: 0.6278 - val_acc: 0.8610\n",
      "Epoch 5/200\n",
      "4457/4457 [==============================] - 3s 574us/step - loss: 0.6157 - acc: 0.8813 - val_loss: 0.6120 - val_acc: 0.8592\n",
      "Epoch 6/200\n",
      "4457/4457 [==============================] - 3s 723us/step - loss: 0.5988 - acc: 0.8777 - val_loss: 0.5963 - val_acc: 0.8574\n",
      "Epoch 7/200\n",
      "4457/4457 [==============================] - 5s 1ms/step - loss: 0.5819 - acc: 0.8761 - val_loss: 0.5807 - val_acc: 0.8574\n",
      "Epoch 8/200\n",
      "4457/4457 [==============================] - 4s 868us/step - loss: 0.5653 - acc: 0.8757 - val_loss: 0.5653 - val_acc: 0.8574\n",
      "Epoch 9/200\n",
      "4457/4457 [==============================] - 3s 722us/step - loss: 0.5489 - acc: 0.8750 - val_loss: 0.5503 - val_acc: 0.8574\n",
      "Epoch 10/200\n",
      "4457/4457 [==============================] - 3s 628us/step - loss: 0.5328 - acc: 0.8748 - val_loss: 0.5356 - val_acc: 0.8574\n",
      "Epoch 11/200\n",
      "4457/4457 [==============================] - 3s 586us/step - loss: 0.5172 - acc: 0.8741 - val_loss: 0.5213 - val_acc: 0.8574\n",
      "Epoch 12/200\n",
      "4457/4457 [==============================] - 3s 775us/step - loss: 0.5019 - acc: 0.8744 - val_loss: 0.5074 - val_acc: 0.8574\n",
      "Epoch 13/200\n",
      "4457/4457 [==============================] - 4s 807us/step - loss: 0.4870 - acc: 0.8741 - val_loss: 0.4939 - val_acc: 0.8574\n",
      "Epoch 14/200\n",
      "4457/4457 [==============================] - 3s 602us/step - loss: 0.4727 - acc: 0.8744 - val_loss: 0.4809 - val_acc: 0.8574\n",
      "Epoch 15/200\n",
      "4457/4457 [==============================] - 3s 562us/step - loss: 0.4588 - acc: 0.8750 - val_loss: 0.4684 - val_acc: 0.8574\n",
      "Epoch 16/200\n",
      "4457/4457 [==============================] - 3s 586us/step - loss: 0.4454 - acc: 0.8755 - val_loss: 0.4563 - val_acc: 0.8574\n",
      "Epoch 17/200\n",
      "4457/4457 [==============================] - 5s 1ms/step - loss: 0.4325 - acc: 0.8761 - val_loss: 0.4447 - val_acc: 0.8583\n",
      "Epoch 18/200\n",
      "4457/4457 [==============================] - 4s 893us/step - loss: 0.4201 - acc: 0.8770 - val_loss: 0.4335 - val_acc: 0.8592\n",
      "Epoch 19/200\n",
      "4457/4457 [==============================] - 4s 816us/step - loss: 0.4081 - acc: 0.8775 - val_loss: 0.4227 - val_acc: 0.8601\n",
      "Epoch 20/200\n",
      "4457/4457 [==============================] - 4s 794us/step - loss: 0.3966 - acc: 0.8786 - val_loss: 0.4124 - val_acc: 0.8601\n",
      "Epoch 21/200\n",
      "4457/4457 [==============================] - 3s 778us/step - loss: 0.3856 - acc: 0.8788 - val_loss: 0.4026 - val_acc: 0.8619\n",
      "Epoch 22/200\n",
      "4457/4457 [==============================] - 4s 844us/step - loss: 0.3750 - acc: 0.8788 - val_loss: 0.3931 - val_acc: 0.8619\n",
      "Epoch 23/200\n",
      "4457/4457 [==============================] - 4s 871us/step - loss: 0.3649 - acc: 0.8793 - val_loss: 0.3841 - val_acc: 0.8628\n",
      "Epoch 24/200\n",
      "4457/4457 [==============================] - 3s 662us/step - loss: 0.3552 - acc: 0.8793 - val_loss: 0.3755 - val_acc: 0.8628\n",
      "Epoch 25/200\n",
      "4457/4457 [==============================] - 3s 730us/step - loss: 0.3459 - acc: 0.8800 - val_loss: 0.3672 - val_acc: 0.8628\n",
      "Epoch 26/200\n",
      "4457/4457 [==============================] - 3s 763us/step - loss: 0.3369 - acc: 0.8815 - val_loss: 0.3593 - val_acc: 0.8637\n",
      "Epoch 27/200\n",
      "4457/4457 [==============================] - 3s 679us/step - loss: 0.3284 - acc: 0.8824 - val_loss: 0.3517 - val_acc: 0.8637\n",
      "Epoch 28/200\n",
      "4457/4457 [==============================] - 3s 568us/step - loss: 0.3201 - acc: 0.8840 - val_loss: 0.3445 - val_acc: 0.8646\n",
      "Epoch 29/200\n",
      "4457/4457 [==============================] - 3s 583us/step - loss: 0.3123 - acc: 0.8853 - val_loss: 0.3375 - val_acc: 0.8673\n",
      "Epoch 30/200\n",
      "4457/4457 [==============================] - 3s 617us/step - loss: 0.3047 - acc: 0.8865 - val_loss: 0.3309 - val_acc: 0.8691\n",
      "Epoch 31/200\n",
      "4457/4457 [==============================] - 5s 1ms/step - loss: 0.2975 - acc: 0.8887 - val_loss: 0.3246 - val_acc: 0.8691 0s - loss: 0.2988 - acc: 0\n",
      "Epoch 32/200\n",
      "4457/4457 [==============================] - 4s 974us/step - loss: 0.2906 - acc: 0.8905 - val_loss: 0.3185 - val_acc: 0.8691\n",
      "Epoch 33/200\n",
      "4457/4457 [==============================] - 3s 680us/step - loss: 0.2839 - acc: 0.8934 - val_loss: 0.3126 - val_acc: 0.8717\n",
      "Epoch 34/200\n",
      "4457/4457 [==============================] - 3s 642us/step - loss: 0.2775 - acc: 0.8952 - val_loss: 0.3071 - val_acc: 0.8726\n",
      "Epoch 35/200\n",
      "4457/4457 [==============================] - 4s 842us/step - loss: 0.2714 - acc: 0.8975 - val_loss: 0.3017 - val_acc: 0.8744\n",
      "Epoch 36/200\n",
      "4457/4457 [==============================] - 4s 917us/step - loss: 0.2655 - acc: 0.8993 - val_loss: 0.2965 - val_acc: 0.8744\n",
      "Epoch 37/200\n",
      "4457/4457 [==============================] - 3s 747us/step - loss: 0.2598 - acc: 0.8999 - val_loss: 0.2915 - val_acc: 0.8762\n",
      "Epoch 38/200\n",
      "4457/4457 [==============================] - 3s 724us/step - loss: 0.2543 - acc: 0.9015 - val_loss: 0.2868 - val_acc: 0.8771\n",
      "Epoch 39/200\n",
      "4457/4457 [==============================] - 3s 671us/step - loss: 0.2490 - acc: 0.9035 - val_loss: 0.2821 - val_acc: 0.8807\n",
      "Epoch 40/200\n",
      "4457/4457 [==============================] - 4s 877us/step - loss: 0.2439 - acc: 0.9064 - val_loss: 0.2777 - val_acc: 0.8807\n",
      "Epoch 41/200\n",
      "4457/4457 [==============================] - 4s 910us/step - loss: 0.2390 - acc: 0.9082 - val_loss: 0.2734 - val_acc: 0.8816ss: 0.2399 - a - ETA: 1s - l\n",
      "Epoch 42/200\n",
      "4457/4457 [==============================] - 3s 719us/step - loss: 0.2342 - acc: 0.9105 - val_loss: 0.2692 - val_acc: 0.8843\n",
      "Epoch 43/200\n",
      "4457/4457 [==============================] - 3s 662us/step - loss: 0.2296 - acc: 0.9127 - val_loss: 0.2652 - val_acc: 0.8852\n",
      "Epoch 44/200\n",
      "4457/4457 [==============================] - 4s 793us/step - loss: 0.2250 - acc: 0.9147 - val_loss: 0.2613 - val_acc: 0.8861\n",
      "Epoch 45/200\n",
      "4457/4457 [==============================] - 4s 973us/step - loss: 0.2207 - acc: 0.9168 - val_loss: 0.2574 - val_acc: 0.8879\n",
      "Epoch 46/200\n",
      "4457/4457 [==============================] - 3s 740us/step - loss: 0.2164 - acc: 0.9186 - val_loss: 0.2536 - val_acc: 0.8906\n",
      "Epoch 47/200\n",
      "4457/4457 [==============================] - 3s 764us/step - loss: 0.2122 - acc: 0.9197 - val_loss: 0.2499 - val_acc: 0.8906\n",
      "Epoch 48/200\n",
      "4457/4457 [==============================] - 3s 702us/step - loss: 0.2082 - acc: 0.9212 - val_loss: 0.2464 - val_acc: 0.8933\n",
      "Epoch 49/200\n",
      "4457/4457 [==============================] - 4s 969us/step - loss: 0.2042 - acc: 0.9239 - val_loss: 0.2429 - val_acc: 0.8960\n",
      "Epoch 50/200\n",
      "4457/4457 [==============================] - 4s 891us/step - loss: 0.2003 - acc: 0.9255 - val_loss: 0.2394 - val_acc: 0.8978\n",
      "Epoch 51/200\n",
      "4457/4457 [==============================] - 3s 765us/step - loss: 0.1965 - acc: 0.9278 - val_loss: 0.2360 - val_acc: 0.8996\n",
      "Epoch 52/200\n",
      "4457/4457 [==============================] - 3s 679us/step - loss: 0.1927 - acc: 0.9304 - val_loss: 0.2326 - val_acc: 0.9013\n",
      "Epoch 53/200\n",
      "4457/4457 [==============================] - 3s 723us/step - loss: 0.1890 - acc: 0.9329 - val_loss: 0.2293 - val_acc: 0.9022\n",
      "Epoch 54/200\n",
      "4457/4457 [==============================] - 4s 788us/step - loss: 0.1854 - acc: 0.9345 - val_loss: 0.2260 - val_acc: 0.9031\n",
      "Epoch 55/200\n",
      "4457/4457 [==============================] - 3s 639us/step - loss: 0.1818 - acc: 0.9365 - val_loss: 0.2228 - val_acc: 0.9040\n",
      "Epoch 56/200\n",
      "4457/4457 [==============================] - 3s 612us/step - loss: 0.1784 - acc: 0.9379 - val_loss: 0.2196 - val_acc: 0.9067\n",
      "Epoch 57/200\n",
      "4457/4457 [==============================] - 2s 557us/step - loss: 0.1749 - acc: 0.9390 - val_loss: 0.2165 - val_acc: 0.9067\n",
      "Epoch 58/200\n",
      "4457/4457 [==============================] - 3s 581us/step - loss: 0.1715 - acc: 0.9414 - val_loss: 0.2134 - val_acc: 0.9085\n",
      "Epoch 59/200\n",
      "4457/4457 [==============================] - 3s 782us/step - loss: 0.1682 - acc: 0.9430 - val_loss: 0.2103 - val_acc: 0.9139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/200\n",
      "4457/4457 [==============================] - 4s 837us/step - loss: 0.1649 - acc: 0.9450 - val_loss: 0.2073 - val_acc: 0.9166\n",
      "Epoch 61/200\n",
      "4457/4457 [==============================] - 3s 578us/step - loss: 0.1616 - acc: 0.9473 - val_loss: 0.2043 - val_acc: 0.9193\n",
      "Epoch 62/200\n",
      "4457/4457 [==============================] - 3s 583us/step - loss: 0.1585 - acc: 0.9500 - val_loss: 0.2013 - val_acc: 0.9247\n",
      "Epoch 63/200\n",
      "4457/4457 [==============================] - 3s 604us/step - loss: 0.1553 - acc: 0.9518 - val_loss: 0.1984 - val_acc: 0.9256\n",
      "Epoch 64/200\n",
      "4457/4457 [==============================] - 3s 701us/step - loss: 0.1522 - acc: 0.9529 - val_loss: 0.1955 - val_acc: 0.9265\n",
      "Epoch 65/200\n",
      "4457/4457 [==============================] - 3s 769us/step - loss: 0.1492 - acc: 0.9547 - val_loss: 0.1927 - val_acc: 0.9274\n",
      "Epoch 66/200\n",
      "4457/4457 [==============================] - 3s 626us/step - loss: 0.1462 - acc: 0.9551 - val_loss: 0.1899 - val_acc: 0.9283\n",
      "Epoch 67/200\n",
      "4457/4457 [==============================] - 3s 606us/step - loss: 0.1433 - acc: 0.9578 - val_loss: 0.1871 - val_acc: 0.9283\n",
      "Epoch 68/200\n",
      "4457/4457 [==============================] - 3s 602us/step - loss: 0.1404 - acc: 0.9587 - val_loss: 0.1845 - val_acc: 0.9291\n",
      "Epoch 69/200\n",
      "4457/4457 [==============================] - 3s 567us/step - loss: 0.1376 - acc: 0.9607 - val_loss: 0.1818 - val_acc: 0.9300\n",
      "Epoch 70/200\n",
      "4457/4457 [==============================] - 4s 832us/step - loss: 0.1348 - acc: 0.9619 - val_loss: 0.1792 - val_acc: 0.9309\n",
      "Epoch 71/200\n",
      "4457/4457 [==============================] - 3s 710us/step - loss: 0.1321 - acc: 0.9623 - val_loss: 0.1766 - val_acc: 0.9318\n",
      "Epoch 72/200\n",
      "4457/4457 [==============================] - 3s 583us/step - loss: 0.1295 - acc: 0.9630 - val_loss: 0.1741 - val_acc: 0.9345\n",
      "Epoch 73/200\n",
      "4457/4457 [==============================] - 3s 582us/step - loss: 0.1269 - acc: 0.9639 - val_loss: 0.1716 - val_acc: 0.9363\n",
      "Epoch 74/200\n",
      "4457/4457 [==============================] - 3s 610us/step - loss: 0.1243 - acc: 0.9648 - val_loss: 0.1692 - val_acc: 0.9363\n",
      "Epoch 75/200\n",
      "4457/4457 [==============================] - 3s 682us/step - loss: 0.1219 - acc: 0.9657 - val_loss: 0.1668 - val_acc: 0.9363\n",
      "Epoch 76/200\n",
      "4457/4457 [==============================] - 3s 742us/step - loss: 0.1194 - acc: 0.9668 - val_loss: 0.1645 - val_acc: 0.9381\n",
      "Epoch 77/200\n",
      "4457/4457 [==============================] - 3s 651us/step - loss: 0.1170 - acc: 0.9681 - val_loss: 0.1622 - val_acc: 0.9399\n",
      "Epoch 78/200\n",
      "4457/4457 [==============================] - 3s 574us/step - loss: 0.1147 - acc: 0.9695 - val_loss: 0.1600 - val_acc: 0.9408\n",
      "Epoch 79/200\n",
      "4457/4457 [==============================] - 3s 605us/step - loss: 0.1124 - acc: 0.9706 - val_loss: 0.1578 - val_acc: 0.9408\n",
      "Epoch 80/200\n",
      "4457/4457 [==============================] - 3s 590us/step - loss: 0.1102 - acc: 0.9722 - val_loss: 0.1556 - val_acc: 0.9417\n",
      "Epoch 81/200\n",
      "4457/4457 [==============================] - 4s 850us/step - loss: 0.1080 - acc: 0.9726 - val_loss: 0.1535 - val_acc: 0.9426\n",
      "Epoch 82/200\n",
      "4457/4457 [==============================] - 3s 710us/step - loss: 0.1059 - acc: 0.9733 - val_loss: 0.1515 - val_acc: 0.9435\n",
      "Epoch 83/200\n",
      "4457/4457 [==============================] - 2s 556us/step - loss: 0.1038 - acc: 0.9735 - val_loss: 0.1495 - val_acc: 0.9453\n",
      "Epoch 84/200\n",
      "4457/4457 [==============================] - 2s 560us/step - loss: 0.1018 - acc: 0.9746 - val_loss: 0.1475 - val_acc: 0.9471\n",
      "Epoch 85/200\n",
      "4457/4457 [==============================] - 3s 628us/step - loss: 0.0998 - acc: 0.9762 - val_loss: 0.1456 - val_acc: 0.9489\n",
      "Epoch 86/200\n",
      "4457/4457 [==============================] - 3s 705us/step - loss: 0.0978 - acc: 0.9778 - val_loss: 0.1437 - val_acc: 0.9489\n",
      "Epoch 87/200\n",
      "4457/4457 [==============================] - 3s 776us/step - loss: 0.0959 - acc: 0.9782 - val_loss: 0.1418 - val_acc: 0.9489\n",
      "Epoch 88/200\n",
      "4457/4457 [==============================] - 3s 643us/step - loss: 0.0941 - acc: 0.9785 - val_loss: 0.1401 - val_acc: 0.9507\n",
      "Epoch 89/200\n",
      "4457/4457 [==============================] - 3s 569us/step - loss: 0.0923 - acc: 0.9785 - val_loss: 0.1384 - val_acc: 0.9507\n",
      "Epoch 90/200\n",
      "4457/4457 [==============================] - 3s 569us/step - loss: 0.0905 - acc: 0.9791 - val_loss: 0.1366 - val_acc: 0.9534\n",
      "Epoch 91/200\n",
      "4457/4457 [==============================] - 2s 552us/step - loss: 0.0888 - acc: 0.9794 - val_loss: 0.1350 - val_acc: 0.9552\n",
      "Epoch 92/200\n",
      "4457/4457 [==============================] - 4s 788us/step - loss: 0.0871 - acc: 0.9800 - val_loss: 0.1334 - val_acc: 0.9561\n",
      "Epoch 93/200\n",
      "4457/4457 [==============================] - 3s 757us/step - loss: 0.0855 - acc: 0.9807 - val_loss: 0.1318 - val_acc: 0.9570\n",
      "Epoch 94/200\n",
      "4457/4457 [==============================] - 3s 572us/step - loss: 0.0839 - acc: 0.9809 - val_loss: 0.1302 - val_acc: 0.9570\n",
      "Epoch 95/200\n",
      "4457/4457 [==============================] - 3s 581us/step - loss: 0.0823 - acc: 0.9818 - val_loss: 0.1287 - val_acc: 0.9570\n",
      "Epoch 96/200\n",
      "4457/4457 [==============================] - 3s 576us/step - loss: 0.0808 - acc: 0.9825 - val_loss: 0.1272 - val_acc: 0.9596\n",
      "Epoch 97/200\n",
      "4457/4457 [==============================] - 4s 815us/step - loss: 0.0793 - acc: 0.9827 - val_loss: 0.1258 - val_acc: 0.9596\n",
      "Epoch 98/200\n",
      "4457/4457 [==============================] - 4s 918us/step - loss: 0.0778 - acc: 0.9829 - val_loss: 0.1244 - val_acc: 0.9596\n",
      "Epoch 99/200\n",
      "4457/4457 [==============================] - 4s 895us/step - loss: 0.0764 - acc: 0.9832 - val_loss: 0.1230 - val_acc: 0.9596\n",
      "Epoch 100/200\n",
      "4457/4457 [==============================] - 3s 708us/step - loss: 0.0750 - acc: 0.9838 - val_loss: 0.1217 - val_acc: 0.9596\n",
      "Epoch 101/200\n",
      "4457/4457 [==============================] - 3s 719us/step - loss: 0.0736 - acc: 0.9841 - val_loss: 0.1204 - val_acc: 0.9605\n",
      "Epoch 102/200\n",
      "4457/4457 [==============================] - 4s 969us/step - loss: 0.0723 - acc: 0.9841 - val_loss: 0.1191 - val_acc: 0.9605\n",
      "Epoch 103/200\n",
      "4457/4457 [==============================] - 4s 810us/step - loss: 0.0710 - acc: 0.9847 - val_loss: 0.1178 - val_acc: 0.9614- loss: 0.073\n",
      "Epoch 104/200\n",
      "4457/4457 [==============================] - 3s 737us/step - loss: 0.0697 - acc: 0.9847 - val_loss: 0.1166 - val_acc: 0.9614\n",
      "Epoch 105/200\n",
      "4457/4457 [==============================] - 3s 671us/step - loss: 0.0684 - acc: 0.9850 - val_loss: 0.1154 - val_acc: 0.9614\n",
      "Epoch 106/200\n",
      "4457/4457 [==============================] - 4s 805us/step - loss: 0.0672 - acc: 0.9852 - val_loss: 0.1142 - val_acc: 0.9623\n",
      "Epoch 107/200\n",
      "4457/4457 [==============================] - 5s 1ms/step - loss: 0.0660 - acc: 0.9854 - val_loss: 0.1131 - val_acc: 0.9623\n",
      "Epoch 108/200\n",
      "4457/4457 [==============================] - 3s 767us/step - loss: 0.0649 - acc: 0.9854 - val_loss: 0.1120 - val_acc: 0.9623\n",
      "Epoch 109/200\n",
      "4457/4457 [==============================] - 4s 848us/step - loss: 0.0637 - acc: 0.9863 - val_loss: 0.1109 - val_acc: 0.9623\n",
      "Epoch 110/200\n",
      "4457/4457 [==============================] - 5s 1ms/step - loss: 0.0626 - acc: 0.9868 - val_loss: 0.1098 - val_acc: 0.9632\n",
      "Epoch 111/200\n",
      "4457/4457 [==============================] - 6s 1ms/step - loss: 0.0615 - acc: 0.9870 - val_loss: 0.1088 - val_acc: 0.9632\n",
      "Epoch 112/200\n",
      "4457/4457 [==============================] - 4s 930us/step - loss: 0.0604 - acc: 0.9872 - val_loss: 0.1078 - val_acc: 0.9632\n",
      "Epoch 113/200\n",
      "4457/4457 [==============================] - 4s 971us/step - loss: 0.0594 - acc: 0.9872 - val_loss: 0.1068 - val_acc: 0.9641\n",
      "Epoch 114/200\n",
      "4457/4457 [==============================] - 5s 1ms/step - loss: 0.0584 - acc: 0.9874 - val_loss: 0.1059 - val_acc: 0.9650\n",
      "Epoch 115/200\n",
      "4457/4457 [==============================] - 4s 986us/step - loss: 0.0574 - acc: 0.9872 - val_loss: 0.1049 - val_acc: 0.9650\n",
      "Epoch 116/200\n",
      "4457/4457 [==============================] - 4s 995us/step - loss: 0.0564 - acc: 0.9877 - val_loss: 0.1040 - val_acc: 0.9659s - loss: 0.0563 - acc: 0.9 - ETA: 0s - loss: 0.0566 - acc: 0.98\n",
      "Epoch 117/200\n",
      "4457/4457 [==============================] - 5s 1ms/step - loss: 0.0554 - acc: 0.9881 - val_loss: 0.1031 - val_acc: 0.9659\n",
      "Epoch 118/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4457/4457 [==============================] - 5s 1ms/step - loss: 0.0545 - acc: 0.9881 - val_loss: 0.1022 - val_acc: 0.9659\n",
      "Epoch 119/200\n",
      "4457/4457 [==============================] - 3s 756us/step - loss: 0.0536 - acc: 0.9881 - val_loss: 0.1014 - val_acc: 0.9668\n",
      "Epoch 120/200\n",
      "4457/4457 [==============================] - 5s 1ms/step - loss: 0.0527 - acc: 0.9881 - val_loss: 0.1006 - val_acc: 0.9668\n",
      "Epoch 121/200\n",
      "4457/4457 [==============================] - 5s 1ms/step - loss: 0.0518 - acc: 0.9883 - val_loss: 0.0998 - val_acc: 0.9668\n",
      "Epoch 122/200\n",
      "4457/4457 [==============================] - 4s 969us/step - loss: 0.0510 - acc: 0.9883 - val_loss: 0.0990 - val_acc: 0.9668\n",
      "Epoch 123/200\n",
      "4457/4457 [==============================] - 4s 946us/step - loss: 0.0502 - acc: 0.9886 - val_loss: 0.0982 - val_acc: 0.9668\n",
      "Epoch 124/200\n",
      "4457/4457 [==============================] - 5s 1ms/step - loss: 0.0493 - acc: 0.9886 - val_loss: 0.0975 - val_acc: 0.9668\n",
      "Epoch 125/200\n",
      "4457/4457 [==============================] - 5s 1ms/step - loss: 0.0485 - acc: 0.9888 - val_loss: 0.0967 - val_acc: 0.9668\n",
      "Epoch 126/200\n",
      "4457/4457 [==============================] - 4s 808us/step - loss: 0.0478 - acc: 0.9897 - val_loss: 0.0960 - val_acc: 0.9668\n",
      "Epoch 127/200\n",
      "4457/4457 [==============================] - 4s 867us/step - loss: 0.0470 - acc: 0.9897 - val_loss: 0.0954 - val_acc: 0.9668\n",
      "Epoch 128/200\n",
      "4457/4457 [==============================] - 5s 1ms/step - loss: 0.0462 - acc: 0.9899 - val_loss: 0.0947 - val_acc: 0.9668\n",
      "Epoch 129/200\n",
      "4457/4457 [==============================] - 5s 1ms/step - loss: 0.0455 - acc: 0.9901 - val_loss: 0.0940 - val_acc: 0.9668\n",
      "Epoch 130/200\n",
      "4457/4457 [==============================] - 5s 1ms/step - loss: 0.0448 - acc: 0.9901 - val_loss: 0.0933 - val_acc: 0.9677\n",
      "Epoch 131/200\n",
      "4457/4457 [==============================] - 5s 1ms/step - loss: 0.0441 - acc: 0.9904 - val_loss: 0.0927 - val_acc: 0.9686\n",
      "Epoch 132/200\n",
      "4457/4457 [==============================] - 5s 1ms/step - loss: 0.0434 - acc: 0.9904 - val_loss: 0.0921 - val_acc: 0.9686\n",
      "Epoch 133/200\n",
      "4457/4457 [==============================] - 4s 892us/step - loss: 0.0427 - acc: 0.9904 - val_loss: 0.0915 - val_acc: 0.9695\n",
      "Epoch 134/200\n",
      "4457/4457 [==============================] - 5s 1ms/step - loss: 0.0420 - acc: 0.9904 - val_loss: 0.0910 - val_acc: 0.9695\n",
      "Epoch 135/200\n",
      "4457/4457 [==============================] - 5s 1ms/step - loss: 0.0414 - acc: 0.9906 - val_loss: 0.0904 - val_acc: 0.9695 - loss: 0.0392 - \n",
      "Epoch 136/200\n",
      "4457/4457 [==============================] - 4s 881us/step - loss: 0.0408 - acc: 0.9906 - val_loss: 0.0899 - val_acc: 0.9704 1s - loss: \n",
      "Epoch 137/200\n",
      "4457/4457 [==============================] - 4s 883us/step - loss: 0.0401 - acc: 0.9908 - val_loss: 0.0893 - val_acc: 0.9704\n",
      "Epoch 138/200\n",
      "4457/4457 [==============================] - 5s 1ms/step - loss: 0.0395 - acc: 0.9908 - val_loss: 0.0888 - val_acc: 0.9704\n",
      "Epoch 139/200\n",
      "4457/4457 [==============================] - 5s 1ms/step - loss: 0.0389 - acc: 0.9910 - val_loss: 0.0883 - val_acc: 0.9713\n",
      "Epoch 140/200\n",
      "4457/4457 [==============================] - 4s 832us/step - loss: 0.0383 - acc: 0.9910 - val_loss: 0.0878 - val_acc: 0.9722\n",
      "Epoch 141/200\n",
      "4457/4457 [==============================] - 4s 813us/step - loss: 0.0378 - acc: 0.9912 - val_loss: 0.0873 - val_acc: 0.9722\n",
      "Epoch 142/200\n",
      "4457/4457 [==============================] - 5s 1ms/step - loss: 0.0372 - acc: 0.9917 - val_loss: 0.0868 - val_acc: 0.9722- \n",
      "Epoch 143/200\n",
      "4457/4457 [==============================] - 4s 1ms/step - loss: 0.0366 - acc: 0.9921 - val_loss: 0.0864 - val_acc: 0.9722\n",
      "Epoch 144/200\n",
      "4457/4457 [==============================] - 4s 859us/step - loss: 0.0361 - acc: 0.9924 - val_loss: 0.0859 - val_acc: 0.9722\n",
      "Epoch 145/200\n",
      "4457/4457 [==============================] - 5s 1ms/step - loss: 0.0355 - acc: 0.9924 - val_loss: 0.0855 - val_acc: 0.9722\n",
      "Epoch 146/200\n",
      "4457/4457 [==============================] - 6s 1ms/step - loss: 0.0350 - acc: 0.9924 - val_loss: 0.0851 - val_acc: 0.9722\n",
      "Epoch 147/200\n",
      "4457/4457 [==============================] - 4s 1ms/step - loss: 0.0345 - acc: 0.9924 - val_loss: 0.0846 - val_acc: 0.9722\n",
      "Epoch 148/200\n",
      "4457/4457 [==============================] - 5s 1ms/step - loss: 0.0340 - acc: 0.9926 - val_loss: 0.0842 - val_acc: 0.9722\n",
      "Epoch 149/200\n",
      "4457/4457 [==============================] - 5s 1ms/step - loss: 0.0335 - acc: 0.9930 - val_loss: 0.0838 - val_acc: 0.9713\n",
      "Epoch 150/200\n",
      "4457/4457 [==============================] - 4s 841us/step - loss: 0.0330 - acc: 0.9935 - val_loss: 0.0834 - val_acc: 0.9713\n",
      "Epoch 151/200\n",
      "4457/4457 [==============================] - 4s 801us/step - loss: 0.0325 - acc: 0.9935 - val_loss: 0.0830 - val_acc: 0.9713\n",
      "Epoch 152/200\n",
      "4457/4457 [==============================] - 4s 856us/step - loss: 0.0320 - acc: 0.9935 - val_loss: 0.0826 - val_acc: 0.9713\n",
      "Epoch 153/200\n",
      "4457/4457 [==============================] - 5s 1ms/step - loss: 0.0316 - acc: 0.9935 - val_loss: 0.0822 - val_acc: 0.9713\n",
      "Epoch 154/200\n",
      "4457/4457 [==============================] - 4s 982us/step - loss: 0.0311 - acc: 0.9935 - val_loss: 0.0819 - val_acc: 0.9713\n",
      "Epoch 155/200\n",
      "4457/4457 [==============================] - 4s 813us/step - loss: 0.0307 - acc: 0.9935 - val_loss: 0.0816 - val_acc: 0.9713\n",
      "Epoch 156/200\n",
      "4457/4457 [==============================] - 5s 1ms/step - loss: 0.0302 - acc: 0.9939 - val_loss: 0.0812 - val_acc: 0.9731\n",
      "Epoch 157/200\n",
      "4457/4457 [==============================] - 5s 1ms/step - loss: 0.0298 - acc: 0.9939 - val_loss: 0.0809 - val_acc: 0.9731\n",
      "Epoch 158/200\n",
      "4457/4457 [==============================] - 4s 899us/step - loss: 0.0294 - acc: 0.9939 - val_loss: 0.0806 - val_acc: 0.9731\n",
      "Epoch 159/200\n",
      "4457/4457 [==============================] - 3s 753us/step - loss: 0.0289 - acc: 0.9939 - val_loss: 0.0803 - val_acc: 0.9731\n",
      "Epoch 160/200\n",
      "4457/4457 [==============================] - 4s 892us/step - loss: 0.0285 - acc: 0.9942 - val_loss: 0.0800 - val_acc: 0.9731\n",
      "Epoch 161/200\n",
      "4457/4457 [==============================] - 4s 857us/step - loss: 0.0281 - acc: 0.9942 - val_loss: 0.0797 - val_acc: 0.9731\n",
      "Epoch 162/200\n",
      "4457/4457 [==============================] - 3s 640us/step - loss: 0.0277 - acc: 0.9942 - val_loss: 0.0794 - val_acc: 0.9731\n",
      "Epoch 163/200\n",
      "4457/4457 [==============================] - 3s 690us/step - loss: 0.0273 - acc: 0.9944 - val_loss: 0.0791 - val_acc: 0.9731\n",
      "Epoch 164/200\n",
      "4457/4457 [==============================] - 4s 837us/step - loss: 0.0270 - acc: 0.9944 - val_loss: 0.0788 - val_acc: 0.9731\n",
      "Epoch 165/200\n",
      "4457/4457 [==============================] - 4s 925us/step - loss: 0.0266 - acc: 0.9944 - val_loss: 0.0786 - val_acc: 0.9731\n",
      "Epoch 166/200\n",
      "4457/4457 [==============================] - 4s 786us/step - loss: 0.0262 - acc: 0.9944 - val_loss: 0.0783 - val_acc: 0.9731\n",
      "Epoch 167/200\n",
      "4457/4457 [==============================] - 3s 668us/step - loss: 0.0259 - acc: 0.9944 - val_loss: 0.0781 - val_acc: 0.9731loss: 0.0263 - acc: 0\n",
      "Epoch 168/200\n",
      "4457/4457 [==============================] - 3s 674us/step - loss: 0.0255 - acc: 0.9944 - val_loss: 0.0779 - val_acc: 0.9731\n",
      "Epoch 169/200\n",
      "4457/4457 [==============================] - 4s 869us/step - loss: 0.0252 - acc: 0.9944 - val_loss: 0.0777 - val_acc: 0.9731\n",
      "Epoch 170/200\n",
      "4457/4457 [==============================] - 4s 955us/step - loss: 0.0248 - acc: 0.9944 - val_loss: 0.0775 - val_acc: 0.9731\n",
      "Epoch 171/200\n",
      "4457/4457 [==============================] - 3s 663us/step - loss: 0.0245 - acc: 0.9944 - val_loss: 0.0772 - val_acc: 0.9731\n",
      "Epoch 172/200\n",
      "4457/4457 [==============================] - 3s 678us/step - loss: 0.0241 - acc: 0.9944 - val_loss: 0.0770 - val_acc: 0.9731\n",
      "Epoch 173/200\n",
      "4457/4457 [==============================] - 4s 828us/step - loss: 0.0238 - acc: 0.9946 - val_loss: 0.0768 - val_acc: 0.9731\n",
      "Epoch 174/200\n",
      "4457/4457 [==============================] - 4s 936us/step - loss: 0.0235 - acc: 0.9948 - val_loss: 0.0766 - val_acc: 0.9731\n",
      "Epoch 175/200\n",
      "4457/4457 [==============================] - 4s 810us/step - loss: 0.0232 - acc: 0.9948 - val_loss: 0.0764 - val_acc: 0.9731\n",
      "Epoch 176/200\n",
      "4457/4457 [==============================] - 3s 704us/step - loss: 0.0228 - acc: 0.9948 - val_loss: 0.0762 - val_acc: 0.9731\n",
      "Epoch 177/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4457/4457 [==============================] - 3s 677us/step - loss: 0.0225 - acc: 0.9948 - val_loss: 0.0760 - val_acc: 0.9731\n",
      "Epoch 178/200\n",
      "4457/4457 [==============================] - 4s 829us/step - loss: 0.0222 - acc: 0.9948 - val_loss: 0.0758 - val_acc: 0.9740\n",
      "Epoch 179/200\n",
      "4457/4457 [==============================] - 4s 891us/step - loss: 0.0219 - acc: 0.9951 - val_loss: 0.0756 - val_acc: 0.9740\n",
      "Epoch 180/200\n",
      "4457/4457 [==============================] - 3s 740us/step - loss: 0.0216 - acc: 0.9951 - val_loss: 0.0755 - val_acc: 0.9740cc: \n",
      "Epoch 181/200\n",
      "4457/4457 [==============================] - 3s 669us/step - loss: 0.0213 - acc: 0.9955 - val_loss: 0.0753 - val_acc: 0.9740\n",
      "Epoch 182/200\n",
      "4457/4457 [==============================] - 4s 796us/step - loss: 0.0211 - acc: 0.9955 - val_loss: 0.0751 - val_acc: 0.9740\n",
      "Epoch 183/200\n",
      "4457/4457 [==============================] - 4s 864us/step - loss: 0.0208 - acc: 0.9955 - val_loss: 0.0749 - val_acc: 0.9740\n",
      "Epoch 184/200\n",
      "4457/4457 [==============================] - 4s 855us/step - loss: 0.0205 - acc: 0.9955 - val_loss: 0.0748 - val_acc: 0.9740\n",
      "Epoch 185/200\n",
      "4457/4457 [==============================] - 3s 662us/step - loss: 0.0202 - acc: 0.9955 - val_loss: 0.0747 - val_acc: 0.9740\n",
      "Epoch 186/200\n",
      "4457/4457 [==============================] - 3s 702us/step - loss: 0.0200 - acc: 0.9955 - val_loss: 0.0746 - val_acc: 0.9740\n",
      "Epoch 187/200\n",
      "4457/4457 [==============================] - 4s 793us/step - loss: 0.0197 - acc: 0.9955 - val_loss: 0.0744 - val_acc: 0.9740\n",
      "Epoch 188/200\n",
      "4457/4457 [==============================] - 4s 965us/step - loss: 0.0194 - acc: 0.9957 - val_loss: 0.0743 - val_acc: 0.9740\n",
      "Epoch 189/200\n",
      "4457/4457 [==============================] - 3s 757us/step - loss: 0.0192 - acc: 0.9960 - val_loss: 0.0742 - val_acc: 0.9740\n",
      "Epoch 190/200\n",
      "4457/4457 [==============================] - 3s 778us/step - loss: 0.0189 - acc: 0.9960 - val_loss: 0.0741 - val_acc: 0.9740\n",
      "Epoch 191/200\n",
      "4457/4457 [==============================] - 4s 873us/step - loss: 0.0187 - acc: 0.9960 - val_loss: 0.0740 - val_acc: 0.9740\n",
      "Epoch 192/200\n",
      "4457/4457 [==============================] - 4s 850us/step - loss: 0.0184 - acc: 0.9960 - val_loss: 0.0739 - val_acc: 0.9740\n",
      "Epoch 193/200\n",
      "4457/4457 [==============================] - 4s 852us/step - loss: 0.0182 - acc: 0.9960 - val_loss: 0.0738 - val_acc: 0.9740\n",
      "Epoch 194/200\n",
      "4457/4457 [==============================] - 4s 788us/step - loss: 0.0180 - acc: 0.9960 - val_loss: 0.0737 - val_acc: 0.9740\n",
      "Epoch 195/200\n",
      "4457/4457 [==============================] - 3s 669us/step - loss: 0.0177 - acc: 0.9960 - val_loss: 0.0736 - val_acc: 0.9740\n",
      "Epoch 196/200\n",
      "4457/4457 [==============================] - 4s 857us/step - loss: 0.0175 - acc: 0.9960 - val_loss: 0.0735 - val_acc: 0.9740\n",
      "Epoch 197/200\n",
      "4457/4457 [==============================] - 4s 922us/step - loss: 0.0173 - acc: 0.9960 - val_loss: 0.0734 - val_acc: 0.9740\n",
      "Epoch 198/200\n",
      "4457/4457 [==============================] - 3s 722us/step - loss: 0.0170 - acc: 0.9960 - val_loss: 0.0733 - val_acc: 0.9740\n",
      "Epoch 199/200\n",
      "4457/4457 [==============================] - 3s 691us/step - loss: 0.0168 - acc: 0.9960 - val_loss: 0.0733 - val_acc: 0.9740\n",
      "Epoch 200/200\n",
      "4457/4457 [==============================] - 3s 781us/step - loss: 0.0166 - acc: 0.9964 - val_loss: 0.0732 - val_acc: 0.9740\n",
      "1115/1115 [==============================] - 0s 233us/step\n"
     ]
    }
   ],
   "source": [
    "acc_training = model.fit(xtrain, ytrain_one_hot, epochs=200,verbose=1,validation_data=(xtest, ytest_one_hot),batch_size=32)\n",
    "acc_testing = model.evaluate(xtest, ytest_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.07318325126752458, 0.9739910313901345]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "akurasi = 0.9739910313901345\n"
     ]
    }
   ],
   "source": [
    "print('akurasi = {}'.format(acc_testing[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training & validation accuracy values\n",
    "\n",
    "plt.plot(acc_training.history['acc'])\n",
    "plt.plot(acc_training.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(acc_training.history['loss'])\n",
    "plt.plot(acc_training.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': [0.6753745980326905,\n",
       "  0.6594258624876561,\n",
       "  0.6435538303157139,\n",
       "  0.6277861697256832,\n",
       "  0.6119799229596228,\n",
       "  0.5962739522681643,\n",
       "  0.5806967732617674,\n",
       "  0.5653421172112093,\n",
       "  0.5502775291690912,\n",
       "  0.5356132081805858,\n",
       "  0.521265028891542,\n",
       "  0.5073633331621709,\n",
       "  0.49391045391292315,\n",
       "  0.4809138423391522,\n",
       "  0.4683758260690578,\n",
       "  0.45627118407878103,\n",
       "  0.44466499384208646,\n",
       "  0.4334706821249205,\n",
       "  0.42273067840011663,\n",
       "  0.41242336296714477,\n",
       "  0.4026024708833395,\n",
       "  0.3931471838812122,\n",
       "  0.3841417960521886,\n",
       "  0.37545579214801705,\n",
       "  0.36720636048659083,\n",
       "  0.3592942829891171,\n",
       "  0.3517004523309357,\n",
       "  0.3444655108344929,\n",
       "  0.33753656622005684,\n",
       "  0.33091017777609716,\n",
       "  0.32456357569972494,\n",
       "  0.3184949892117838,\n",
       "  0.31263250120552133,\n",
       "  0.3070627874991284,\n",
       "  0.3016737420462707,\n",
       "  0.296525391976395,\n",
       "  0.29153929428936654,\n",
       "  0.28675424290879425,\n",
       "  0.28214796821632726,\n",
       "  0.2777088545496688,\n",
       "  0.2733916700420893,\n",
       "  0.2692336552346234,\n",
       "  0.26518944320122756,\n",
       "  0.26125961363048295,\n",
       "  0.25743826703907663,\n",
       "  0.2536369241807493,\n",
       "  0.24994863683332777,\n",
       "  0.24638615016712737,\n",
       "  0.24286342793783264,\n",
       "  0.239372870219128,\n",
       "  0.2359701498474241,\n",
       "  0.2325960933226641,\n",
       "  0.22929729907501972,\n",
       "  0.22603815099049043,\n",
       "  0.22280555606690222,\n",
       "  0.21962219280245057,\n",
       "  0.21645826514259048,\n",
       "  0.21340423279813586,\n",
       "  0.2103156067865312,\n",
       "  0.20728485009595418,\n",
       "  0.20430516777685404,\n",
       "  0.20134776185847184,\n",
       "  0.1983997504072339,\n",
       "  0.1955301199592817,\n",
       "  0.1926708260726501,\n",
       "  0.18991625617705118,\n",
       "  0.18713106793673048,\n",
       "  0.184460587398621,\n",
       "  0.18178685336235928,\n",
       "  0.17915858856231107,\n",
       "  0.17659911212632473,\n",
       "  0.17411251819855428,\n",
       "  0.17163144484362794,\n",
       "  0.16919933284211053,\n",
       "  0.16682528533342172,\n",
       "  0.1644969420515903,\n",
       "  0.1621781086975149,\n",
       "  0.1599535878129604,\n",
       "  0.15779972161279132,\n",
       "  0.15560222789979303,\n",
       "  0.15351330320530407,\n",
       "  0.15146639733972037,\n",
       "  0.14948945309415526,\n",
       "  0.14752531017556852,\n",
       "  0.1455724741444994,\n",
       "  0.14372172048941856,\n",
       "  0.14184595760074967,\n",
       "  0.14008391272326756,\n",
       "  0.13835177074767013,\n",
       "  0.13661812129576645,\n",
       "  0.13497652215006106,\n",
       "  0.13335971872034094,\n",
       "  0.13177344320600878,\n",
       "  0.13021027964326834,\n",
       "  0.12872697272976952,\n",
       "  0.1272484419224241,\n",
       "  0.12581810914815275,\n",
       "  0.12440448116026652,\n",
       "  0.12304581193913258,\n",
       "  0.12166780411796185,\n",
       "  0.12036321584151999,\n",
       "  0.11912288181984906,\n",
       "  0.11783404778605619,\n",
       "  0.11661015734276964,\n",
       "  0.11540487054818949,\n",
       "  0.11421766036561787,\n",
       "  0.11310881187830271,\n",
       "  0.11200777218748101,\n",
       "  0.11092972747413567,\n",
       "  0.10983950858225737,\n",
       "  0.10884167493525641,\n",
       "  0.10781845811570706,\n",
       "  0.10683773102514413,\n",
       "  0.10588964896084482,\n",
       "  0.10493890829885487,\n",
       "  0.1040152476106524,\n",
       "  0.10314456885304686,\n",
       "  0.1021976911533841,\n",
       "  0.10139940083026885,\n",
       "  0.10056657057439265,\n",
       "  0.0998239777982235,\n",
       "  0.09899234227802721,\n",
       "  0.09823011861521032,\n",
       "  0.0975073992203704,\n",
       "  0.09674250891123118,\n",
       "  0.09604027376848485,\n",
       "  0.09535183648290656,\n",
       "  0.0947098716709245,\n",
       "  0.094032342229122,\n",
       "  0.09330502603152942,\n",
       "  0.0926904384982292,\n",
       "  0.09210187705743206,\n",
       "  0.0915483901719876,\n",
       "  0.09096014853316305,\n",
       "  0.09038620712945547,\n",
       "  0.0898651481042261,\n",
       "  0.08930641491751233,\n",
       "  0.08878479638709082,\n",
       "  0.08826321420080191,\n",
       "  0.08779648124770734,\n",
       "  0.08728466679930955,\n",
       "  0.08683294628667457,\n",
       "  0.08637687271385716,\n",
       "  0.08591906869358011,\n",
       "  0.08547302264503032,\n",
       "  0.08505988514443416,\n",
       "  0.08458397383780758,\n",
       "  0.08418196521432143,\n",
       "  0.08379539016617521,\n",
       "  0.08341006878011697,\n",
       "  0.08295472401774785,\n",
       "  0.0826168780620189,\n",
       "  0.08223769338008,\n",
       "  0.08190061663665846,\n",
       "  0.08155233334967106,\n",
       "  0.08121319138883475,\n",
       "  0.08088012166956081,\n",
       "  0.08059531845018736,\n",
       "  0.080334281218092,\n",
       "  0.07997985553046513,\n",
       "  0.07968667966601828,\n",
       "  0.07943241600231205,\n",
       "  0.07911043965629398,\n",
       "  0.07883389368152137,\n",
       "  0.07857443328629428,\n",
       "  0.07829602978822896,\n",
       "  0.07811751108399421,\n",
       "  0.07789953961099744,\n",
       "  0.07766374989408549,\n",
       "  0.07746770484164156,\n",
       "  0.07719397399357349,\n",
       "  0.07700967228900424,\n",
       "  0.07678556534474205,\n",
       "  0.0765730386758599,\n",
       "  0.07636048881495747,\n",
       "  0.07619773582325655,\n",
       "  0.07600752827064071,\n",
       "  0.0758019325761918,\n",
       "  0.07557889345882987,\n",
       "  0.07549598078022089,\n",
       "  0.0753138344484327,\n",
       "  0.07512815694320496,\n",
       "  0.07494909878937119,\n",
       "  0.0748416222715458,\n",
       "  0.0746974024762955,\n",
       "  0.07456543197512894,\n",
       "  0.07442048148256246,\n",
       "  0.07432249837174945,\n",
       "  0.0742289494814961,\n",
       "  0.074093810265107,\n",
       "  0.0739962910097343,\n",
       "  0.07390256934034985,\n",
       "  0.07375652170635659,\n",
       "  0.07370241828546796,\n",
       "  0.07355420016871574,\n",
       "  0.07352502042920467,\n",
       "  0.07341791315781028,\n",
       "  0.07331273986502747,\n",
       "  0.07325867864683337,\n",
       "  0.07318325126752458],\n",
       " 'val_acc': [0.7587443953137761,\n",
       "  0.8582959642859318,\n",
       "  0.8627802686840964,\n",
       "  0.8609865467110023,\n",
       "  0.859192824737908,\n",
       "  0.8573991027648139,\n",
       "  0.8573991027648139,\n",
       "  0.8573991027648139,\n",
       "  0.8573991027648139,\n",
       "  0.8573991027648139,\n",
       "  0.8573991027648139,\n",
       "  0.8573991027648139,\n",
       "  0.8573991027648139,\n",
       "  0.8573991027648139,\n",
       "  0.8573991027648139,\n",
       "  0.8573991027648139,\n",
       "  0.858295963751361,\n",
       "  0.859192824737908,\n",
       "  0.8600896857244551,\n",
       "  0.8600896857244551,\n",
       "  0.8618834076975493,\n",
       "  0.8618834076975493,\n",
       "  0.8627802686840964,\n",
       "  0.8627802686840964,\n",
       "  0.8627802686840964,\n",
       "  0.8636771296706435,\n",
       "  0.8636771296706435,\n",
       "  0.8645739906571905,\n",
       "  0.8672645736168318,\n",
       "  0.869058295589926,\n",
       "  0.869058295589926,\n",
       "  0.869058295589926,\n",
       "  0.8717488785495673,\n",
       "  0.8726457395361144,\n",
       "  0.8744394615092085,\n",
       "  0.8744394615092085,\n",
       "  0.8762331834823027,\n",
       "  0.8771300444688498,\n",
       "  0.880717488415038,\n",
       "  0.880717488415038,\n",
       "  0.8816143494015852,\n",
       "  0.8843049323612264,\n",
       "  0.8852017933477735,\n",
       "  0.8860986543343207,\n",
       "  0.8878923763074148,\n",
       "  0.890582959267056,\n",
       "  0.890582959267056,\n",
       "  0.8932735422266973,\n",
       "  0.8959641251863385,\n",
       "  0.8977578471594327,\n",
       "  0.8995515700412973,\n",
       "  0.9013452920143914,\n",
       "  0.9022421530009385,\n",
       "  0.9031390139874856,\n",
       "  0.9040358749740327,\n",
       "  0.9067264579336739,\n",
       "  0.9067264579336739,\n",
       "  0.9085201799067681,\n",
       "  0.9139013458260506,\n",
       "  0.9165919287856918,\n",
       "  0.9192825117453332,\n",
       "  0.9246636776646157,\n",
       "  0.9255605386511627,\n",
       "  0.9264573996377098,\n",
       "  0.9273542600896861,\n",
       "  0.9282511210762332,\n",
       "  0.9282511210762332,\n",
       "  0.9291479820627803,\n",
       "  0.9300448430493273,\n",
       "  0.9309417040358744,\n",
       "  0.9318385650224216,\n",
       "  0.9345291479820628,\n",
       "  0.9363228699551569,\n",
       "  0.9363228699551569,\n",
       "  0.9363228699551569,\n",
       "  0.9381165919282511,\n",
       "  0.9399103139013453,\n",
       "  0.9408071748878923,\n",
       "  0.9408071748878923,\n",
       "  0.9417040358744395,\n",
       "  0.9426008968609866,\n",
       "  0.9434977578475336,\n",
       "  0.9452914798206278,\n",
       "  0.947085201793722,\n",
       "  0.9488789237668162,\n",
       "  0.9488789237668162,\n",
       "  0.9488789237668162,\n",
       "  0.9506726457399103,\n",
       "  0.9506726457399103,\n",
       "  0.9533632286995516,\n",
       "  0.9551569506726457,\n",
       "  0.9560538116591928,\n",
       "  0.95695067264574,\n",
       "  0.95695067264574,\n",
       "  0.95695067264574,\n",
       "  0.9596412556053812,\n",
       "  0.9596412556053812,\n",
       "  0.9596412556053812,\n",
       "  0.9596412556053812,\n",
       "  0.9596412556053812,\n",
       "  0.9605381165919282,\n",
       "  0.9605381165919282,\n",
       "  0.9614349775784753,\n",
       "  0.9614349775784753,\n",
       "  0.9614349775784753,\n",
       "  0.9623318385650225,\n",
       "  0.9623318385650225,\n",
       "  0.9623318385650225,\n",
       "  0.9623318385650225,\n",
       "  0.9632286995515695,\n",
       "  0.9632286995515695,\n",
       "  0.9632286995515695,\n",
       "  0.9641255605381166,\n",
       "  0.9650224215246637,\n",
       "  0.9650224215246637,\n",
       "  0.9659192825112107,\n",
       "  0.9659192825112107,\n",
       "  0.9659192825112107,\n",
       "  0.9668161434977578,\n",
       "  0.9668161434977578,\n",
       "  0.9668161434977578,\n",
       "  0.9668161434977578,\n",
       "  0.9668161434977578,\n",
       "  0.9668161434977578,\n",
       "  0.9668161434977578,\n",
       "  0.9668161434977578,\n",
       "  0.9668161434977578,\n",
       "  0.9668161434977578,\n",
       "  0.9668161434977578,\n",
       "  0.967713004484305,\n",
       "  0.968609865470852,\n",
       "  0.968609865470852,\n",
       "  0.9695067264573991,\n",
       "  0.9695067264573991,\n",
       "  0.9695067264573991,\n",
       "  0.9704035874439462,\n",
       "  0.9704035874439462,\n",
       "  0.9704035874439462,\n",
       "  0.9713004484304932,\n",
       "  0.9721973094170404,\n",
       "  0.9721973094170404,\n",
       "  0.9721973094170404,\n",
       "  0.9721973094170404,\n",
       "  0.9721973094170404,\n",
       "  0.9721973094170404,\n",
       "  0.9721973094170404,\n",
       "  0.9721973094170404,\n",
       "  0.9721973094170404,\n",
       "  0.9713004484304932,\n",
       "  0.9713004484304932,\n",
       "  0.9713004484304932,\n",
       "  0.9713004484304932,\n",
       "  0.9713004484304932,\n",
       "  0.9713004484304932,\n",
       "  0.9713004484304932,\n",
       "  0.9730941704035875,\n",
       "  0.9730941704035875,\n",
       "  0.9730941704035875,\n",
       "  0.9730941704035875,\n",
       "  0.9730941704035875,\n",
       "  0.9730941704035875,\n",
       "  0.9730941704035875,\n",
       "  0.9730941704035875,\n",
       "  0.9730941704035875,\n",
       "  0.9730941704035875,\n",
       "  0.9730941704035875,\n",
       "  0.9730941704035875,\n",
       "  0.9730941704035875,\n",
       "  0.9730941704035875,\n",
       "  0.9730941704035875,\n",
       "  0.9730941704035875,\n",
       "  0.9730941704035875,\n",
       "  0.9730941704035875,\n",
       "  0.9730941704035875,\n",
       "  0.9730941704035875,\n",
       "  0.9730941704035875,\n",
       "  0.9730941704035875,\n",
       "  0.9739910313901345,\n",
       "  0.9739910313901345,\n",
       "  0.9739910313901345,\n",
       "  0.9739910313901345,\n",
       "  0.9739910313901345,\n",
       "  0.9739910313901345,\n",
       "  0.9739910313901345,\n",
       "  0.9739910313901345,\n",
       "  0.9739910313901345,\n",
       "  0.9739910313901345,\n",
       "  0.9739910313901345,\n",
       "  0.9739910313901345,\n",
       "  0.9739910313901345,\n",
       "  0.9739910313901345,\n",
       "  0.9739910313901345,\n",
       "  0.9739910313901345,\n",
       "  0.9739910313901345,\n",
       "  0.9739910313901345,\n",
       "  0.9739910313901345,\n",
       "  0.9739910313901345,\n",
       "  0.9739910313901345,\n",
       "  0.9739910313901345,\n",
       "  0.9739910313901345],\n",
       " 'loss': [0.6837922951044832,\n",
       "  0.6665683640662345,\n",
       "  0.6495752147770206,\n",
       "  0.6326263291648024,\n",
       "  0.6156936780417316,\n",
       "  0.5987645602156716,\n",
       "  0.5819033295123438,\n",
       "  0.5652526774052309,\n",
       "  0.5488815641280165,\n",
       "  0.5328227475482786,\n",
       "  0.5171535715597854,\n",
       "  0.5018675859289531,\n",
       "  0.4870471180969754,\n",
       "  0.4727167147262925,\n",
       "  0.458838686825102,\n",
       "  0.4454288825222214,\n",
       "  0.4325313327663919,\n",
       "  0.4200940945302528,\n",
       "  0.4081198163756828,\n",
       "  0.3966378525003125,\n",
       "  0.3856229481005417,\n",
       "  0.3750480509925001,\n",
       "  0.36490852220432357,\n",
       "  0.3551742673883102,\n",
       "  0.3458574227161249,\n",
       "  0.33691467542350895,\n",
       "  0.3283567262860673,\n",
       "  0.32014807774891146,\n",
       "  0.31228479898133726,\n",
       "  0.3047437834076371,\n",
       "  0.2975033518476071,\n",
       "  0.2905688090043028,\n",
       "  0.2839054691368939,\n",
       "  0.2775201874415062,\n",
       "  0.27137472796980405,\n",
       "  0.26546309908757115,\n",
       "  0.2597650650145629,\n",
       "  0.2542893258290144,\n",
       "  0.24899655972691642,\n",
       "  0.2438907096183752,\n",
       "  0.23895805974511627,\n",
       "  0.23418560213563366,\n",
       "  0.22955242068157192,\n",
       "  0.22504644020673056,\n",
       "  0.22067664419558916,\n",
       "  0.2163873349360508,\n",
       "  0.21222270465843984,\n",
       "  0.20816210229580837,\n",
       "  0.20418520007990443,\n",
       "  0.20027412398930808,\n",
       "  0.19645501025955533,\n",
       "  0.19270583686337547,\n",
       "  0.18902348428931182,\n",
       "  0.18540364310616786,\n",
       "  0.18184475587346186,\n",
       "  0.1783522061898298,\n",
       "  0.17489775742801392,\n",
       "  0.17150506705533825,\n",
       "  0.16817117516358443,\n",
       "  0.1648827534681322,\n",
       "  0.16164388796634302,\n",
       "  0.15846060826582092,\n",
       "  0.1553242055856783,\n",
       "  0.15222778142715532,\n",
       "  0.14919416124988677,\n",
       "  0.1462113748009707,\n",
       "  0.14329265605341512,\n",
       "  0.14042238475884827,\n",
       "  0.137606339023898,\n",
       "  0.13484589010978565,\n",
       "  0.1321400824348328,\n",
       "  0.12948759051617792,\n",
       "  0.1268882007902918,\n",
       "  0.12434283428818542,\n",
       "  0.12185346240138885,\n",
       "  0.11942018357552513,\n",
       "  0.11703228834654575,\n",
       "  0.11469815031031683,\n",
       "  0.11241897078728713,\n",
       "  0.11017554666072177,\n",
       "  0.10799821976731919,\n",
       "  0.1058763052138989,\n",
       "  0.10379574900388129,\n",
       "  0.10177058624076502,\n",
       "  0.09978147757770182,\n",
       "  0.09784768029816727,\n",
       "  0.09594801751763424,\n",
       "  0.09410147003567029,\n",
       "  0.092296307781552,\n",
       "  0.09054151182755617,\n",
       "  0.08879918367880889,\n",
       "  0.08711993295447773,\n",
       "  0.08546928836156868,\n",
       "  0.08386907054268915,\n",
       "  0.08230353738892793,\n",
       "  0.08077071509822377,\n",
       "  0.07927352492755001,\n",
       "  0.0778066991960652,\n",
       "  0.07637294349761081,\n",
       "  0.07498006695212404,\n",
       "  0.07360952089998068,\n",
       "  0.07227693221140849,\n",
       "  0.07096329483518907,\n",
       "  0.06968671716758026,\n",
       "  0.06844023528780696,\n",
       "  0.06722050258295938,\n",
       "  0.06602249573435595,\n",
       "  0.06485698529010747,\n",
       "  0.06371626883598087,\n",
       "  0.0626041781285137,\n",
       "  0.061505443330654413,\n",
       "  0.060439693652693534,\n",
       "  0.05939805276469583,\n",
       "  0.05837770184681291,\n",
       "  0.057376137383604815,\n",
       "  0.05640215578804322,\n",
       "  0.055446671799664435,\n",
       "  0.05450916455944531,\n",
       "  0.05360080499268043,\n",
       "  0.052713123769310255,\n",
       "  0.051843017451707524,\n",
       "  0.050988191293645714,\n",
       "  0.050158797444210264,\n",
       "  0.04933945639554176,\n",
       "  0.04853772544000103,\n",
       "  0.047756312529930275,\n",
       "  0.04699243759221966,\n",
       "  0.04624056312931956,\n",
       "  0.045501811216303874,\n",
       "  0.04476417440295032,\n",
       "  0.0440685068839517,\n",
       "  0.043379482317741795,\n",
       "  0.04270820391909983,\n",
       "  0.04204410490752711,\n",
       "  0.04139625501043932,\n",
       "  0.04075928074524011,\n",
       "  0.04013363315916286,\n",
       "  0.039520309913284125,\n",
       "  0.038923554010446865,\n",
       "  0.03833217393178121,\n",
       "  0.03775148049902964,\n",
       "  0.03718354181688056,\n",
       "  0.03662895231800468,\n",
       "  0.03607806586697012,\n",
       "  0.035539474894119426,\n",
       "  0.0350161586562788,\n",
       "  0.03448831289588723,\n",
       "  0.03398198540731412,\n",
       "  0.0334803543847814,\n",
       "  0.032988169995931224,\n",
       "  0.03249951796462003,\n",
       "  0.03202853792773078,\n",
       "  0.031564416412104467,\n",
       "  0.031110296685132433,\n",
       "  0.030661528730931732,\n",
       "  0.030221526067188257,\n",
       "  0.029793590049800357,\n",
       "  0.029366224451295216,\n",
       "  0.028947172576546106,\n",
       "  0.028537766247029053,\n",
       "  0.028134248258505457,\n",
       "  0.027740380185346365,\n",
       "  0.027346674335749797,\n",
       "  0.026963630729200917,\n",
       "  0.026590366803723144,\n",
       "  0.026222505242916295,\n",
       "  0.02586072948166801,\n",
       "  0.02550465656001023,\n",
       "  0.025153540908982728,\n",
       "  0.02481137162263762,\n",
       "  0.02446640849237585,\n",
       "  0.024137344793599396,\n",
       "  0.023804874324054027,\n",
       "  0.02348035737123639,\n",
       "  0.023160596733166013,\n",
       "  0.022849697034387263,\n",
       "  0.02253939003853091,\n",
       "  0.02223841221290558,\n",
       "  0.021934432090615942,\n",
       "  0.02164001248790286,\n",
       "  0.021349162797679483,\n",
       "  0.02106436224869961,\n",
       "  0.020783844636335286,\n",
       "  0.02050685479395745,\n",
       "  0.020237481137033445,\n",
       "  0.019972834099938355,\n",
       "  0.019709368739949967,\n",
       "  0.019447342703895823,\n",
       "  0.019192967533359806,\n",
       "  0.018942855834703502,\n",
       "  0.018690710001125085,\n",
       "  0.018446614026637997,\n",
       "  0.01820739749204706,\n",
       "  0.01796837008646185,\n",
       "  0.017736850709731498,\n",
       "  0.017502622207677267,\n",
       "  0.017276744202150474,\n",
       "  0.017049939563677185,\n",
       "  0.016826314055096523,\n",
       "  0.01661139389256378],\n",
       " 'acc': [0.6493156832083474,\n",
       "  0.827462418667265,\n",
       "  0.8878169172088849,\n",
       "  0.886695086394347,\n",
       "  0.8813102984203735,\n",
       "  0.8777204397576845,\n",
       "  0.8761498765986089,\n",
       "  0.8757011442674445,\n",
       "  0.875028045784071,\n",
       "  0.8748036796184888,\n",
       "  0.8741305811083688,\n",
       "  0.8743549473006976,\n",
       "  0.8741305811351154,\n",
       "  0.8743549472873243,\n",
       "  0.8750280457706978,\n",
       "  0.8754767781286088,\n",
       "  0.8761498765986089,\n",
       "  0.8770473412609379,\n",
       "  0.8774960736054755,\n",
       "  0.87861790444676,\n",
       "  0.8788422705989689,\n",
       "  0.8788422706123422,\n",
       "  0.8792910029167601,\n",
       "  0.8792910029435067,\n",
       "  0.8799641014135069,\n",
       "  0.8815346645859558,\n",
       "  0.8824321292349113,\n",
       "  0.884002692393987,\n",
       "  0.8853488893874804,\n",
       "  0.8864707202153915,\n",
       "  0.8887143818712138,\n",
       "  0.8905093111958716,\n",
       "  0.893426071361814,\n",
       "  0.8952210006730985,\n",
       "  0.8974646623556674,\n",
       "  0.8992595916535786,\n",
       "  0.8999326901770719,\n",
       "  0.9015032533227743,\n",
       "  0.903522548799641,\n",
       "  0.9064393089655832,\n",
       "  0.9082342382902411,\n",
       "  0.9104778999326901,\n",
       "  0.9127215615885125,\n",
       "  0.9147408570921258,\n",
       "  0.9167601525823659,\n",
       "  0.9185550818936504,\n",
       "  0.9196769127349349,\n",
       "  0.9212474758806372,\n",
       "  0.923939869867624,\n",
       "  0.9255104330266996,\n",
       "  0.9277540946825219,\n",
       "  0.9304464886695086,\n",
       "  0.9329145164909132,\n",
       "  0.9344850796499888,\n",
       "  0.9365043751402289,\n",
       "  0.9378505721470956,\n",
       "  0.9389724029616334,\n",
       "  0.9414404307964112,\n",
       "  0.9430109939421135,\n",
       "  0.9450302894457269,\n",
       "  0.9472739510881759,\n",
       "  0.9499663450751626,\n",
       "  0.9517612743998205,\n",
       "  0.952883105241105,\n",
       "  0.9546780345657627,\n",
       "  0.9551267668969272,\n",
       "  0.957819160883914,\n",
       "  0.9587166255462429,\n",
       "  0.9607359210231097,\n",
       "  0.9618577518643942,\n",
       "  0.9623064841821853,\n",
       "  0.962979582678932,\n",
       "  0.9638770473412609,\n",
       "  0.9647745120035899,\n",
       "  0.9656719766659188,\n",
       "  0.9667938074938299,\n",
       "  0.9681400044873233,\n",
       "  0.9694862014808167,\n",
       "  0.9706080323087278,\n",
       "  0.9721785954678035,\n",
       "  0.9726273277989679,\n",
       "  0.9733004262957146,\n",
       "  0.9735247924612969,\n",
       "  0.9746466233025812,\n",
       "  0.9762171864616569,\n",
       "  0.9777877496073593,\n",
       "  0.9782364819385236,\n",
       "  0.9784608481041059,\n",
       "  0.9784608481041059,\n",
       "  0.9791339466008526,\n",
       "  0.9793583127798081,\n",
       "  0.9800314112765548,\n",
       "  0.9807045097599282,\n",
       "  0.9809288759255105,\n",
       "  0.9818263405878394,\n",
       "  0.982499439084586,\n",
       "  0.9827238052501682,\n",
       "  0.9829481714157505,\n",
       "  0.9831725375813327,\n",
       "  0.9838456360780794,\n",
       "  0.9840700022436617,\n",
       "  0.9840700022436617,\n",
       "  0.9847431007537816,\n",
       "  0.9847431007404084,\n",
       "  0.9849674669059906,\n",
       "  0.9851918330849461,\n",
       "  0.9854161992371551,\n",
       "  0.9854161992371551,\n",
       "  0.9863136638994839,\n",
       "  0.9867623962306484,\n",
       "  0.9869867623962306,\n",
       "  0.9872111285618129,\n",
       "  0.9872111285618129,\n",
       "  0.9874354947273951,\n",
       "  0.9872111285618129,\n",
       "  0.9876598608929773,\n",
       "  0.9881085932508883,\n",
       "  0.9881085932241418,\n",
       "  0.9881085932241418,\n",
       "  0.9881085932241418,\n",
       "  0.988332959389724,\n",
       "  0.988332959389724,\n",
       "  0.9885573255553063,\n",
       "  0.9885573255686795,\n",
       "  0.9887816917208885,\n",
       "  0.9896791563832175,\n",
       "  0.9896791563832175,\n",
       "  0.9899035225487997,\n",
       "  0.9901278887277551,\n",
       "  0.9901278887143818,\n",
       "  0.9903522548799641,\n",
       "  0.9903522548799641,\n",
       "  0.9903522548799641,\n",
       "  0.9903522548799641,\n",
       "  0.9905766210455463,\n",
       "  0.9905766210455463,\n",
       "  0.9908009872111285,\n",
       "  0.9908009872111285,\n",
       "  0.9910253533767108,\n",
       "  0.9910253533767108,\n",
       "  0.991249719542293,\n",
       "  0.9916984518734575,\n",
       "  0.9921471842046219,\n",
       "  0.9923715503702042,\n",
       "  0.9923715503702042,\n",
       "  0.9923715503702042,\n",
       "  0.9923715503702042,\n",
       "  0.9925959165357864,\n",
       "  0.9930446488669509,\n",
       "  0.9934933812114886,\n",
       "  0.9934933811981154,\n",
       "  0.9934933811981154,\n",
       "  0.9934933811981154,\n",
       "  0.9934933811981154,\n",
       "  0.9934933811981154,\n",
       "  0.9939421135292797,\n",
       "  0.9939421135292797,\n",
       "  0.9939421135292797,\n",
       "  0.9939421135292797,\n",
       "  0.994166479694862,\n",
       "  0.994166479694862,\n",
       "  0.9941664797082352,\n",
       "  0.9943908458604442,\n",
       "  0.9943908458604442,\n",
       "  0.9943908458604442,\n",
       "  0.9943908458604442,\n",
       "  0.9943908458604442,\n",
       "  0.9943908458604442,\n",
       "  0.9943908458604442,\n",
       "  0.9943908458604442,\n",
       "  0.9943908458604442,\n",
       "  0.9943908458604442,\n",
       "  0.9946152120260264,\n",
       "  0.9948395781916087,\n",
       "  0.9948395781916087,\n",
       "  0.9948395781916087,\n",
       "  0.9948395781916087,\n",
       "  0.9948395782049819,\n",
       "  0.9950639443571909,\n",
       "  0.9950639443571909,\n",
       "  0.9955126766883554,\n",
       "  0.9955126766883554,\n",
       "  0.9955126766883554,\n",
       "  0.9955126766883554,\n",
       "  0.9955126766883554,\n",
       "  0.9955126766883554,\n",
       "  0.9955126766883554,\n",
       "  0.9957370428539376,\n",
       "  0.9959614090195199,\n",
       "  0.9959614090195199,\n",
       "  0.9959614090195199,\n",
       "  0.9959614090195199,\n",
       "  0.9959614090195199,\n",
       "  0.9959614090195199,\n",
       "  0.9959614090195199,\n",
       "  0.9959614090195199,\n",
       "  0.9959614090195199,\n",
       "  0.9959614090195199,\n",
       "  0.9959614090195199,\n",
       "  0.9964101413506843]}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_training.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F. Confusion Matriks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matriks digunakan untuk mengevaluasi hasil prediksi. Klasifikasi yang digunakan adalah klasifikasi binary yaitu 1 dan 0. Untuk itu ada 4 kemungkinan prediksi yang akan digunakan yaitu True Positive, False Positive, False Negative dan True Negative. \n",
    "- True Positive (TP) = Data yang berada di kelas 1 dan hasil prediksi bernilai 1 \n",
    "- False Negative (FN) = Data yang berada di kelas 0 dan hasil prediksi bernilai 0\n",
    "- False Positive (FP) = Data yang berada di kelas 0 dan hasil prediksi bernilai 1\n",
    "- False Negative (FN) = Data yang berada di kelas 1 dan hasil prediksi bernilai 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = model.predict_classes(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(ytest, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[947,   2],\n",
       "       [ 27, 139]], dtype=int64)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(ytest)\n",
    "print(ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(ytest, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98       949\n",
      "           1       0.99      0.84      0.91       166\n",
      "\n",
      "    accuracy                           0.97      1115\n",
      "   macro avg       0.98      0.92      0.95      1115\n",
      "weighted avg       0.97      0.97      0.97      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Accurasy = Rasio prediksi benar (positive dan negative) dengan keseluruhan data\n",
    "- Precission =  Rasio prediksi benar positif dibandingkan dengan keseluruhan hasil yang diprediksi positf\n",
    "- Recall = Rasio prediksi benar positif dibandingkan dengan keseluruhan data yang benar positif\n",
    "- f1-score = F1 Score merupakan perbandingan rata-rata presisi dan recall yang dibobotkan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HASIL ANALISA\n",
    "- Akurasi = 97 %\n",
    "---------------------\n",
    "- Precision\n",
    "- kelas 0 = 97 %\n",
    "- kelas 1 = 99 %\n",
    "---------------------\n",
    "- Recall\n",
    "- kelas 0 = 100 %\n",
    "- kelas 1 = 84 %\n",
    "---------------------\n",
    "- F1-score\n",
    "- kelas 0 = 98 %\n",
    "- kelas 1 = 91 %\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
